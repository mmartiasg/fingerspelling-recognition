{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "\n",
    "from src.constants import TARGET_MAX_LENGHT, MAX_LENGHT_SOURCE\n",
    "from src.data_utils.dataset import build_datset_train_val, VOCAB_SIZE, LHAND_IDX, LHAND_IDX, start_token_idx, end_token_idx, pre_process, pad_token_idx, FEATURE_COLUMNS\n",
    "from src.prod_models.builder import build_prod_transformer_model_v2\n",
    "from src.callbacks import get_predefine_callbacks\n",
    "import optuna\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "TRIALS = 15\n",
    "EPOCHS = 5000\n",
    "EPOCHS_PER_TRIAL = 10\n",
    "BATCH_SIZE = 128\n",
    "TRAIN_SPLIT = 0.8\n",
    "MODEL_NAME = \"prod_v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FEATURE_COLUMNS.shape[0]/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train split: 28160 | val split: 6656\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset = build_datset_train_val(split=TRAIN_SPLIT, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((<tf.Tensor: shape=(128, 128, 52), dtype=float32, numpy=\n",
       "  array([[[0.27098814, 0.8101677 , 0.3654514 , ..., 0.75940263,\n",
       "           0.4921588 , 0.7733124 ],\n",
       "          [0.32206193, 0.8371788 , 0.41678107, ..., 0.7513371 ,\n",
       "           0.47293442, 0.76303047],\n",
       "          [0.        , 0.        , 0.        , ..., 0.72678065,\n",
       "           0.5150777 , 0.7387048 ],\n",
       "          ...,\n",
       "          [0.18945673, 0.8469254 , 0.32516694, ..., 0.705916  ,\n",
       "           0.39349455, 0.7400357 ],\n",
       "          [0.18166085, 0.85682863, 0.3225292 , ..., 0.711607  ,\n",
       "           0.3447126 , 0.74080074],\n",
       "          [0.17465053, 0.85880864, 0.3080451 , ..., 0.7054042 ,\n",
       "           0.33971542, 0.73193616]],\n",
       "  \n",
       "         [[0.25190622, 0.7471702 , 0.32096434, ..., 0.67004734,\n",
       "           0.24290428, 0.69245994],\n",
       "          [0.26125604, 0.7291572 , 0.33123383, ..., 0.6084038 ,\n",
       "           0.29093763, 0.6359742 ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.5928458 ,\n",
       "           0.28803307, 0.6307548 ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        , ..., 0.99216217,\n",
       "           0.05327149, 0.9897317 ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.8088826 ,\n",
       "           0.20474997, 0.83953875],\n",
       "          [0.2506625 , 0.7761538 , 0.340713  , ..., 0.8188474 ,\n",
       "           0.28352112, 0.82849795]],\n",
       "  \n",
       "         [[0.29960674, 0.6473164 , 0.37954128, ..., 0.6355624 ,\n",
       "           0.3563149 , 0.6601243 ],\n",
       "          [0.33247063, 0.70579326, 0.4055317 , ..., 0.6087875 ,\n",
       "           0.3384041 , 0.63568676],\n",
       "          [0.32919934, 0.7012937 , 0.40354568, ..., 0.6020873 ,\n",
       "           0.350928  , 0.6295492 ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        , ..., 0.90581745,\n",
       "           0.26111898, 0.88490933],\n",
       "          [0.        , 0.        , 0.        , ..., 0.9720977 ,\n",
       "           0.15527454, 0.96366906],\n",
       "          [0.        , 0.        , 0.        , ..., 0.753996  ,\n",
       "           0.38927484, 0.77012086]],\n",
       "  \n",
       "         ...,\n",
       "  \n",
       "         [[0.5096093 , 0.85622174, 0.6234309 , ..., 0.82753026,\n",
       "           0.5603945 , 0.84889805],\n",
       "          [0.        , 0.        , 0.        , ..., 0.7317528 ,\n",
       "           0.59769315, 0.7600221 ],\n",
       "          [0.44480604, 0.77933097, 0.38950026, ..., 0.7655641 ,\n",
       "           0.5349108 , 0.7864583 ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ]],\n",
       "  \n",
       "         [[0.59452486, 0.8644028 , 0.6862629 , ..., 0.7558017 ,\n",
       "           0.54074615, 0.76674473],\n",
       "          [0.55921197, 0.83637077, 0.65444136, ..., 0.69549316,\n",
       "           0.5610106 , 0.71729386],\n",
       "          [0.5394393 , 0.8109504 , 0.6446482 , ..., 0.67225736,\n",
       "           0.5460794 , 0.6843218 ],\n",
       "          ...,\n",
       "          [0.3675244 , 0.82155275, 0.4583047 , ..., 0.72080374,\n",
       "           0.3290277 , 0.7543387 ],\n",
       "          [0.3806122 , 0.85328263, 0.4727258 , ..., 0.7016352 ,\n",
       "           0.39913118, 0.72355914],\n",
       "          [0.        , 0.        , 0.        , ..., 0.        ,\n",
       "           0.        , 0.        ]],\n",
       "  \n",
       "         [[0.3260491 , 0.7853819 , 0.45958647, ..., 0.6855632 ,\n",
       "           0.4311411 , 0.7011218 ],\n",
       "          [0.32295418, 0.8044064 , 0.43294612, ..., 0.6473223 ,\n",
       "           0.41427034, 0.6727597 ],\n",
       "          [0.30810815, 0.79935664, 0.41945422, ..., 0.6321157 ,\n",
       "           0.43398595, 0.6483916 ],\n",
       "          ...,\n",
       "          [0.        , 0.        , 0.        , ..., 0.823815  ,\n",
       "           0.37912583, 0.8299897 ],\n",
       "          [0.        , 0.        , 0.        , ..., 0.80033094,\n",
       "           0.4206461 , 0.81101304],\n",
       "          [0.42035043, 0.77790475, 0.52925754, ..., 0.7958199 ,\n",
       "           0.54537374, 0.803428  ]]], dtype=float32)>,\n",
       "  <tf.Tensor: shape=(128, 64), dtype=int32, numpy=\n",
       "  array([[60, 32, 44, ..., 59, 59, 59],\n",
       "         [60, 39, 51, ..., 59, 59, 59],\n",
       "         [60, 16, 23, ..., 59, 59, 59],\n",
       "         ...,\n",
       "         [60, 40, 35, ..., 59, 59, 59],\n",
       "         [60, 18, 18, ..., 59, 59, 59],\n",
       "         [60, 44, 13, ..., 59, 59, 59]], dtype=int32)>),\n",
       " <tf.Tensor: shape=(128, 64), dtype=int32, numpy=\n",
       " array([[32, 44, 33, ..., 59, 59, 59],\n",
       "        [39, 51, 51, ..., 59, 59, 59],\n",
       "        [16, 23,  0, ..., 59, 59, 59],\n",
       "        ...,\n",
       "        [40, 35, 38, ..., 59, 59, 59],\n",
       "        [18, 18, 18, ..., 59, 59, 59],\n",
       "        [44, 13, 36, ..., 59, 59, 59]], dtype=int32)>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = build_prod_transformer_model_v2(trial=trial)\n",
    "    model.build([(None, MAX_LENGHT_SOURCE, int(FEATURE_COLUMNS.shape[0]/2)), (None, TARGET_MAX_LENGHT)])\n",
    "    model.fit(train_dataset, validation_data=val_dataset, epochs=EPOCHS_PER_TRIAL, callbacks=get_predefine_callbacks(model_name=MODEL_NAME, patience=3), verbose=0)\n",
    "    levenshtein = model.evaluate(val_dataset)[-1]\n",
    "\n",
    "    return  levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 23:39:54,369] A new study created in memory with name: no-name-31db08f6-208d-47d3-9ea6-06f12150a65d\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "506e0b169a5543fb97f1cfa0ec7789e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 1s 11ms/step - loss: 0.9177 - accuracy: 0.7487 - sparse_levenshtein_v1: 0.2466\n",
      "[I 2023-08-24 23:41:24,474] Trial 0 finished with value: 0.24664828181266785 and parameters: {'attention_heads': 3, 'learning_rate': 1.3466229815841302e-05, 'drop_out_out_decoder': 0.5, 'dense_layers': 5, 'drop_out': 0.2, 'encoder_kernel_size': 5}. Best is trial 0 with value: 0.24664828181266785.\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 0.7066 - accuracy: 0.7870 - sparse_levenshtein_v1: 0.2115\n",
      "[I 2023-08-24 23:43:16,439] Trial 1 finished with value: 0.21152471005916595 and parameters: {'attention_heads': 6, 'learning_rate': 0.0006269092262292042, 'drop_out_out_decoder': 0.05, 'dense_layers': 5, 'drop_out': 0.30000000000000004, 'encoder_kernel_size': 11}. Best is trial 1 with value: 0.21152471005916595.\n",
      "77/77 [==============================] - 3s 36ms/step - loss: 0.7710 - accuracy: 0.7728 - sparse_levenshtein_v1: 0.2240\n",
      "[I 2023-08-24 23:45:13,637] Trial 2 finished with value: 0.22399476170539856 and parameters: {'attention_heads': 2, 'learning_rate': 0.0024734140958120538, 'drop_out_out_decoder': 0.5, 'dense_layers': 5, 'drop_out': 0.25, 'encoder_kernel_size': 11}. Best is trial 1 with value: 0.21152471005916595.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 1.6371 - accuracy: 0.7081 - sparse_levenshtein_v1: 0.2919\n",
      "[I 2023-08-24 23:46:32,978] Trial 3 finished with value: 0.29194554686546326 and parameters: {'attention_heads': 5, 'learning_rate': 0.0723334979763194, 'drop_out_out_decoder': 0.25, 'dense_layers': 1, 'drop_out': 0.30000000000000004, 'encoder_kernel_size': 6}. Best is trial 1 with value: 0.21152471005916595.\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.7521 - accuracy: 0.7795 - sparse_levenshtein_v1: 0.2191\n",
      "[I 2023-08-24 23:48:14,616] Trial 4 finished with value: 0.21912527084350586 and parameters: {'attention_heads': 2, 'learning_rate': 0.0016197661245284387, 'drop_out_out_decoder': 0.25, 'dense_layers': 4, 'drop_out': 0.1, 'encoder_kernel_size': 4}. Best is trial 1 with value: 0.21152471005916595.\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 0.7988 - accuracy: 0.7627 - sparse_levenshtein_v1: 0.2323\n",
      "[I 2023-08-24 23:49:28,648] Trial 5 finished with value: 0.23229263722896576 and parameters: {'attention_heads': 1, 'learning_rate': 0.0019182900089870457, 'drop_out_out_decoder': 0.4, 'dense_layers': 2, 'drop_out': 0.2, 'encoder_kernel_size': 4}. Best is trial 1 with value: 0.21152471005916595.\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.8922 - accuracy: 0.7436 - sparse_levenshtein_v1: 0.2525\n",
      "[I 2023-08-24 23:50:41,876] Trial 6 finished with value: 0.2524782121181488 and parameters: {'attention_heads': 1, 'learning_rate': 5.481766785908131e-05, 'drop_out_out_decoder': 0.30000000000000004, 'dense_layers': 5, 'drop_out': 0.5, 'encoder_kernel_size': 3}. Best is trial 1 with value: 0.21152471005916595.\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.8002 - accuracy: 0.7623 - sparse_levenshtein_v1: 0.2329\n",
      "[I 2023-08-24 23:52:31,211] Trial 7 finished with value: 0.23290742933750153 and parameters: {'attention_heads': 1, 'learning_rate': 0.001364778692896508, 'drop_out_out_decoder': 0.1, 'dense_layers': 4, 'drop_out': 0.2, 'encoder_kernel_size': 8}. Best is trial 1 with value: 0.21152471005916595.\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 0.8496 - accuracy: 0.7512 - sparse_levenshtein_v1: 0.2427\n",
      "[I 2023-08-24 23:53:37,751] Trial 8 finished with value: 0.24272534251213074 and parameters: {'attention_heads': 6, 'learning_rate': 0.03247057388188425, 'drop_out_out_decoder': 0.2, 'dense_layers': 2, 'drop_out': 0.2, 'encoder_kernel_size': 7}. Best is trial 1 with value: 0.21152471005916595.\n",
      "77/77 [==============================] - 12s 149ms/step - loss: 0.7089 - accuracy: 0.7869 - sparse_levenshtein_v1: 0.2117\n",
      "[I 2023-08-25 00:02:29,272] Trial 9 finished with value: 0.21170487999916077 and parameters: {'attention_heads': 4, 'learning_rate': 0.0019571854477205396, 'drop_out_out_decoder': 0.35000000000000003, 'dense_layers': 3, 'drop_out': 0.35000000000000003, 'encoder_kernel_size': 4}. Best is trial 1 with value: 0.21152471005916595.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.7272 - accuracy: 0.7815 - sparse_levenshtein_v1: 0.2171\n",
      "[I 2023-08-25 00:13:13,735] Trial 10 finished with value: 0.21707814931869507 and parameters: {'attention_heads': 8, 'learning_rate': 0.00019201108427591428, 'drop_out_out_decoder': 0.0, 'dense_layers': 4, 'drop_out': 0.0, 'encoder_kernel_size': 12}. Best is trial 1 with value: 0.21152471005916595.\n",
      "77/77 [==============================] - 12s 156ms/step - loss: 0.7239 - accuracy: 0.7829 - sparse_levenshtein_v1: 0.2149\n",
      "[I 2023-08-25 00:17:21,991] Trial 11 finished with value: 0.21488425135612488 and parameters: {'attention_heads': 6, 'learning_rate': 0.0002954993796492026, 'drop_out_out_decoder': 0.0, 'dense_layers': 3, 'drop_out': 0.4, 'encoder_kernel_size': 9}. Best is trial 1 with value: 0.21152471005916595.\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 0.7371 - accuracy: 0.7803 - sparse_levenshtein_v1: 0.2175\n",
      "[I 2023-08-25 00:25:02,339] Trial 12 finished with value: 0.2175426185131073 and parameters: {'attention_heads': 4, 'learning_rate': 0.008632442834146414, 'drop_out_out_decoder': 0.35000000000000003, 'dense_layers': 3, 'drop_out': 0.35000000000000003, 'encoder_kernel_size': 9}. Best is trial 1 with value: 0.21152471005916595.\n",
      "77/77 [==============================] - 1s 14ms/step - loss: 0.7132 - accuracy: 0.7856 - sparse_levenshtein_v1: 0.2122\n",
      "[I 2023-08-25 00:26:51,159] Trial 13 finished with value: 0.212222620844841 and parameters: {'attention_heads': 8, 'learning_rate': 0.0003787204398141964, 'drop_out_out_decoder': 0.15000000000000002, 'dense_layers': 2, 'drop_out': 0.45, 'encoder_kernel_size': 10}. Best is trial 1 with value: 0.21152471005916595.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.7627 - accuracy: 0.7733 - sparse_levenshtein_v1: 0.2229\n",
      "[I 2023-08-25 00:28:17,840] Trial 14 finished with value: 0.22293785214424133 and parameters: {'attention_heads': 6, 'learning_rate': 0.01065855311206791, 'drop_out_out_decoder': 0.05, 'dense_layers': 3, 'drop_out': 0.35000000000000003, 'encoder_kernel_size': 12}. Best is trial 1 with value: 0.21152471005916595.\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=TRIALS, gc_after_trial=True, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = study.best_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenTrial(number=1, state=TrialState.COMPLETE, values=[0.21152471005916595], datetime_start=datetime.datetime(2023, 8, 24, 23, 41, 24, 606679), datetime_complete=datetime.datetime(2023, 8, 24, 23, 43, 16, 439325), params={'attention_heads': 6, 'learning_rate': 0.0006269092262292042, 'drop_out_out_decoder': 0.05, 'dense_layers': 5, 'drop_out': 0.30000000000000004, 'encoder_kernel_size': 11}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'attention_heads': IntDistribution(high=8, log=False, low=1, step=1), 'learning_rate': FloatDistribution(high=0.1, log=True, low=1e-05, step=None), 'drop_out_out_decoder': FloatDistribution(high=0.5, log=False, low=0.0, step=0.05), 'dense_layers': IntDistribution(high=5, log=False, low=1, step=1), 'drop_out': FloatDistribution(high=0.5, log=False, low=0.0, step=0.05), 'encoder_kernel_size': IntDistribution(high=12, log=False, low=3, step=1)}, trial_id=1, value=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: 1\n",
      "Model: \"finger_spelling_v2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " landmark_embedding_v2 (Lan  multiple                  89984     \n",
      " dmarkEmbeddingV2)                                               \n",
      "                                                                 \n",
      " basic_positional_embedding  multiple                  8064      \n",
      " s (BasicPositionalEmbeddin                                      \n",
      " gs)                                                             \n",
      "                                                                 \n",
      " transformer_encoder (Trans  multiple                  51207     \n",
      " formerEncoder)                                                  \n",
      "                                                                 \n",
      " transformer_decoder (Trans  multiple                  102093    \n",
      " formerDecoder)                                                  \n",
      "                                                                 \n",
      " dropout (Dropout)           multiple                  0         \n",
      "                                                                 \n",
      " sequential_2 (Sequential)   (None, 64, 52)            14404     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         multiple                  0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             multiple                  3286      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 269038 (1.03 MB)\n",
      "Trainable params: 269038 (1.03 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5000\n",
      "321/321 [==============================] - 16s 32ms/step - loss: 1.0871 - accuracy: 0.7297 - sparse_levenshtein_v1: 0.2593 - val_loss: 0.8300 - val_accuracy: 0.7636 - val_sparse_levenshtein_v1: 0.2310\n",
      "Epoch 2/5000\n",
      "321/321 [==============================] - 9s 28ms/step - loss: 0.8425 - accuracy: 0.7595 - sparse_levenshtein_v1: 0.2329 - val_loss: 0.7888 - val_accuracy: 0.7707 - val_sparse_levenshtein_v1: 0.2266\n",
      "Epoch 3/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.8064 - accuracy: 0.7665 - sparse_levenshtein_v1: 0.2285 - val_loss: 0.7735 - val_accuracy: 0.7732 - val_sparse_levenshtein_v1: 0.2243\n",
      "Epoch 4/5000\n",
      "321/321 [==============================] - 9s 28ms/step - loss: 0.7879 - accuracy: 0.7704 - sparse_levenshtein_v1: 0.2260 - val_loss: 0.7607 - val_accuracy: 0.7754 - val_sparse_levenshtein_v1: 0.2230\n",
      "Epoch 5/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.7700 - accuracy: 0.7740 - sparse_levenshtein_v1: 0.2232 - val_loss: 0.7448 - val_accuracy: 0.7779 - val_sparse_levenshtein_v1: 0.2206\n",
      "Epoch 6/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.7544 - accuracy: 0.7766 - sparse_levenshtein_v1: 0.2206 - val_loss: 0.7324 - val_accuracy: 0.7811 - val_sparse_levenshtein_v1: 0.2171\n",
      "Epoch 7/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.7425 - accuracy: 0.7794 - sparse_levenshtein_v1: 0.2181 - val_loss: 0.7211 - val_accuracy: 0.7839 - val_sparse_levenshtein_v1: 0.2140\n",
      "Epoch 8/5000\n",
      "321/321 [==============================] - 9s 28ms/step - loss: 0.7338 - accuracy: 0.7814 - sparse_levenshtein_v1: 0.2163 - val_loss: 0.7143 - val_accuracy: 0.7856 - val_sparse_levenshtein_v1: 0.2126\n",
      "Epoch 9/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.7274 - accuracy: 0.7830 - sparse_levenshtein_v1: 0.2148 - val_loss: 0.7117 - val_accuracy: 0.7862 - val_sparse_levenshtein_v1: 0.2121\n",
      "Epoch 10/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.7233 - accuracy: 0.7840 - sparse_levenshtein_v1: 0.2138 - val_loss: 0.7081 - val_accuracy: 0.7865 - val_sparse_levenshtein_v1: 0.2118\n",
      "Epoch 11/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.7194 - accuracy: 0.7850 - sparse_levenshtein_v1: 0.2128 - val_loss: 0.7044 - val_accuracy: 0.7874 - val_sparse_levenshtein_v1: 0.2110\n",
      "Epoch 12/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.7158 - accuracy: 0.7856 - sparse_levenshtein_v1: 0.2123 - val_loss: 0.6986 - val_accuracy: 0.7889 - val_sparse_levenshtein_v1: 0.2093\n",
      "Epoch 13/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.7126 - accuracy: 0.7867 - sparse_levenshtein_v1: 0.2112 - val_loss: 0.6957 - val_accuracy: 0.7902 - val_sparse_levenshtein_v1: 0.2079\n",
      "Epoch 14/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.7091 - accuracy: 0.7874 - sparse_levenshtein_v1: 0.2104 - val_loss: 0.6911 - val_accuracy: 0.7910 - val_sparse_levenshtein_v1: 0.2074\n",
      "Epoch 15/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.7063 - accuracy: 0.7881 - sparse_levenshtein_v1: 0.2098 - val_loss: 0.6886 - val_accuracy: 0.7920 - val_sparse_levenshtein_v1: 0.2068\n",
      "Epoch 16/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.7060 - accuracy: 0.7886 - sparse_levenshtein_v1: 0.2092 - val_loss: 0.6874 - val_accuracy: 0.7925 - val_sparse_levenshtein_v1: 0.2057\n",
      "Epoch 17/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.7008 - accuracy: 0.7901 - sparse_levenshtein_v1: 0.2078 - val_loss: 0.6839 - val_accuracy: 0.7939 - val_sparse_levenshtein_v1: 0.2049\n",
      "Epoch 18/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.6982 - accuracy: 0.7908 - sparse_levenshtein_v1: 0.2070 - val_loss: 0.6822 - val_accuracy: 0.7949 - val_sparse_levenshtein_v1: 0.2038\n",
      "Epoch 19/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.6951 - accuracy: 0.7917 - sparse_levenshtein_v1: 0.2061 - val_loss: 0.6776 - val_accuracy: 0.7957 - val_sparse_levenshtein_v1: 0.2031\n",
      "Epoch 20/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.6938 - accuracy: 0.7923 - sparse_levenshtein_v1: 0.2054 - val_loss: 0.6798 - val_accuracy: 0.7955 - val_sparse_levenshtein_v1: 0.2027\n",
      "Epoch 21/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.6909 - accuracy: 0.7928 - sparse_levenshtein_v1: 0.2050 - val_loss: 0.6757 - val_accuracy: 0.7965 - val_sparse_levenshtein_v1: 0.2020\n",
      "Epoch 22/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.6883 - accuracy: 0.7937 - sparse_levenshtein_v1: 0.2041 - val_loss: 0.6739 - val_accuracy: 0.7972 - val_sparse_levenshtein_v1: 0.2012\n",
      "Epoch 23/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.6884 - accuracy: 0.7936 - sparse_levenshtein_v1: 0.2043 - val_loss: 0.6720 - val_accuracy: 0.7975 - val_sparse_levenshtein_v1: 0.2007\n",
      "Epoch 24/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.6855 - accuracy: 0.7944 - sparse_levenshtein_v1: 0.2035 - val_loss: 0.6690 - val_accuracy: 0.7982 - val_sparse_levenshtein_v1: 0.1998\n",
      "Epoch 25/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.6836 - accuracy: 0.7951 - sparse_levenshtein_v1: 0.2028 - val_loss: 0.6675 - val_accuracy: 0.7986 - val_sparse_levenshtein_v1: 0.1996\n",
      "Epoch 26/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.6800 - accuracy: 0.7962 - sparse_levenshtein_v1: 0.2018 - val_loss: 0.6646 - val_accuracy: 0.7997 - val_sparse_levenshtein_v1: 0.1986\n",
      "Epoch 27/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.6773 - accuracy: 0.7968 - sparse_levenshtein_v1: 0.2010 - val_loss: 0.6620 - val_accuracy: 0.8000 - val_sparse_levenshtein_v1: 0.1982\n",
      "Epoch 28/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.6749 - accuracy: 0.7978 - sparse_levenshtein_v1: 0.2001 - val_loss: 0.6598 - val_accuracy: 0.8007 - val_sparse_levenshtein_v1: 0.1974\n",
      "Epoch 29/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.6727 - accuracy: 0.7985 - sparse_levenshtein_v1: 0.1994 - val_loss: 0.6570 - val_accuracy: 0.8018 - val_sparse_levenshtein_v1: 0.1963\n",
      "Epoch 30/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.6705 - accuracy: 0.7990 - sparse_levenshtein_v1: 0.1988 - val_loss: 0.6583 - val_accuracy: 0.8016 - val_sparse_levenshtein_v1: 0.1966\n",
      "Epoch 31/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.6687 - accuracy: 0.7997 - sparse_levenshtein_v1: 0.1982 - val_loss: 0.6569 - val_accuracy: 0.8020 - val_sparse_levenshtein_v1: 0.1964\n",
      "Epoch 32/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.6665 - accuracy: 0.8006 - sparse_levenshtein_v1: 0.1974 - val_loss: 0.6507 - val_accuracy: 0.8039 - val_sparse_levenshtein_v1: 0.1942\n",
      "Epoch 33/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.6641 - accuracy: 0.8014 - sparse_levenshtein_v1: 0.1965 - val_loss: 0.6502 - val_accuracy: 0.8046 - val_sparse_levenshtein_v1: 0.1937\n",
      "Epoch 34/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.6617 - accuracy: 0.8023 - sparse_levenshtein_v1: 0.1957 - val_loss: 0.6475 - val_accuracy: 0.8054 - val_sparse_levenshtein_v1: 0.1930\n",
      "Epoch 35/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.6594 - accuracy: 0.8030 - sparse_levenshtein_v1: 0.1949 - val_loss: 0.6458 - val_accuracy: 0.8061 - val_sparse_levenshtein_v1: 0.1923\n",
      "Epoch 36/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.6575 - accuracy: 0.8037 - sparse_levenshtein_v1: 0.1942 - val_loss: 0.6443 - val_accuracy: 0.8064 - val_sparse_levenshtein_v1: 0.1920\n",
      "Epoch 37/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.6553 - accuracy: 0.8046 - sparse_levenshtein_v1: 0.1933 - val_loss: 0.6400 - val_accuracy: 0.8081 - val_sparse_levenshtein_v1: 0.1902\n",
      "Epoch 38/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.6527 - accuracy: 0.8055 - sparse_levenshtein_v1: 0.1924 - val_loss: 0.6375 - val_accuracy: 0.8091 - val_sparse_levenshtein_v1: 0.1890\n",
      "Epoch 39/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.6500 - accuracy: 0.8067 - sparse_levenshtein_v1: 0.1913 - val_loss: 0.6360 - val_accuracy: 0.8096 - val_sparse_levenshtein_v1: 0.1885\n",
      "Epoch 40/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.6479 - accuracy: 0.8074 - sparse_levenshtein_v1: 0.1905 - val_loss: 0.6358 - val_accuracy: 0.8103 - val_sparse_levenshtein_v1: 0.1878\n",
      "Epoch 41/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.6454 - accuracy: 0.8085 - sparse_levenshtein_v1: 0.1894 - val_loss: 0.6309 - val_accuracy: 0.8112 - val_sparse_levenshtein_v1: 0.1871\n",
      "Epoch 42/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.6423 - accuracy: 0.8097 - sparse_levenshtein_v1: 0.1881 - val_loss: 0.6286 - val_accuracy: 0.8128 - val_sparse_levenshtein_v1: 0.1853\n",
      "Epoch 43/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.6397 - accuracy: 0.8108 - sparse_levenshtein_v1: 0.1870 - val_loss: 0.6254 - val_accuracy: 0.8143 - val_sparse_levenshtein_v1: 0.1835\n",
      "Epoch 44/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.6357 - accuracy: 0.8123 - sparse_levenshtein_v1: 0.1857 - val_loss: 0.6208 - val_accuracy: 0.8160 - val_sparse_levenshtein_v1: 0.1820\n",
      "Epoch 45/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.6333 - accuracy: 0.8133 - sparse_levenshtein_v1: 0.1846 - val_loss: 0.6207 - val_accuracy: 0.8161 - val_sparse_levenshtein_v1: 0.1818\n",
      "Epoch 46/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.6313 - accuracy: 0.8142 - sparse_levenshtein_v1: 0.1836 - val_loss: 0.6145 - val_accuracy: 0.8184 - val_sparse_levenshtein_v1: 0.1796\n",
      "Epoch 47/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.6277 - accuracy: 0.8155 - sparse_levenshtein_v1: 0.1823 - val_loss: 0.6107 - val_accuracy: 0.8200 - val_sparse_levenshtein_v1: 0.1784\n",
      "Epoch 48/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.6245 - accuracy: 0.8169 - sparse_levenshtein_v1: 0.1809 - val_loss: 0.6075 - val_accuracy: 0.8215 - val_sparse_levenshtein_v1: 0.1767\n",
      "Epoch 49/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.6209 - accuracy: 0.8182 - sparse_levenshtein_v1: 0.1797 - val_loss: 0.6045 - val_accuracy: 0.8229 - val_sparse_levenshtein_v1: 0.1754\n",
      "Epoch 50/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.6179 - accuracy: 0.8198 - sparse_levenshtein_v1: 0.1782 - val_loss: 0.6033 - val_accuracy: 0.8234 - val_sparse_levenshtein_v1: 0.1748\n",
      "Epoch 51/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.6152 - accuracy: 0.8208 - sparse_levenshtein_v1: 0.1772 - val_loss: 0.5970 - val_accuracy: 0.8260 - val_sparse_levenshtein_v1: 0.1723\n",
      "Epoch 52/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.6116 - accuracy: 0.8222 - sparse_levenshtein_v1: 0.1760 - val_loss: 0.5949 - val_accuracy: 0.8270 - val_sparse_levenshtein_v1: 0.1712\n",
      "Epoch 53/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.6074 - accuracy: 0.8238 - sparse_levenshtein_v1: 0.1743 - val_loss: 0.5899 - val_accuracy: 0.8281 - val_sparse_levenshtein_v1: 0.1700\n",
      "Epoch 54/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.6044 - accuracy: 0.8249 - sparse_levenshtein_v1: 0.1732 - val_loss: 0.5890 - val_accuracy: 0.8289 - val_sparse_levenshtein_v1: 0.1695\n",
      "Epoch 55/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.6023 - accuracy: 0.8257 - sparse_levenshtein_v1: 0.1724 - val_loss: 0.5877 - val_accuracy: 0.8297 - val_sparse_levenshtein_v1: 0.1686\n",
      "Epoch 56/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5982 - accuracy: 0.8271 - sparse_levenshtein_v1: 0.1710 - val_loss: 0.5799 - val_accuracy: 0.8320 - val_sparse_levenshtein_v1: 0.1666\n",
      "Epoch 57/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5957 - accuracy: 0.8282 - sparse_levenshtein_v1: 0.1701 - val_loss: 0.5817 - val_accuracy: 0.8315 - val_sparse_levenshtein_v1: 0.1671\n",
      "Epoch 58/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5939 - accuracy: 0.8290 - sparse_levenshtein_v1: 0.1693 - val_loss: 0.5773 - val_accuracy: 0.8327 - val_sparse_levenshtein_v1: 0.1659\n",
      "Epoch 59/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5907 - accuracy: 0.8301 - sparse_levenshtein_v1: 0.1683 - val_loss: 0.5738 - val_accuracy: 0.8341 - val_sparse_levenshtein_v1: 0.1645\n",
      "Epoch 60/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5883 - accuracy: 0.8307 - sparse_levenshtein_v1: 0.1677 - val_loss: 0.5727 - val_accuracy: 0.8339 - val_sparse_levenshtein_v1: 0.1647\n",
      "Epoch 61/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5866 - accuracy: 0.8314 - sparse_levenshtein_v1: 0.1669 - val_loss: 0.5709 - val_accuracy: 0.8348 - val_sparse_levenshtein_v1: 0.1638\n",
      "Epoch 62/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5832 - accuracy: 0.8327 - sparse_levenshtein_v1: 0.1658 - val_loss: 0.5678 - val_accuracy: 0.8360 - val_sparse_levenshtein_v1: 0.1627\n",
      "Epoch 63/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5823 - accuracy: 0.8329 - sparse_levenshtein_v1: 0.1655 - val_loss: 0.5661 - val_accuracy: 0.8368 - val_sparse_levenshtein_v1: 0.1618\n",
      "Epoch 64/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5792 - accuracy: 0.8341 - sparse_levenshtein_v1: 0.1644 - val_loss: 0.5637 - val_accuracy: 0.8371 - val_sparse_levenshtein_v1: 0.1618\n",
      "Epoch 65/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5768 - accuracy: 0.8347 - sparse_levenshtein_v1: 0.1638 - val_loss: 0.5650 - val_accuracy: 0.8369 - val_sparse_levenshtein_v1: 0.1619\n",
      "Epoch 66/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5751 - accuracy: 0.8354 - sparse_levenshtein_v1: 0.1632 - val_loss: 0.5651 - val_accuracy: 0.8366 - val_sparse_levenshtein_v1: 0.1621\n",
      "Epoch 67/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5742 - accuracy: 0.8357 - sparse_levenshtein_v1: 0.1628 - val_loss: 0.5601 - val_accuracy: 0.8393 - val_sparse_levenshtein_v1: 0.1596\n",
      "Epoch 68/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5716 - accuracy: 0.8364 - sparse_levenshtein_v1: 0.1622 - val_loss: 0.5560 - val_accuracy: 0.8405 - val_sparse_levenshtein_v1: 0.1582\n",
      "Epoch 69/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5705 - accuracy: 0.8369 - sparse_levenshtein_v1: 0.1618 - val_loss: 0.5543 - val_accuracy: 0.8409 - val_sparse_levenshtein_v1: 0.1580\n",
      "Epoch 70/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5689 - accuracy: 0.8373 - sparse_levenshtein_v1: 0.1613 - val_loss: 0.5584 - val_accuracy: 0.8393 - val_sparse_levenshtein_v1: 0.1594\n",
      "Epoch 71/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5667 - accuracy: 0.8381 - sparse_levenshtein_v1: 0.1606 - val_loss: 0.5529 - val_accuracy: 0.8414 - val_sparse_levenshtein_v1: 0.1574\n",
      "Epoch 72/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5653 - accuracy: 0.8387 - sparse_levenshtein_v1: 0.1599 - val_loss: 0.5542 - val_accuracy: 0.8410 - val_sparse_levenshtein_v1: 0.1580\n",
      "Epoch 73/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5629 - accuracy: 0.8396 - sparse_levenshtein_v1: 0.1591 - val_loss: 0.5524 - val_accuracy: 0.8414 - val_sparse_levenshtein_v1: 0.1575\n",
      "Epoch 74/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5630 - accuracy: 0.8392 - sparse_levenshtein_v1: 0.1595 - val_loss: 0.5519 - val_accuracy: 0.8416 - val_sparse_levenshtein_v1: 0.1572\n",
      "Epoch 75/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5605 - accuracy: 0.8401 - sparse_levenshtein_v1: 0.1586 - val_loss: 0.5472 - val_accuracy: 0.8432 - val_sparse_levenshtein_v1: 0.1557\n",
      "Epoch 76/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5594 - accuracy: 0.8406 - sparse_levenshtein_v1: 0.1582 - val_loss: 0.5509 - val_accuracy: 0.8421 - val_sparse_levenshtein_v1: 0.1569\n",
      "Epoch 77/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5583 - accuracy: 0.8407 - sparse_levenshtein_v1: 0.1580 - val_loss: 0.5475 - val_accuracy: 0.8431 - val_sparse_levenshtein_v1: 0.1558\n",
      "Epoch 78/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5578 - accuracy: 0.8411 - sparse_levenshtein_v1: 0.1577 - val_loss: 0.5454 - val_accuracy: 0.8442 - val_sparse_levenshtein_v1: 0.1547\n",
      "Epoch 79/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5559 - accuracy: 0.8415 - sparse_levenshtein_v1: 0.1573 - val_loss: 0.5455 - val_accuracy: 0.8443 - val_sparse_levenshtein_v1: 0.1546\n",
      "Epoch 80/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5563 - accuracy: 0.8414 - sparse_levenshtein_v1: 0.1575 - val_loss: 0.5463 - val_accuracy: 0.8440 - val_sparse_levenshtein_v1: 0.1548\n",
      "Epoch 81/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5528 - accuracy: 0.8425 - sparse_levenshtein_v1: 0.1563 - val_loss: 0.5457 - val_accuracy: 0.8444 - val_sparse_levenshtein_v1: 0.1548\n",
      "Epoch 82/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5516 - accuracy: 0.8429 - sparse_levenshtein_v1: 0.1560 - val_loss: 0.5440 - val_accuracy: 0.8445 - val_sparse_levenshtein_v1: 0.1545\n",
      "Epoch 83/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5505 - accuracy: 0.8431 - sparse_levenshtein_v1: 0.1558 - val_loss: 0.5477 - val_accuracy: 0.8426 - val_sparse_levenshtein_v1: 0.1564\n",
      "Epoch 84/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5509 - accuracy: 0.8430 - sparse_levenshtein_v1: 0.1558 - val_loss: 0.5487 - val_accuracy: 0.8428 - val_sparse_levenshtein_v1: 0.1561\n",
      "Epoch 85/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5490 - accuracy: 0.8437 - sparse_levenshtein_v1: 0.1552 - val_loss: 0.5376 - val_accuracy: 0.8468 - val_sparse_levenshtein_v1: 0.1522\n",
      "Epoch 86/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5469 - accuracy: 0.8444 - sparse_levenshtein_v1: 0.1545 - val_loss: 0.5426 - val_accuracy: 0.8451 - val_sparse_levenshtein_v1: 0.1539\n",
      "Epoch 87/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5457 - accuracy: 0.8446 - sparse_levenshtein_v1: 0.1543 - val_loss: 0.5433 - val_accuracy: 0.8445 - val_sparse_levenshtein_v1: 0.1545\n",
      "Epoch 88/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5458 - accuracy: 0.8448 - sparse_levenshtein_v1: 0.1541 - val_loss: 0.5407 - val_accuracy: 0.8455 - val_sparse_levenshtein_v1: 0.1534\n",
      "Epoch 89/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5442 - accuracy: 0.8452 - sparse_levenshtein_v1: 0.1538 - val_loss: 0.5445 - val_accuracy: 0.8439 - val_sparse_levenshtein_v1: 0.1550\n",
      "Epoch 90/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5439 - accuracy: 0.8453 - sparse_levenshtein_v1: 0.1537 - val_loss: 0.5375 - val_accuracy: 0.8463 - val_sparse_levenshtein_v1: 0.1529\n",
      "Epoch 91/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5434 - accuracy: 0.8456 - sparse_levenshtein_v1: 0.1534 - val_loss: 0.5350 - val_accuracy: 0.8470 - val_sparse_levenshtein_v1: 0.1521\n",
      "Epoch 92/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5423 - accuracy: 0.8460 - sparse_levenshtein_v1: 0.1530 - val_loss: 0.5309 - val_accuracy: 0.8486 - val_sparse_levenshtein_v1: 0.1504\n",
      "Epoch 93/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5408 - accuracy: 0.8464 - sparse_levenshtein_v1: 0.1526 - val_loss: 0.5311 - val_accuracy: 0.8484 - val_sparse_levenshtein_v1: 0.1504\n",
      "Epoch 94/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5400 - accuracy: 0.8466 - sparse_levenshtein_v1: 0.1524 - val_loss: 0.5339 - val_accuracy: 0.8475 - val_sparse_levenshtein_v1: 0.1516\n",
      "Epoch 95/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5385 - accuracy: 0.8470 - sparse_levenshtein_v1: 0.1520 - val_loss: 0.5334 - val_accuracy: 0.8478 - val_sparse_levenshtein_v1: 0.1515\n",
      "Epoch 96/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5380 - accuracy: 0.8472 - sparse_levenshtein_v1: 0.1518 - val_loss: 0.5312 - val_accuracy: 0.8486 - val_sparse_levenshtein_v1: 0.1504\n",
      "Epoch 97/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5372 - accuracy: 0.8475 - sparse_levenshtein_v1: 0.1515 - val_loss: 0.5374 - val_accuracy: 0.8461 - val_sparse_levenshtein_v1: 0.1532\n",
      "Epoch 98/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5374 - accuracy: 0.8476 - sparse_levenshtein_v1: 0.1514 - val_loss: 0.5299 - val_accuracy: 0.8487 - val_sparse_levenshtein_v1: 0.1503\n",
      "Epoch 99/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5354 - accuracy: 0.8480 - sparse_levenshtein_v1: 0.1510 - val_loss: 0.5364 - val_accuracy: 0.8462 - val_sparse_levenshtein_v1: 0.1528\n",
      "Epoch 100/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5353 - accuracy: 0.8482 - sparse_levenshtein_v1: 0.1508 - val_loss: 0.5263 - val_accuracy: 0.8498 - val_sparse_levenshtein_v1: 0.1493\n",
      "Epoch 101/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5336 - accuracy: 0.8487 - sparse_levenshtein_v1: 0.1503 - val_loss: 0.5250 - val_accuracy: 0.8505 - val_sparse_levenshtein_v1: 0.1488\n",
      "Epoch 102/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5331 - accuracy: 0.8489 - sparse_levenshtein_v1: 0.1500 - val_loss: 0.5257 - val_accuracy: 0.8504 - val_sparse_levenshtein_v1: 0.1489\n",
      "Epoch 103/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5323 - accuracy: 0.8492 - sparse_levenshtein_v1: 0.1498 - val_loss: 0.5383 - val_accuracy: 0.8456 - val_sparse_levenshtein_v1: 0.1534\n",
      "Epoch 104/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5330 - accuracy: 0.8490 - sparse_levenshtein_v1: 0.1500 - val_loss: 0.5324 - val_accuracy: 0.8487 - val_sparse_levenshtein_v1: 0.1504\n",
      "Epoch 105/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5328 - accuracy: 0.8490 - sparse_levenshtein_v1: 0.1500 - val_loss: 0.5251 - val_accuracy: 0.8510 - val_sparse_levenshtein_v1: 0.1481\n",
      "Epoch 106/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5317 - accuracy: 0.8494 - sparse_levenshtein_v1: 0.1496 - val_loss: 0.5268 - val_accuracy: 0.8500 - val_sparse_levenshtein_v1: 0.1492\n",
      "Epoch 107/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5297 - accuracy: 0.8500 - sparse_levenshtein_v1: 0.1490 - val_loss: 0.5191 - val_accuracy: 0.8528 - val_sparse_levenshtein_v1: 0.1462\n",
      "Epoch 108/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5296 - accuracy: 0.8501 - sparse_levenshtein_v1: 0.1490 - val_loss: 0.5228 - val_accuracy: 0.8513 - val_sparse_levenshtein_v1: 0.1479\n",
      "Epoch 109/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5282 - accuracy: 0.8505 - sparse_levenshtein_v1: 0.1485 - val_loss: 0.5216 - val_accuracy: 0.8515 - val_sparse_levenshtein_v1: 0.1476\n",
      "Epoch 110/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5276 - accuracy: 0.8508 - sparse_levenshtein_v1: 0.1483 - val_loss: 0.5183 - val_accuracy: 0.8524 - val_sparse_levenshtein_v1: 0.1467\n",
      "Epoch 111/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5272 - accuracy: 0.8507 - sparse_levenshtein_v1: 0.1483 - val_loss: 0.5195 - val_accuracy: 0.8527 - val_sparse_levenshtein_v1: 0.1466\n",
      "Epoch 112/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5273 - accuracy: 0.8507 - sparse_levenshtein_v1: 0.1483 - val_loss: 0.5182 - val_accuracy: 0.8532 - val_sparse_levenshtein_v1: 0.1458\n",
      "Epoch 113/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5254 - accuracy: 0.8514 - sparse_levenshtein_v1: 0.1476 - val_loss: 0.5181 - val_accuracy: 0.8528 - val_sparse_levenshtein_v1: 0.1462\n",
      "Epoch 114/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5257 - accuracy: 0.8511 - sparse_levenshtein_v1: 0.1479 - val_loss: 0.5184 - val_accuracy: 0.8524 - val_sparse_levenshtein_v1: 0.1469\n",
      "Epoch 115/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5251 - accuracy: 0.8515 - sparse_levenshtein_v1: 0.1476 - val_loss: 0.5163 - val_accuracy: 0.8536 - val_sparse_levenshtein_v1: 0.1456\n",
      "Epoch 116/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5242 - accuracy: 0.8519 - sparse_levenshtein_v1: 0.1472 - val_loss: 0.5147 - val_accuracy: 0.8542 - val_sparse_levenshtein_v1: 0.1451\n",
      "Epoch 117/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5238 - accuracy: 0.8519 - sparse_levenshtein_v1: 0.1472 - val_loss: 0.5194 - val_accuracy: 0.8528 - val_sparse_levenshtein_v1: 0.1463\n",
      "Epoch 118/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5229 - accuracy: 0.8520 - sparse_levenshtein_v1: 0.1471 - val_loss: 0.5162 - val_accuracy: 0.8531 - val_sparse_levenshtein_v1: 0.1461\n",
      "Epoch 119/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5228 - accuracy: 0.8522 - sparse_levenshtein_v1: 0.1468 - val_loss: 0.5159 - val_accuracy: 0.8538 - val_sparse_levenshtein_v1: 0.1457\n",
      "Epoch 120/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5218 - accuracy: 0.8525 - sparse_levenshtein_v1: 0.1466 - val_loss: 0.5166 - val_accuracy: 0.8536 - val_sparse_levenshtein_v1: 0.1455\n",
      "Epoch 121/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5208 - accuracy: 0.8529 - sparse_levenshtein_v1: 0.1463 - val_loss: 0.5218 - val_accuracy: 0.8519 - val_sparse_levenshtein_v1: 0.1477\n",
      "Epoch 122/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5226 - accuracy: 0.8522 - sparse_levenshtein_v1: 0.1468 - val_loss: 0.5150 - val_accuracy: 0.8540 - val_sparse_levenshtein_v1: 0.1451\n",
      "Epoch 123/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5202 - accuracy: 0.8530 - sparse_levenshtein_v1: 0.1460 - val_loss: 0.5179 - val_accuracy: 0.8532 - val_sparse_levenshtein_v1: 0.1461\n",
      "Epoch 124/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5204 - accuracy: 0.8530 - sparse_levenshtein_v1: 0.1461 - val_loss: 0.5154 - val_accuracy: 0.8540 - val_sparse_levenshtein_v1: 0.1451\n",
      "Epoch 125/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5187 - accuracy: 0.8535 - sparse_levenshtein_v1: 0.1457 - val_loss: 0.5253 - val_accuracy: 0.8507 - val_sparse_levenshtein_v1: 0.1486\n",
      "Epoch 126/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5187 - accuracy: 0.8535 - sparse_levenshtein_v1: 0.1457 - val_loss: 0.5143 - val_accuracy: 0.8545 - val_sparse_levenshtein_v1: 0.1447\n",
      "Epoch 127/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5190 - accuracy: 0.8534 - sparse_levenshtein_v1: 0.1457 - val_loss: 0.5164 - val_accuracy: 0.8537 - val_sparse_levenshtein_v1: 0.1454\n",
      "Epoch 128/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5189 - accuracy: 0.8534 - sparse_levenshtein_v1: 0.1457 - val_loss: 0.5181 - val_accuracy: 0.8528 - val_sparse_levenshtein_v1: 0.1465\n",
      "Epoch 129/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5185 - accuracy: 0.8535 - sparse_levenshtein_v1: 0.1456 - val_loss: 0.5163 - val_accuracy: 0.8535 - val_sparse_levenshtein_v1: 0.1457\n",
      "Epoch 130/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5166 - accuracy: 0.8542 - sparse_levenshtein_v1: 0.1449 - val_loss: 0.5133 - val_accuracy: 0.8545 - val_sparse_levenshtein_v1: 0.1447\n",
      "Epoch 131/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5168 - accuracy: 0.8541 - sparse_levenshtein_v1: 0.1451 - val_loss: 0.5139 - val_accuracy: 0.8552 - val_sparse_levenshtein_v1: 0.1442\n",
      "Epoch 132/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5168 - accuracy: 0.8542 - sparse_levenshtein_v1: 0.1450 - val_loss: 0.5110 - val_accuracy: 0.8553 - val_sparse_levenshtein_v1: 0.1437\n",
      "Epoch 133/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5162 - accuracy: 0.8542 - sparse_levenshtein_v1: 0.1449 - val_loss: 0.5135 - val_accuracy: 0.8547 - val_sparse_levenshtein_v1: 0.1446\n",
      "Epoch 134/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5149 - accuracy: 0.8548 - sparse_levenshtein_v1: 0.1443 - val_loss: 0.5189 - val_accuracy: 0.8532 - val_sparse_levenshtein_v1: 0.1459\n",
      "Epoch 135/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5152 - accuracy: 0.8547 - sparse_levenshtein_v1: 0.1445 - val_loss: 0.5117 - val_accuracy: 0.8549 - val_sparse_levenshtein_v1: 0.1443\n",
      "Epoch 136/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5143 - accuracy: 0.8549 - sparse_levenshtein_v1: 0.1442 - val_loss: 0.5094 - val_accuracy: 0.8559 - val_sparse_levenshtein_v1: 0.1431\n",
      "Epoch 137/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5140 - accuracy: 0.8550 - sparse_levenshtein_v1: 0.1442 - val_loss: 0.5101 - val_accuracy: 0.8559 - val_sparse_levenshtein_v1: 0.1432\n",
      "Epoch 138/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5134 - accuracy: 0.8551 - sparse_levenshtein_v1: 0.1440 - val_loss: 0.5121 - val_accuracy: 0.8550 - val_sparse_levenshtein_v1: 0.1441\n",
      "Epoch 139/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5132 - accuracy: 0.8552 - sparse_levenshtein_v1: 0.1439 - val_loss: 0.5131 - val_accuracy: 0.8552 - val_sparse_levenshtein_v1: 0.1442\n",
      "Epoch 140/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5127 - accuracy: 0.8555 - sparse_levenshtein_v1: 0.1437 - val_loss: 0.5066 - val_accuracy: 0.8570 - val_sparse_levenshtein_v1: 0.1421\n",
      "Epoch 141/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5125 - accuracy: 0.8554 - sparse_levenshtein_v1: 0.1437 - val_loss: 0.5106 - val_accuracy: 0.8560 - val_sparse_levenshtein_v1: 0.1432\n",
      "Epoch 142/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5125 - accuracy: 0.8554 - sparse_levenshtein_v1: 0.1438 - val_loss: 0.5103 - val_accuracy: 0.8566 - val_sparse_levenshtein_v1: 0.1425\n",
      "Epoch 143/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5115 - accuracy: 0.8556 - sparse_levenshtein_v1: 0.1435 - val_loss: 0.5058 - val_accuracy: 0.8574 - val_sparse_levenshtein_v1: 0.1419\n",
      "Epoch 144/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5113 - accuracy: 0.8558 - sparse_levenshtein_v1: 0.1433 - val_loss: 0.5119 - val_accuracy: 0.8551 - val_sparse_levenshtein_v1: 0.1441\n",
      "Epoch 145/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5107 - accuracy: 0.8561 - sparse_levenshtein_v1: 0.1431 - val_loss: 0.5071 - val_accuracy: 0.8568 - val_sparse_levenshtein_v1: 0.1424\n",
      "Epoch 146/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5110 - accuracy: 0.8560 - sparse_levenshtein_v1: 0.1432 - val_loss: 0.5095 - val_accuracy: 0.8557 - val_sparse_levenshtein_v1: 0.1435\n",
      "Epoch 147/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5094 - accuracy: 0.8563 - sparse_levenshtein_v1: 0.1428 - val_loss: 0.5050 - val_accuracy: 0.8575 - val_sparse_levenshtein_v1: 0.1417\n",
      "Epoch 148/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5089 - accuracy: 0.8565 - sparse_levenshtein_v1: 0.1426 - val_loss: 0.5104 - val_accuracy: 0.8556 - val_sparse_levenshtein_v1: 0.1435\n",
      "Epoch 149/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5102 - accuracy: 0.8561 - sparse_levenshtein_v1: 0.1431 - val_loss: 0.5152 - val_accuracy: 0.8549 - val_sparse_levenshtein_v1: 0.1441\n",
      "Epoch 150/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5082 - accuracy: 0.8569 - sparse_levenshtein_v1: 0.1423 - val_loss: 0.5053 - val_accuracy: 0.8577 - val_sparse_levenshtein_v1: 0.1414\n",
      "Epoch 151/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5088 - accuracy: 0.8565 - sparse_levenshtein_v1: 0.1426 - val_loss: 0.5059 - val_accuracy: 0.8577 - val_sparse_levenshtein_v1: 0.1414\n",
      "Epoch 152/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5071 - accuracy: 0.8569 - sparse_levenshtein_v1: 0.1422 - val_loss: 0.5034 - val_accuracy: 0.8579 - val_sparse_levenshtein_v1: 0.1413\n",
      "Epoch 153/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5090 - accuracy: 0.8564 - sparse_levenshtein_v1: 0.1427 - val_loss: 0.5060 - val_accuracy: 0.8576 - val_sparse_levenshtein_v1: 0.1416\n",
      "Epoch 154/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5091 - accuracy: 0.8566 - sparse_levenshtein_v1: 0.1426 - val_loss: 0.5039 - val_accuracy: 0.8578 - val_sparse_levenshtein_v1: 0.1415\n",
      "Epoch 155/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5086 - accuracy: 0.8566 - sparse_levenshtein_v1: 0.1425 - val_loss: 0.5026 - val_accuracy: 0.8580 - val_sparse_levenshtein_v1: 0.1410\n",
      "Epoch 156/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5068 - accuracy: 0.8571 - sparse_levenshtein_v1: 0.1421 - val_loss: 0.5046 - val_accuracy: 0.8580 - val_sparse_levenshtein_v1: 0.1411\n",
      "Epoch 157/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5067 - accuracy: 0.8571 - sparse_levenshtein_v1: 0.1420 - val_loss: 0.5080 - val_accuracy: 0.8564 - val_sparse_levenshtein_v1: 0.1428\n",
      "Epoch 158/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5071 - accuracy: 0.8571 - sparse_levenshtein_v1: 0.1421 - val_loss: 0.5016 - val_accuracy: 0.8586 - val_sparse_levenshtein_v1: 0.1408\n",
      "Epoch 159/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5057 - accuracy: 0.8574 - sparse_levenshtein_v1: 0.1418 - val_loss: 0.5039 - val_accuracy: 0.8581 - val_sparse_levenshtein_v1: 0.1413\n",
      "Epoch 160/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5063 - accuracy: 0.8573 - sparse_levenshtein_v1: 0.1418 - val_loss: 0.5020 - val_accuracy: 0.8585 - val_sparse_levenshtein_v1: 0.1406\n",
      "Epoch 161/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5070 - accuracy: 0.8569 - sparse_levenshtein_v1: 0.1423 - val_loss: 0.5018 - val_accuracy: 0.8584 - val_sparse_levenshtein_v1: 0.1408\n",
      "Epoch 162/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5053 - accuracy: 0.8576 - sparse_levenshtein_v1: 0.1415 - val_loss: 0.5076 - val_accuracy: 0.8566 - val_sparse_levenshtein_v1: 0.1426\n",
      "Epoch 163/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5063 - accuracy: 0.8574 - sparse_levenshtein_v1: 0.1418 - val_loss: 0.5049 - val_accuracy: 0.8581 - val_sparse_levenshtein_v1: 0.1411\n",
      "Epoch 164/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5057 - accuracy: 0.8576 - sparse_levenshtein_v1: 0.1415 - val_loss: 0.4984 - val_accuracy: 0.8600 - val_sparse_levenshtein_v1: 0.1391\n",
      "Epoch 165/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5032 - accuracy: 0.8582 - sparse_levenshtein_v1: 0.1409 - val_loss: 0.5005 - val_accuracy: 0.8589 - val_sparse_levenshtein_v1: 0.1405\n",
      "Epoch 166/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5042 - accuracy: 0.8579 - sparse_levenshtein_v1: 0.1413 - val_loss: 0.5006 - val_accuracy: 0.8594 - val_sparse_levenshtein_v1: 0.1398\n",
      "Epoch 167/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5048 - accuracy: 0.8577 - sparse_levenshtein_v1: 0.1414 - val_loss: 0.5024 - val_accuracy: 0.8584 - val_sparse_levenshtein_v1: 0.1411\n",
      "Epoch 168/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5048 - accuracy: 0.8579 - sparse_levenshtein_v1: 0.1413 - val_loss: 0.5052 - val_accuracy: 0.8575 - val_sparse_levenshtein_v1: 0.1419\n",
      "Epoch 169/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5050 - accuracy: 0.8577 - sparse_levenshtein_v1: 0.1415 - val_loss: 0.5091 - val_accuracy: 0.8570 - val_sparse_levenshtein_v1: 0.1423\n",
      "Epoch 170/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5065 - accuracy: 0.8573 - sparse_levenshtein_v1: 0.1419 - val_loss: 0.5033 - val_accuracy: 0.8587 - val_sparse_levenshtein_v1: 0.1404\n",
      "Epoch 171/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5037 - accuracy: 0.8580 - sparse_levenshtein_v1: 0.1412 - val_loss: 0.5053 - val_accuracy: 0.8578 - val_sparse_levenshtein_v1: 0.1416\n",
      "Epoch 172/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5046 - accuracy: 0.8578 - sparse_levenshtein_v1: 0.1415 - val_loss: 0.5004 - val_accuracy: 0.8592 - val_sparse_levenshtein_v1: 0.1399\n",
      "Epoch 173/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5036 - accuracy: 0.8582 - sparse_levenshtein_v1: 0.1410 - val_loss: 0.5017 - val_accuracy: 0.8591 - val_sparse_levenshtein_v1: 0.1401\n",
      "Epoch 174/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.5025 - accuracy: 0.8584 - sparse_levenshtein_v1: 0.1407 - val_loss: 0.5059 - val_accuracy: 0.8572 - val_sparse_levenshtein_v1: 0.1421\n",
      "validation levenshtein distance: 0.21152471005916595\n",
      "Best hyperparameters: {'attention_heads': 6, 'learning_rate': 0.0006269092262292042, 'drop_out_out_decoder': 0.05, 'dense_layers': 5, 'drop_out': 0.30000000000000004, 'encoder_kernel_size': 11}\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.4984 - accuracy: 0.8600 - sparse_levenshtein_v1: 0.1391\n",
      "Metrics in Validation: [0.4984012544155121, 0.8599989414215088, 0.13911764323711395]\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "trials = study.best_trials\n",
    "\n",
    "for index, trial in enumerate(trials):\n",
    "    print(f\"Best model: {index+1}\")\n",
    "\n",
    "    model = build_prod_transformer_model_v2(trial=trial)\n",
    "\n",
    "    model.build([(None, MAX_LENGHT_SOURCE, int(FEATURE_COLUMNS.shape[0]/2)), (None, TARGET_MAX_LENGHT)])\n",
    "\n",
    "    print(model.summary())\n",
    "    model.fit(train_dataset, validation_data=val_dataset, epochs=EPOCHS, callbacks=get_predefine_callbacks(model_name=MODEL_NAME, patience=10))\n",
    "   \n",
    "    print('validation levenshtein distance: {}'.format(trial.value))\n",
    "    print(\"Best hyperparameters: {}\".format(trial.params))\n",
    "\n",
    "    model.load_weights(f\"../best_model/prototype/{MODEL_NAME}\")\n",
    "\n",
    "    print(f\"Metrics in Validation: {model.evaluate(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_utils.dataset import char_to_num, num_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_sequence = [char_to_num[w] for w in [\"<\"]]\n",
    "\n",
    "# for batch_index, batch in enumerate(val_dataset):\n",
    "#     batch = batch[0]\n",
    "\n",
    "#     sources = batch[0] #batch[\"source\"]\n",
    "#     targets = batch[1] #batch[\"target\"]\n",
    "    \n",
    "#     print(sources.shape)\n",
    "#     print(targets.shape)\n",
    "\n",
    "#     for index_sample, (source, target) in enumerate(zip(sources, targets)):\n",
    "#         source = tf.expand_dims(source, axis=0)\n",
    "#         target_sequence = [char_to_num[w] for w in [\"<\"]]\n",
    "#         y_true = \"\".join([num_to_char[w] for w in target.numpy()])\n",
    "    \n",
    "#         for i in range(TARGET_MAX_LENGHT):\n",
    "#             next_token = tf.expand_dims(tf.pad(tf.constant(target_sequence),\n",
    "#              [[0, TARGET_MAX_LENGHT-len(target_sequence)]],\n",
    "#               mode='CONSTANT',\n",
    "#                constant_values=pad_token_idx,\n",
    "#                 name=None),\n",
    "#                  axis=0)\n",
    "\n",
    "#             print(\"next target sequence to predict: \", next_token)\n",
    "#             y_pred = model((source, next_token))\n",
    "\n",
    "#             y_pred = tf.cast(tf.argmax(y_pred, axis=2), dtype=tf.int32)\n",
    "\n",
    "#             print(\"argmax:\", y_pred)\n",
    "\n",
    "#             mask = tf.not_equal(y_pred, pad_token_idx)\n",
    "#             next_token = y_pred[mask][-1].numpy()\n",
    "\n",
    "#             target_sequence.append(next_token)\n",
    "\n",
    "#             print(\"sequence so far: \", \"\".join([num_to_char[w] for w in target_sequence]))\n",
    "#             print(\"Label: \", y_true)\n",
    "\n",
    "#             if num_to_char[next_token]==\">\":\n",
    "#                 break\n",
    "\n",
    "#         print(f\"================================={index_sample}=========================================\")\n",
    "#         if index_sample==1:\n",
    "#             break\n",
    "\n",
    "#     if batch_index==1:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"finger_spelling_v2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " landmark_embedding_v2 (Lan  multiple                  89984     \n",
      " dmarkEmbeddingV2)                                               \n",
      "                                                                 \n",
      " basic_positional_embedding  multiple                  8064      \n",
      " s (BasicPositionalEmbeddin                                      \n",
      " gs)                                                             \n",
      "                                                                 \n",
      " transformer_encoder (Trans  multiple                  51207     \n",
      " formerEncoder)                                                  \n",
      "                                                                 \n",
      " transformer_decoder (Trans  multiple                  102093    \n",
      " formerDecoder)                                                  \n",
      "                                                                 \n",
      " dropout (Dropout)           multiple                  0         \n",
      "                                                                 \n",
      " sequential_2 (Sequential)   (None, None, 52)          14404     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         multiple                  0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             multiple                  3286      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 269038 (1.03 MB)\n",
      "Trainable params: 269038 (1.03 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/prod_v2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/prod_v2/assets\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "\n",
    "model.save(f\"../models/{MODEL_NAME}\", save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFLiteModel(tf.Module):\n",
    "    def __init__(self, model):\n",
    "        super(TFLiteModel, self).__init__()\n",
    "        self.target_start_token_idx = start_token_idx\n",
    "        self.target_end_token_idx = end_token_idx\n",
    "        # Load the feature generation and main models\n",
    "        self.model = model\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=[None, FEATURE_COLUMNS.shape[0]], dtype=tf.float32, name='inputs')])\n",
    "    def __call__(self, inputs, training=False):\n",
    "        # Preprocess Data\n",
    "        x = tf.cast(inputs, tf.float32)\n",
    "\n",
    "        x = x[None]\n",
    "\n",
    "        x = tf.cond(tf.shape(x)[1] == 0, lambda: tf.zeros((1, 1, FEATURE_COLUMNS.shape[0])), lambda: tf.identity(x))\n",
    "\n",
    "        x = x[0]\n",
    "\n",
    "        x = pre_process(x)\n",
    "        #shape after [MAX_LENGHT_SOURCE, FEATURE_SIZE]\n",
    "\n",
    "        x = x[None]\n",
    "\n",
    "        x = self.model.generate(x)\n",
    "\n",
    "        x = x[0]\n",
    "        idx = tf.argmax(tf.cast(tf.equal(x, self.target_end_token_idx), tf.int32))\n",
    "        idx = tf.where(tf.math.less(idx, 1), tf.constant(2, dtype=tf.int64), idx)\n",
    "        x = x[1:idx]\n",
    "\n",
    "        x = tf.one_hot(x, 59)\n",
    "        return {\"outputs\": x}\n",
    "\n",
    "tflitemodel_base = TFLiteModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp_d6nmpai/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp_d6nmpai/assets\n"
     ]
    }
   ],
   "source": [
    "keras_model_converter = tf.lite.TFLiteConverter.from_keras_model(tflitemodel_base)\n",
    "keras_model_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "keras_model_converter.allow_custom_ops = True\n",
    "keras_model_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "tflite_model = keras_model_converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# with open('/kaggle/working/model.tflite', 'wb') as f:\n",
    "with open(\"../models/model.tflite\", \"wb\") as f:    \n",
    "    f.write(tflite_model)\n",
    "\n",
    "infargs = {\"selected_columns\" : list(FEATURE_COLUMNS)}\n",
    "\n",
    "# with open(\"inference_args.json\", \"w\") as json_file:\n",
    "with open(\"../models/inference_args.json\", \"w\") as json_file:\n",
    "    json.dump(infargs, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: ../models/model.tflite (deflated 78%)\n",
      "updating: ../models/inference_args.json (deflated 84%)\n"
     ]
    }
   ],
   "source": [
    "!zip submission.zip  '../models/model.tflite' '../models/inference_args.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from batch 1\n",
    "\n",
    "source_batch, target_batch = next(iter(val_dataset))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated:  www.curre\n",
      "target:  144-421-3778\n",
      "generated:  4494 bricre roort da\n",
      "target:  annemarie vance\n",
      "generated:  +655-10-15-1-35\n",
      "target:  805283 little bald branch road\n",
      "generated:  999\n",
      "target:  423730 west tobacco road\n",
      "generated:  www\n",
      "target:  533-348-6983\n",
      "generated:  briri cerr\n",
      "target:  www.elg4ml.com\n",
      "generated:  www.cecrasar\n",
      "target:  lydia mullins\n",
      "generated:  brie brer\n",
      "target:  8107 tallagson lane northeast\n",
      "generated:  sirie comans\n",
      "target:  921 marc avenue\n",
      "generated:  999 s fir\n",
      "target:  completos66.rssing.com/8902\n",
      "generated:  476 bre brer\n",
      "target:  graphemica.com/\n",
      "generated:  www.frercorce\n",
      "target:  +7-1975-49\n",
      "generated:  4547 co routcor\n",
      "target:  www.runfengweixiu.com\n",
      "generated:  brociri crisa\n",
      "target:  848-463-2595\n",
      "generated:  ribarrie ru\n",
      "target:  +61-928-216\n",
      "generated:  www.frer\n",
      "target:  376 russell county\n",
      "generated:  brerr cerr\n",
      "target:  jerod cardenas\n",
      "generated:  www.frrice\n",
      "target:  iecoevent.com/thisandthatbajan\n",
      "generated:  riri\n",
      "target:  +44-0923-79-920\n",
      "generated:  979 leice\n",
      "target:  327-392-6606\n",
      "generated:  5995 co 2115\n",
      "target:  7240 usfs 5605\n",
      "generated:  www.cririce\n",
      "target:  8561 haags ln\n",
      "generated:  www.frer\n",
      "target:  www.chdesign.idv.tw/hoshina_\n",
      "generated:  www.frier\n",
      "target:  4542 west fountain street\n",
      "generated:  mira sace\n",
      "target:  +95-2136-510\n",
      "generated:  www.comres.com/\n",
      "target:  dion_celine\n",
      "generated:  499 frrer\n",
      "target:  +236-0133-2567-16-4802\n",
      "generated:  rirri brinter\n",
      "target:  264 e river vista cir\n",
      "generated:  www.frier\n",
      "target:  +31-93-290-57\n",
      "generated:  9444 frigs roort da\n",
      "target:  /projektmanagement-grundlagen\n",
      "generated:  9195 south\n",
      "target:  hiptankrecords.com\n",
      "generated:  466-434-2499\n",
      "target:  75 alder st nw\n",
      "generated:  934 brar airss\n",
      "target:  espacenature.aveyron.fr/\n",
      "generated:  hrer\n",
      "target:  www.52jf.com/\n",
      "generated:  www.crecerssace\n",
      "target:  chirurgie/meaning-punjabi\n",
      "generated:  brirra cor\n",
      "target:  456-007-0028\n",
      "generated:  www.ferrer\n",
      "target:  cassandra lucero\n",
      "generated:  9099 co corct\n",
      "target:  ffarsi.ir/impacts\n",
      "generated:  bririe fran\n",
      "target:  j-o-s-schoten/tour-pak-luggage\n",
      "generated:  frer\n",
      "target:  3391 mount laurel court\n",
      "generated:  mirre \n",
      "target:  3709 stering forest road\n",
      "generated:  499 frer cose\n",
      "target:  dayna maxwell\n",
      "generated:  www.frierfi\n",
      "target:  www.douglas.hu/mauroszeta\n",
      "generated:  www.coms-astan\n",
      "target:  kelvin pope\n",
      "generated:  wuctercorcorce\n",
      "target:  marginededbf/pepalerts\n",
      "generated:  frerre bram road\n",
      "target:  2169 herman nelson highway\n",
      "generated:  ririri crin\n",
      "target:  4994 chn tun jose\n",
      "generated:  brurrin crie\n",
      "target:  447-186-6606\n",
      "generated:  brirfa 949\n",
      "target:  www.beaucaire.fr\n",
      "generated:  www.cerinla\n",
      "target:  962564 ara drive\n",
      "generated:  4495 w bartct\n",
      "target:  567827 e 97th st n\n",
      "generated:  4971 braall all\n",
      "target:  rick wilkins\n",
      "generated:  www/wordercorcor\n",
      "target:  www.hanguoman.com/hillel\n",
      "generated:  www.cerrer\n",
      "target:  271484 partridge hills\n",
      "generated:  449 bro roe aaa\n",
      "target:  www.ov-ermeloo.nl/wolf_sphere\n",
      "generated:  www.frer\n",
      "target:  799-127-1498\n",
      "generated:  www.comrasas\n",
      "target:  7349 kutz bridge\n",
      "generated:  www.comice\n",
      "target:  ru.hentai-cosplays.com\n",
      "generated:  brerre brer\n",
      "target:  jenifer potts\n",
      "generated:  www.cerraastan\n",
      "target:  www.chazelles.com\n",
      "generated:  www.frifer\n",
      "target:  465 ecksminster st\n",
      "generated:  445 bre rood courcor\n",
      "target:  262-552-8812\n",
      "generated:  7449 bre roor\n",
      "target:  www.koinecoopsociale.it\n",
      "generated:  www\n",
      "target:  bitcoincryptoadvice.com\n",
      "generated:  bren brer cord\n",
      "target:  6296 lawrence 2072\n",
      "generated:  rierrin com\n",
      "target:  faith holmes\n",
      "generated:  www.corre\n",
      "target:  conchiapas/59248\n",
      "generated:  www.fribrer\n",
      "target:  dominique parsons\n",
      "generated:  899 fricra roort da\n",
      "target:  www.jujiaotaizhou.com\n",
      "generated:  www.frier\n",
      "target:  maiu.sch.id/pohlednice\n",
      "generated:  4495 mare \n",
      "target:  196 private road 1715\n",
      "generated:  8744 craas coot\n",
      "target:  jace shields\n",
      "generated:  9144 w co brint lan\n",
      "target:  toriclinic.ru/more-promotions\n",
      "generated:  brocir firic rd\n",
      "target:  mamilove.com.tw/covers-aspar/\n",
      "generated:  9409 rod co road\n",
      "target:  3838 town hwy 1963\n",
      "generated:  fruce brerr\n",
      "target:  6617 reishe\n",
      "generated:  bruck brisa\n",
      "target:  708-617-6743\n",
      "generated:  969 frich\n",
      "target:  6866 rustic way drive\n",
      "generated:  748 bruck\n",
      "target:  262 pvt rd 4287\n",
      "generated:  brecre frasa\n",
      "target:  9492 southwest 6th lane\n",
      "generated:  www\n",
      "target:  4826 weyers court\n",
      "generated:  www.mcner\n",
      "target:  160 joseph ayers\n",
      "generated:  www.sider\n",
      "target:  cody mccann\n",
      "generated:  www.secerlees.com\n",
      "target:  3453 albequerque\n",
      "generated:  9304 brercot 11155\n",
      "target:  4372 howes pine view drive\n",
      "generated:  www.fricirin\n",
      "target:  333-555-7707\n",
      "generated:  www\n",
      "target:  8392 morlynn\n",
      "generated:  www.mulrille\n",
      "target:  bloise/calle-heno.html\n",
      "generated:  www.frrice\n",
      "target:  uni-fitt.by\n",
      "generated:  frrer frar cree sa\n",
      "target:  354085 co r 219\n",
      "generated:  www.friri\n",
      "target:  900 281st east\n",
      "generated:  9144 bructis\n",
      "target:  320-024-0184\n",
      "generated:  411 brid coad\n",
      "target:  22 co c070\n",
      "generated:  orher\n",
      "target:  231-568-6056\n",
      "generated:  mirra far\n",
      "target:  927-108-0338\n",
      "generated:  499 branter 111155\n",
      "target:  elias wiley\n",
      "generated:  bron cerrid ave\n",
      "target:  999 east 24th avenue\n",
      "generated:  brirribra racre road\n",
      "target:  9682 northeast pecan drive\n",
      "generated:  mirar\n",
      "target:  master_c54/die-werkstatt\n",
      "generated:  www.frier\n",
      "target:  www.stationerystore.bm\n",
      "generated:  briri sran\n",
      "target:  eiji2704eiji.exblog.jp/\n",
      "generated:  www.frar\n",
      "target:  powerbi/medyum-ali-gurses\n",
      "generated:  9343 dalle\n",
      "target:  7544 196\n",
      "generated:  rirrin crer\n",
      "target:  76566 tauber ranch\n",
      "generated:  9245 brod cour road\n",
      "target:  www.education.auburn.edu\n",
      "generated:  lebri frans\n",
      "target:  www.pu-haha.com\n",
      "generated:  www.corris\n",
      "target:  5023 hai\n",
      "generated:  brer\n",
      "target:  7955 deletha\n",
      "generated:  494 e \n",
      "target:  +41-2627-0127\n",
      "generated:  ririr frida\n",
      "target:  4971 hurt road southeast\n",
      "generated:  www\n",
      "target:  8855 parker road west\n",
      "generated:  www.corcarca\n",
      "target:  8111 cyrpess barn\n",
      "generated:  www.cofrerra\n",
      "target:  2028 pso alegre\n",
      "generated:  brerri frin\n",
      "target:  173-129-1297\n",
      "generated:  bririfri\n",
      "target:  +31-82-03-027-848\n",
      "generated:  cririn fran\n",
      "target:  +51-93-32-62-18-375\n",
      "generated:  www.frifra\n",
      "target:  6 cabacier\n",
      "generated:  49 merre road\n",
      "target:  6905 jerry garlington road\n",
      "generated:  729 co hortce roort\n",
      "target:  678-418-5009\n",
      "generated:  www.frrer\n",
      "target:  9708 horseshoe harbor\n",
      "generated:  980 frid fe\n",
      "target:  alexandria virginia\n",
      "generated:  www.corcor\n",
      "target:  6914 0040\n",
      "generated:  mifrri com\n",
      "target:  2394 knox road 670\n",
      "generated:  www.cerrfer\n",
      "target:  5044 gee tipton drive\n",
      "generated:  ririe wer\n",
      "target:  pizzariakirinos/surreal-cereal\n",
      "generated:  www.comlere\n",
      "target:  793-825-6369\n",
      "generated:  www.coms\n",
      "target:  /562940\n",
      "generated:  www.com\n",
      "target:  4924 north 3rd avenue\n"
     ]
    }
   ],
   "source": [
    "REQUIRED_SIGNATURE = \"serving_default\"\n",
    "REQUIRED_OUTPUT = \"outputs\"\n",
    "\n",
    "# interpreter = tf.lite.Interpreter(\"model.tflite\")\n",
    "interpreter = tf.lite.Interpreter(\"../models/model.tflite\")\n",
    "\n",
    "# with open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n",
    "with open (\"../data/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n",
    "    character_map = json.load(f)\n",
    "\n",
    "rev_character_map = {j:i for i,j in character_map.items()}\n",
    "found_signatures = list(interpreter.get_signature_list().keys())\n",
    "\n",
    "if REQUIRED_SIGNATURE not in found_signatures:\n",
    "    raise KernelEvalException('Required input signature not found.')\n",
    "\n",
    "prediction_fn = interpreter.get_signature_runner(REQUIRED_SIGNATURE)\n",
    "\n",
    "prediction_str = \"\"\n",
    "for source_element, target_element in zip(source_batch, target_batch):\n",
    "    # print(tf.expand_dims(target_element, axis=0).numpy())\n",
    "\n",
    "    output = prediction_fn(inputs=source_element)\n",
    "\n",
    "    # print(output[REQUIRED_OUTPUT])\n",
    "\n",
    "    # break\n",
    "\n",
    "    print(\"generated: \", \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output[REQUIRED_OUTPUT], axis=1)]))\n",
    "    print(\"target: \", \"\".join([rev_character_map.get(s, \"\") for s in target_element.numpy()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'serving_default_inputs:0',\n",
       "  'index': 0,\n",
       "  'shape': array([128,  52], dtype=int32),\n",
       "  'shape_signature': array([ -1, 104], dtype=int32),\n",
       "  'dtype': numpy.float32,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
       "   'zero_points': array([], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpreter.get_input_details()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fingerspelling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
