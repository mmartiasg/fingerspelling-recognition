{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'FEATURES_SIZE' from 'src.constants' (/Users/matiasgonzalez/workspace/fingerspelling-recognition/src/constants.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconstants\u001b[39;00m \u001b[39mimport\u001b[39;00m TARGET_MAX_LENGHT, MAX_LENGHT_SOURCE\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata_utils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataset\u001b[39;00m \u001b[39mimport\u001b[39;00m build_datset_train_val, VOCAB_SIZE, LHAND_IDX, LHAND_IDX, start_token_idx, end_token_idx, pre_process, pad_token_idx, FEATURE_COLUMNS\n\u001b[0;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprod_models\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbuilder\u001b[39;00m \u001b[39mimport\u001b[39;00m build_prod_transformer_model_v2\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallbacks\u001b[39;00m \u001b[39mimport\u001b[39;00m get_predefine_callbacks\n\u001b[1;32m     11\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39moptuna\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/fingerspelling-recognition/src/prod_models/builder.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconstants\u001b[39;00m \u001b[39mimport\u001b[39;00m MAX_LENGHT_SOURCE, FEATURES_SIZE, LATENT_DIMS, DIM_EMBEDDINGS, TARGET_MAX_LENGHT, ATTENTION_HEADS\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata_utils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataset\u001b[39;00m \u001b[39mimport\u001b[39;00m VOCAB_SIZE, FEATURE_COLUMNS\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmath\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'FEATURES_SIZE' from 'src.constants' (/Users/matiasgonzalez/workspace/fingerspelling-recognition/src/constants.py)"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "\n",
    "from src.constants import TARGET_MAX_LENGHT, MAX_LENGHT_SOURCE\n",
    "from src.data_utils.dataset import build_datset_train_val, VOCAB_SIZE, LHAND_IDX, LHAND_IDX, start_token_idx, end_token_idx, pre_process, pad_token_idx, FEATURE_COLUMNS\n",
    "from src.prod_models.builder import build_prod_transformer_model_v2\n",
    "from src.callbacks import get_predefine_callbacks\n",
    "import optuna\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "TRIALS = 15\n",
    "EPOCHS = 5000\n",
    "EPOCHS_PER_TRIAL = 10\n",
    "BATCH_SIZE = 128\n",
    "TRAIN_SPLIT = 0.8\n",
    "MODEL_NAME = \"prod_v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_COLUMNS.shape[0]/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = build_datset_train_val(split=TRAIN_SPLIT, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = build_prod_transformer_model_v2(trial=trial)\n",
    "    model.build([(None, MAX_LENGHT_SOURCE, int(FEATURE_COLUMNS.shape[0]/2)), (None, TARGET_MAX_LENGHT)])\n",
    "    model.fit(train_dataset, validation_data=val_dataset, epochs=EPOCHS_PER_TRIAL, callbacks=get_predefine_callbacks(model_name=MODEL_NAME, patience=3), verbose=0)\n",
    "    levenshtein = model.evaluate(val_dataset)[-1]\n",
    "\n",
    "    return  levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=TRIALS, gc_after_trial=True, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = study.best_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "trials = study.best_trials\n",
    "\n",
    "for index, trial in enumerate(trials):\n",
    "    print(f\"Best model: {index+1}\")\n",
    "\n",
    "    model = build_prod_transformer_model_v2(trial=trial)\n",
    "\n",
    "    model.build([(None, MAX_LENGHT_SOURCE, int(FEATURE_COLUMNS.shape[0]/2)), (None, TARGET_MAX_LENGHT)])\n",
    "\n",
    "    print(model.summary())\n",
    "    model.fit(train_dataset, validation_data=val_dataset, epochs=EPOCHS, callbacks=get_predefine_callbacks(model_name=MODEL_NAME, patience=10))\n",
    "   \n",
    "    print('validation levenshtein distance: {}'.format(trial.value))\n",
    "    print(\"Best hyperparameters: {}\".format(trial.params))\n",
    "\n",
    "    model.load_weights(f\"../best_model/prototype/{MODEL_NAME}\")\n",
    "\n",
    "    print(f\"Metrics in Validation: {model.evaluate(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_utils.dataset import char_to_num, num_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_sequence = [char_to_num[w] for w in [\"<\"]]\n",
    "\n",
    "# for batch_index, batch in enumerate(val_dataset):\n",
    "#     batch = batch[0]\n",
    "\n",
    "#     sources = batch[0] #batch[\"source\"]\n",
    "#     targets = batch[1] #batch[\"target\"]\n",
    "    \n",
    "#     print(sources.shape)\n",
    "#     print(targets.shape)\n",
    "\n",
    "#     for index_sample, (source, target) in enumerate(zip(sources, targets)):\n",
    "#         source = tf.expand_dims(source, axis=0)\n",
    "#         target_sequence = [char_to_num[w] for w in [\"<\"]]\n",
    "#         y_true = \"\".join([num_to_char[w] for w in target.numpy()])\n",
    "    \n",
    "#         for i in range(TARGET_MAX_LENGHT):\n",
    "#             next_token = tf.expand_dims(tf.pad(tf.constant(target_sequence),\n",
    "#              [[0, TARGET_MAX_LENGHT-len(target_sequence)]],\n",
    "#               mode='CONSTANT',\n",
    "#                constant_values=pad_token_idx,\n",
    "#                 name=None),\n",
    "#                  axis=0)\n",
    "\n",
    "#             print(\"next target sequence to predict: \", next_token)\n",
    "#             y_pred = model((source, next_token))\n",
    "\n",
    "#             y_pred = tf.cast(tf.argmax(y_pred, axis=2), dtype=tf.int32)\n",
    "\n",
    "#             print(\"argmax:\", y_pred)\n",
    "\n",
    "#             mask = tf.not_equal(y_pred, pad_token_idx)\n",
    "#             next_token = y_pred[mask][-1].numpy()\n",
    "\n",
    "#             target_sequence.append(next_token)\n",
    "\n",
    "#             print(\"sequence so far: \", \"\".join([num_to_char[w] for w in target_sequence]))\n",
    "#             print(\"Label: \", y_true)\n",
    "\n",
    "#             if num_to_char[next_token]==\">\":\n",
    "#                 break\n",
    "\n",
    "#         print(f\"================================={index_sample}=========================================\")\n",
    "#         if index_sample==1:\n",
    "#             break\n",
    "\n",
    "#     if batch_index==1:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "\n",
    "model.save(f\"../models/{MODEL_NAME}\", save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFLiteModel(tf.Module):\n",
    "    def __init__(self, model):\n",
    "        super(TFLiteModel, self).__init__()\n",
    "        self.target_start_token_idx = start_token_idx\n",
    "        self.target_end_token_idx = end_token_idx\n",
    "        # Load the feature generation and main models\n",
    "        self.model = model\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=[None, FEATURE_COLUMNS.shape[0]], dtype=tf.float32, name='inputs')])\n",
    "    def __call__(self, inputs, training=False):\n",
    "        # Preprocess Data\n",
    "        x = tf.cast(inputs, tf.float32)\n",
    "\n",
    "        x = x[None]\n",
    "\n",
    "        x = tf.cond(tf.shape(x)[1] == 0, lambda: tf.zeros((1, 1, FEATURE_COLUMNS.shape[0])), lambda: tf.identity(x))\n",
    "\n",
    "        x = x[0]\n",
    "\n",
    "        x = pre_process(x)\n",
    "        #shape after [MAX_LENGHT_SOURCE, FEATURE_SIZE]\n",
    "\n",
    "        x = x[None]\n",
    "\n",
    "        x = self.model.generate(x)\n",
    "\n",
    "        x = x[0]\n",
    "        idx = tf.argmax(tf.cast(tf.equal(x, self.target_end_token_idx), tf.int32))\n",
    "        idx = tf.where(tf.math.less(idx, 1), tf.constant(2, dtype=tf.int64), idx)\n",
    "        x = x[1:idx]\n",
    "\n",
    "        x = tf.one_hot(x, 59)\n",
    "        return {\"outputs\": x}\n",
    "\n",
    "tflitemodel_base = TFLiteModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model_converter = tf.lite.TFLiteConverter.from_keras_model(tflitemodel_base)\n",
    "keras_model_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "keras_model_converter.allow_custom_ops = True\n",
    "keras_model_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "tflite_model = keras_model_converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# with open('/kaggle/working/model.tflite', 'wb') as f:\n",
    "with open(\"../models/model.tflite\", \"wb\") as f:    \n",
    "    f.write(tflite_model)\n",
    "\n",
    "infargs = {\"selected_columns\" : list(FEATURE_COLUMNS)}\n",
    "\n",
    "# with open(\"inference_args.json\", \"w\") as json_file:\n",
    "with open(\"../models/inference_args.json\", \"w\") as json_file:\n",
    "    json.dump(infargs, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip submission.zip  '../models/model.tflite' '../models/inference_args.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from batch 1\n",
    "\n",
    "source_batch, target_batch = next(iter(val_dataset))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REQUIRED_SIGNATURE = \"serving_default\"\n",
    "REQUIRED_OUTPUT = \"outputs\"\n",
    "\n",
    "# interpreter = tf.lite.Interpreter(\"model.tflite\")\n",
    "interpreter = tf.lite.Interpreter(\"../models/model.tflite\")\n",
    "\n",
    "# with open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n",
    "with open (\"../data/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n",
    "    character_map = json.load(f)\n",
    "\n",
    "rev_character_map = {j:i for i,j in character_map.items()}\n",
    "found_signatures = list(interpreter.get_signature_list().keys())\n",
    "\n",
    "if REQUIRED_SIGNATURE not in found_signatures:\n",
    "    raise KernelEvalException('Required input signature not found.')\n",
    "\n",
    "prediction_fn = interpreter.get_signature_runner(REQUIRED_SIGNATURE)\n",
    "\n",
    "prediction_str = \"\"\n",
    "for source_element, target_element in zip(source_batch, target_batch):\n",
    "    # print(tf.expand_dims(target_element, axis=0).numpy())\n",
    "\n",
    "    output = prediction_fn(inputs=source_element)\n",
    "\n",
    "    # print(output[REQUIRED_OUTPUT])\n",
    "\n",
    "    # break\n",
    "\n",
    "    print(\"generated: \", \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output[REQUIRED_OUTPUT], axis=1)]))\n",
    "    print(\"target: \", \"\".join([rev_character_map.get(s, \"\") for s in target_element.numpy()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.get_input_details()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fingerspelling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
