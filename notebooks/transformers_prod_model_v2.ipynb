{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "from src.constants import TARGET_MAX_LENGHT, MAX_LENGHT_SOURCE\n",
    "from src.data_utils.dataset import build_datset_train_val, VOCAB_SIZE, LHAND_IDX, LHAND_IDX, start_token_idx, end_token_idx, pre_process, pad_token_idx, FEATURE_COLUMNS\n",
    "from src.prod_models.builder import build_prod_transformer_model_v2\n",
    "from src.callbacks import get_predefine_callbacks\n",
    "import optuna\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "TRIALS = 200\n",
    "EPOCHS = 5000\n",
    "EPOCHS_PER_TRIAL = 15\n",
    "BATCH_SIZE = 128\n",
    "TRAIN_SPLIT = 0.8\n",
    "MODEL_NAME = \"prod_v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train split: 28160 | val split: 6656\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset = build_datset_train_val(split=TRAIN_SPLIT, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = build_prod_transformer_model_v2(trial=trial)\n",
    "    model.build([(None, MAX_LENGHT_SOURCE, int(FEATURE_COLUMNS.shape[0]/2)), (None, TARGET_MAX_LENGHT)])\n",
    "    model.fit(train_dataset, validation_data=val_dataset, epochs=EPOCHS_PER_TRIAL, callbacks=get_predefine_callbacks(model_name=MODEL_NAME, patience=3), verbose=0)\n",
    "    levenshtein = model.evaluate(val_dataset)[-1]\n",
    "\n",
    "    return  levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-09-01 16:10:47,910] A new study created in memory with name: no-name-4e63ebef-6817-471a-88f9-3201a07fdb0e\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61dfbbefca414b40bf77b824802ec977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 1s 11ms/step - loss: 0.8188 - accuracy: 0.7599 - sparse_levenshtein_v1: 0.2363\n",
      "[I 2023-09-01 16:11:45,539] Trial 0 finished with value: 0.23633567988872528 and parameters: {'attention_heads': 5, 'learning_rate': 0.02496233768447454, 'drop_out_out_decoder': 0.15000000000000002, 'encoder_kernel_size': 5}. Best is trial 0 with value: 0.23633567988872528.\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 0.7497 - accuracy: 0.7780 - sparse_levenshtein_v1: 0.2200\n",
      "[I 2023-09-01 16:13:51,208] Trial 1 finished with value: 0.2200479805469513 and parameters: {'attention_heads': 5, 'learning_rate': 5.5321478946485995e-05, 'drop_out_out_decoder': 0.15000000000000002, 'encoder_kernel_size': 12}. Best is trial 1 with value: 0.2200479805469513.\n",
      "77/77 [==============================] - 1s 14ms/step - loss: 0.6586 - accuracy: 0.8022 - sparse_levenshtein_v1: 0.1959\n",
      "[I 2023-09-01 16:16:25,157] Trial 2 finished with value: 0.19589154422283173 and parameters: {'attention_heads': 8, 'learning_rate': 0.00033072793276787645, 'drop_out_out_decoder': 0.2, 'encoder_kernel_size': 12}. Best is trial 2 with value: 0.19589154422283173.\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 1.0460 - accuracy: 0.7230 - sparse_levenshtein_v1: 0.2770\n",
      "[I 2023-09-01 16:17:11,478] Trial 3 finished with value: 0.2770122289657593 and parameters: {'attention_heads': 3, 'learning_rate': 0.04080091061498132, 'drop_out_out_decoder': 0.45, 'encoder_kernel_size': 11}. Best is trial 2 with value: 0.19589154422283173.\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.6665 - accuracy: 0.8038 - sparse_levenshtein_v1: 0.1940\n",
      "[I 2023-09-01 16:18:38,095] Trial 4 finished with value: 0.19397340714931488 and parameters: {'attention_heads': 1, 'learning_rate': 0.00035777507083120213, 'drop_out_out_decoder': 0.4, 'encoder_kernel_size': 6}. Best is trial 4 with value: 0.19397340714931488.\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 0.6147 - accuracy: 0.8148 - sparse_levenshtein_v1: 0.1838\n",
      "[I 2023-09-01 16:20:53,124] Trial 5 finished with value: 0.18377727270126343 and parameters: {'attention_heads': 6, 'learning_rate': 0.0008954221966040291, 'drop_out_out_decoder': 0.05, 'encoder_kernel_size': 11}. Best is trial 5 with value: 0.18377727270126343.\n",
      "77/77 [==============================] - 1s 14ms/step - loss: 0.6543 - accuracy: 0.8054 - sparse_levenshtein_v1: 0.1922\n",
      "[I 2023-09-01 16:23:27,038] Trial 6 finished with value: 0.19224973022937775 and parameters: {'attention_heads': 8, 'learning_rate': 0.00020925535726316544, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 7}. Best is trial 5 with value: 0.18377727270126343.\n",
      "77/77 [==============================] - 1s 14ms/step - loss: 0.8206 - accuracy: 0.7666 - sparse_levenshtein_v1: 0.2290\n",
      "[I 2023-09-01 16:25:59,296] Trial 7 finished with value: 0.2289569228887558 and parameters: {'attention_heads': 8, 'learning_rate': 1.4514340763499224e-05, 'drop_out_out_decoder': 0.30000000000000004, 'encoder_kernel_size': 4}. Best is trial 5 with value: 0.18377727270126343.\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.7452 - accuracy: 0.7798 - sparse_levenshtein_v1: 0.2188\n",
      "[I 2023-09-01 16:27:27,131] Trial 8 finished with value: 0.21881705522537231 and parameters: {'attention_heads': 1, 'learning_rate': 8.175514702464524e-05, 'drop_out_out_decoder': 0.25, 'encoder_kernel_size': 11}. Best is trial 5 with value: 0.18377727270126343.\n",
      "77/77 [==============================] - 1s 14ms/step - loss: 0.6051 - accuracy: 0.8186 - sparse_levenshtein_v1: 0.1803\n",
      "[I 2023-09-01 16:29:59,847] Trial 9 finished with value: 0.18025143444538116 and parameters: {'attention_heads': 8, 'learning_rate': 0.0045904717673952156, 'drop_out_out_decoder': 0.4, 'encoder_kernel_size': 3}. Best is trial 9 with value: 0.18025143444538116.\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.7004 - accuracy: 0.7892 - sparse_levenshtein_v1: 0.2097\n",
      "[I 2023-09-01 16:31:40,721] Trial 10 finished with value: 0.20971813797950745 and parameters: {'attention_heads': 3, 'learning_rate': 0.010443829882850286, 'drop_out_out_decoder': 0.5, 'encoder_kernel_size': 3}. Best is trial 9 with value: 0.18025143444538116.\n",
      "77/77 [==============================] - 2s 29ms/step - loss: 0.5486 - accuracy: 0.8378 - sparse_levenshtein_v1: 0.1614\n",
      "[I 2023-09-01 16:35:31,152] Trial 11 finished with value: 0.16142608225345612 and parameters: {'attention_heads': 6, 'learning_rate': 0.0031920356597724044, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 8}. Best is trial 11 with value: 0.16142608225345612.\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 0.6736 - accuracy: 0.7958 - sparse_levenshtein_v1: 0.2032\n",
      "[I 2023-09-01 16:39:00,016] Trial 12 finished with value: 0.20323462784290314 and parameters: {'attention_heads': 7, 'learning_rate': 0.0043126171400799445, 'drop_out_out_decoder': 0.35000000000000003, 'encoder_kernel_size': 9}. Best is trial 11 with value: 0.16142608225345612.\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 0.6234 - accuracy: 0.8095 - sparse_levenshtein_v1: 0.1900\n",
      "[I 2023-09-01 16:41:05,234] Trial 13 finished with value: 0.19001589715480804 and parameters: {'attention_heads': 6, 'learning_rate': 0.0030908315566818374, 'drop_out_out_decoder': 0.1, 'encoder_kernel_size': 8}. Best is trial 11 with value: 0.16142608225345612.\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 0.5237 - accuracy: 0.8470 - sparse_levenshtein_v1: 0.1524\n",
      "[I 2023-09-01 16:43:19,936] Trial 14 finished with value: 0.1523890644311905 and parameters: {'attention_heads': 7, 'learning_rate': 0.002285611759679821, 'drop_out_out_decoder': 0.35000000000000003, 'encoder_kernel_size': 9}. Best is trial 14 with value: 0.1523890644311905.\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 0.8827 - accuracy: 0.7485 - sparse_levenshtein_v1: 0.2442\n",
      "[I 2023-09-01 16:45:17,608] Trial 15 finished with value: 0.2442030906677246 and parameters: {'attention_heads': 6, 'learning_rate': 0.06744528556230169, 'drop_out_out_decoder': 0.30000000000000004, 'encoder_kernel_size': 9}. Best is trial 14 with value: 0.1523890644311905.\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 0.5819 - accuracy: 0.8281 - sparse_levenshtein_v1: 0.1709\n",
      "[I 2023-09-01 16:47:08,846] Trial 16 finished with value: 0.1708775758743286 and parameters: {'attention_heads': 4, 'learning_rate': 0.0015063693111885149, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 9}. Best is trial 14 with value: 0.1523890644311905.\n",
      "77/77 [==============================] - 1s 14ms/step - loss: 0.7832 - accuracy: 0.7681 - sparse_levenshtein_v1: 0.2280\n",
      "[I 2023-09-01 16:48:21,205] Trial 17 finished with value: 0.22801679372787476 and parameters: {'attention_heads': 7, 'learning_rate': 0.020314877478155635, 'drop_out_out_decoder': 0.25, 'encoder_kernel_size': 7}. Best is trial 14 with value: 0.1523890644311905.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.6822 - accuracy: 0.7931 - sparse_levenshtein_v1: 0.2061\n",
      "[I 2023-09-01 16:50:47,359] Trial 18 finished with value: 0.20612983405590057 and parameters: {'attention_heads': 7, 'learning_rate': 0.009534345590378586, 'drop_out_out_decoder': 0.35000000000000003, 'encoder_kernel_size': 8}. Best is trial 14 with value: 0.1523890644311905.\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.8811 - accuracy: 0.7472 - sparse_levenshtein_v1: 0.2500\n",
      "[I 2023-09-01 16:52:26,239] Trial 19 finished with value: 0.2500244081020355 and parameters: {'attention_heads': 4, 'learning_rate': 0.09732237745118974, 'drop_out_out_decoder': 0.1, 'encoder_kernel_size': 10}. Best is trial 14 with value: 0.1523890644311905.\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 0.5919 - accuracy: 0.8246 - sparse_levenshtein_v1: 0.1742\n",
      "[I 2023-09-01 16:54:31,315] Trial 20 finished with value: 0.1741509884595871 and parameters: {'attention_heads': 6, 'learning_rate': 0.0012988879288477783, 'drop_out_out_decoder': 0.5, 'encoder_kernel_size': 6}. Best is trial 14 with value: 0.1523890644311905.\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.5144 - accuracy: 0.8504 - sparse_levenshtein_v1: 0.1483\n",
      "[I 2023-09-01 16:56:18,464] Trial 21 finished with value: 0.1483256220817566 and parameters: {'attention_heads': 4, 'learning_rate': 0.0015780329505647513, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 9}. Best is trial 21 with value: 0.1483256220817566.\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.4971 - accuracy: 0.8560 - sparse_levenshtein_v1: 0.1431\n",
      "[I 2023-09-01 16:57:56,586] Trial 22 finished with value: 0.143096461892128 and parameters: {'attention_heads': 3, 'learning_rate': 0.002250619150352973, 'drop_out_out_decoder': 0.05, 'encoder_kernel_size': 10}. Best is trial 22 with value: 0.143096461892128.\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.6019 - accuracy: 0.8223 - sparse_levenshtein_v1: 0.1760\n",
      "[I 2023-09-01 16:59:24,991] Trial 23 finished with value: 0.1759924739599228 and parameters: {'attention_heads': 2, 'learning_rate': 0.0009226571521550932, 'drop_out_out_decoder': 0.05, 'encoder_kernel_size': 10}. Best is trial 22 with value: 0.143096461892128.\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.5663 - accuracy: 0.8332 - sparse_levenshtein_v1: 0.1655\n",
      "[I 2023-09-01 17:01:03,405] Trial 24 finished with value: 0.16551347076892853 and parameters: {'attention_heads': 3, 'learning_rate': 0.001994828046543757, 'drop_out_out_decoder': 0.15000000000000002, 'encoder_kernel_size': 10}. Best is trial 22 with value: 0.143096461892128.\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.6734 - accuracy: 0.7968 - sparse_levenshtein_v1: 0.2019\n",
      "[I 2023-09-01 17:02:32,256] Trial 25 finished with value: 0.20188359916210175 and parameters: {'attention_heads': 2, 'learning_rate': 0.0006054013085965606, 'drop_out_out_decoder': 0.05, 'encoder_kernel_size': 9}. Best is trial 22 with value: 0.143096461892128.\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.6718 - accuracy: 0.7958 - sparse_levenshtein_v1: 0.2029\n",
      "[I 2023-09-01 17:04:19,061] Trial 26 finished with value: 0.20293664932250977 and parameters: {'attention_heads': 4, 'learning_rate': 0.008494689610828225, 'drop_out_out_decoder': 0.1, 'encoder_kernel_size': 10}. Best is trial 22 with value: 0.143096461892128.\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.5821 - accuracy: 0.8301 - sparse_levenshtein_v1: 0.1688\n",
      "[I 2023-09-01 17:05:47,370] Trial 27 finished with value: 0.16877827048301697 and parameters: {'attention_heads': 2, 'learning_rate': 0.0020480438879239525, 'drop_out_out_decoder': 0.2, 'encoder_kernel_size': 9}. Best is trial 22 with value: 0.143096461892128.\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 0.5954 - accuracy: 0.8250 - sparse_levenshtein_v1: 0.1728\n",
      "[I 2023-09-01 17:07:44,440] Trial 28 finished with value: 0.1727677583694458 and parameters: {'attention_heads': 5, 'learning_rate': 0.0005636236227037762, 'drop_out_out_decoder': 0.05, 'encoder_kernel_size': 7}. Best is trial 22 with value: 0.143096461892128.\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 0.5372 - accuracy: 0.8422 - sparse_levenshtein_v1: 0.1570\n",
      "[I 2023-09-01 17:09:42,854] Trial 29 finished with value: 0.15695199370384216 and parameters: {'attention_heads': 5, 'learning_rate': 0.001967267396484796, 'drop_out_out_decoder': 0.15000000000000002, 'encoder_kernel_size': 11}. Best is trial 22 with value: 0.143096461892128.\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.6794 - accuracy: 0.7939 - sparse_levenshtein_v1: 0.2053\n",
      "[I 2023-09-01 17:11:08,420] Trial 30 finished with value: 0.2053435891866684 and parameters: {'attention_heads': 3, 'learning_rate': 0.006777354250888746, 'drop_out_out_decoder': 0.2, 'encoder_kernel_size': 8}. Best is trial 22 with value: 0.143096461892128.\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 0.5308 - accuracy: 0.8444 - sparse_levenshtein_v1: 0.1547\n",
      "[I 2023-09-01 17:13:06,725] Trial 31 finished with value: 0.15473587810993195 and parameters: {'attention_heads': 5, 'learning_rate': 0.002554919503298436, 'drop_out_out_decoder': 0.15000000000000002, 'encoder_kernel_size': 11}. Best is trial 22 with value: 0.143096461892128.\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.5739 - accuracy: 0.8312 - sparse_levenshtein_v1: 0.1678\n",
      "[I 2023-09-01 17:14:53,806] Trial 32 finished with value: 0.16780662536621094 and parameters: {'attention_heads': 4, 'learning_rate': 0.0030169329950190844, 'drop_out_out_decoder': 0.1, 'encoder_kernel_size': 10}. Best is trial 22 with value: 0.143096461892128.\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.7568 - accuracy: 0.7738 - sparse_levenshtein_v1: 0.2232\n",
      "[I 2023-09-01 17:16:51,076] Trial 33 finished with value: 0.22317172586917877 and parameters: {'attention_heads': 5, 'learning_rate': 0.01633913889579806, 'drop_out_out_decoder': 0.15000000000000002, 'encoder_kernel_size': 12}. Best is trial 22 with value: 0.143096461892128.\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 0.6689 - accuracy: 0.7964 - sparse_levenshtein_v1: 0.2028\n",
      "[I 2023-09-01 17:18:44,997] Trial 34 finished with value: 0.20278967916965485 and parameters: {'attention_heads': 4, 'learning_rate': 0.005794125438015921, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 12}. Best is trial 22 with value: 0.143096461892128.\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.6189 - accuracy: 0.8139 - sparse_levenshtein_v1: 0.1848\n",
      "[I 2023-09-01 17:20:34,584] Trial 35 finished with value: 0.1848035603761673 and parameters: {'attention_heads': 3, 'learning_rate': 0.0013328970993064751, 'drop_out_out_decoder': 0.05, 'encoder_kernel_size': 11}. Best is trial 22 with value: 0.143096461892128.\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 0.6061 - accuracy: 0.8183 - sparse_levenshtein_v1: 0.1805\n",
      "[I 2023-09-01 17:22:44,316] Trial 36 finished with value: 0.18046347796916962 and parameters: {'attention_heads': 5, 'learning_rate': 0.0027482532103873846, 'drop_out_out_decoder': 0.30000000000000004, 'encoder_kernel_size': 11}. Best is trial 22 with value: 0.143096461892128.\n",
      "77/77 [==============================] - 1s 14ms/step - loss: 0.6324 - accuracy: 0.8097 - sparse_levenshtein_v1: 0.1892\n",
      "[I 2023-09-01 17:25:11,593] Trial 37 finished with value: 0.1891997754573822 and parameters: {'attention_heads': 7, 'learning_rate': 0.0008283635945768737, 'drop_out_out_decoder': 0.4, 'encoder_kernel_size': 10}. Best is trial 22 with value: 0.143096461892128.\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.6777 - accuracy: 0.7956 - sparse_levenshtein_v1: 0.2033\n",
      "[I 2023-09-01 17:26:52,190] Trial 38 finished with value: 0.20331090688705444 and parameters: {'attention_heads': 3, 'learning_rate': 0.00042231181860915564, 'drop_out_out_decoder': 0.2, 'encoder_kernel_size': 12}. Best is trial 22 with value: 0.143096461892128.\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.6717 - accuracy: 0.7994 - sparse_levenshtein_v1: 0.1991\n",
      "[I 2023-09-01 17:28:20,719] Trial 39 finished with value: 0.1990612894296646 and parameters: {'attention_heads': 2, 'learning_rate': 0.00020649078096526282, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 9}. Best is trial 22 with value: 0.143096461892128.\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.6690 - accuracy: 0.7963 - sparse_levenshtein_v1: 0.2029\n",
      "[I 2023-09-01 17:30:06,209] Trial 40 finished with value: 0.2029077559709549 and parameters: {'attention_heads': 4, 'learning_rate': 0.00480463447103685, 'drop_out_out_decoder': 0.35000000000000003, 'encoder_kernel_size': 6}. Best is trial 22 with value: 0.143096461892128.\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 0.5688 - accuracy: 0.8309 - sparse_levenshtein_v1: 0.1681\n",
      "[I 2023-09-01 17:32:04,606] Trial 41 finished with value: 0.16814225912094116 and parameters: {'attention_heads': 5, 'learning_rate': 0.0018788432084632157, 'drop_out_out_decoder': 0.15000000000000002, 'encoder_kernel_size': 11}. Best is trial 22 with value: 0.143096461892128.\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 0.6206 - accuracy: 0.8124 - sparse_levenshtein_v1: 0.1863\n",
      "[I 2023-09-01 17:34:03,172] Trial 42 finished with value: 0.1862667053937912 and parameters: {'attention_heads': 5, 'learning_rate': 0.001175486859172521, 'drop_out_out_decoder': 0.1, 'encoder_kernel_size': 11}. Best is trial 22 with value: 0.143096461892128.\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 0.6003 - accuracy: 0.8230 - sparse_levenshtein_v1: 0.1758\n",
      "[I 2023-09-01 17:36:00,623] Trial 43 finished with value: 0.17576028406620026 and parameters: {'attention_heads': 5, 'learning_rate': 0.0022473942323915164, 'drop_out_out_decoder': 0.45, 'encoder_kernel_size': 10}. Best is trial 22 with value: 0.143096461892128.\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 0.6514 - accuracy: 0.8011 - sparse_levenshtein_v1: 0.1982\n",
      "[I 2023-09-01 17:38:07,339] Trial 44 finished with value: 0.1982479840517044 and parameters: {'attention_heads': 6, 'learning_rate': 0.004327986445628637, 'drop_out_out_decoder': 0.2, 'encoder_kernel_size': 11}. Best is trial 22 with value: 0.143096461892128.\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.5391 - accuracy: 0.8428 - sparse_levenshtein_v1: 0.1561\n",
      "[I 2023-09-01 17:39:54,859] Trial 45 finished with value: 0.15613701939582825 and parameters: {'attention_heads': 4, 'learning_rate': 0.0011556783421039546, 'drop_out_out_decoder': 0.05, 'encoder_kernel_size': 12}. Best is trial 22 with value: 0.143096461892128.\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.6515 - accuracy: 0.8022 - sparse_levenshtein_v1: 0.1968\n",
      "[I 2023-09-01 17:41:42,197] Trial 46 finished with value: 0.196829691529274 and parameters: {'attention_heads': 4, 'learning_rate': 0.0007242932388974767, 'drop_out_out_decoder': 0.05, 'encoder_kernel_size': 12}. Best is trial 22 with value: 0.143096461892128.\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.6448 - accuracy: 0.8048 - sparse_levenshtein_v1: 0.1942\n",
      "[I 2023-09-01 17:43:21,152] Trial 47 finished with value: 0.19422878324985504 and parameters: {'attention_heads': 3, 'learning_rate': 0.0009412487209337969, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 12}. Best is trial 22 with value: 0.143096461892128.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5277 - accuracy: 0.8455 - sparse_levenshtein_v1: 0.1540\n",
      "[I 2023-09-01 17:45:45,329] Trial 48 finished with value: 0.15400652587413788 and parameters: {'attention_heads': 8, 'learning_rate': 0.0030329457911091873, 'drop_out_out_decoder': 0.05, 'encoder_kernel_size': 9}. Best is trial 22 with value: 0.143096461892128.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.4966 - accuracy: 0.8537 - sparse_levenshtein_v1: 0.1459\n",
      "[I 2023-09-01 17:48:08,431] Trial 49 finished with value: 0.14593589305877686 and parameters: {'attention_heads': 8, 'learning_rate': 0.003599334312542076, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 8}. Best is trial 22 with value: 0.143096461892128.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.6498 - accuracy: 0.8018 - sparse_levenshtein_v1: 0.1973\n",
      "[I 2023-09-01 17:50:13,402] Trial 50 finished with value: 0.19730812311172485 and parameters: {'attention_heads': 8, 'learning_rate': 0.004197973716056387, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 8}. Best is trial 22 with value: 0.143096461892128.\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 0.5208 - accuracy: 0.8466 - sparse_levenshtein_v1: 0.1526\n",
      "[I 2023-09-01 17:52:28,491] Trial 51 finished with value: 0.15262380242347717 and parameters: {'attention_heads': 7, 'learning_rate': 0.0031626706365114484, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 9}. Best is trial 22 with value: 0.143096461892128.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5559 - accuracy: 0.8348 - sparse_levenshtein_v1: 0.1641\n",
      "[I 2023-09-01 17:54:52,314] Trial 52 finished with value: 0.16413265466690063 and parameters: {'attention_heads': 8, 'learning_rate': 0.0034817608011217193, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 9}. Best is trial 22 with value: 0.143096461892128.\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 0.7109 - accuracy: 0.7850 - sparse_levenshtein_v1: 0.2142\n",
      "[I 2023-09-01 17:56:15,742] Trial 53 finished with value: 0.21416901051998138 and parameters: {'attention_heads': 7, 'learning_rate': 0.012126338911035237, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 8}. Best is trial 22 with value: 0.143096461892128.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.6619 - accuracy: 0.7992 - sparse_levenshtein_v1: 0.2002\n",
      "[I 2023-09-01 17:58:39,251] Trial 54 finished with value: 0.20015351474285126 and parameters: {'attention_heads': 8, 'learning_rate': 0.006462820604002981, 'drop_out_out_decoder': 0.05, 'encoder_kernel_size': 7}. Best is trial 22 with value: 0.143096461892128.\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 0.4754 - accuracy: 0.8609 - sparse_levenshtein_v1: 0.1385\n",
      "[I 2023-09-01 18:00:54,531] Trial 55 finished with value: 0.13852393627166748 and parameters: {'attention_heads': 7, 'learning_rate': 0.0015166829571546356, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 9}. Best is trial 55 with value: 0.13852393627166748.\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 0.4939 - accuracy: 0.8564 - sparse_levenshtein_v1: 0.1427\n",
      "[I 2023-09-01 18:03:08,742] Trial 56 finished with value: 0.1427076756954193 and parameters: {'attention_heads': 7, 'learning_rate': 0.0014755713027344098, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 8}. Best is trial 55 with value: 0.13852393627166748.\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 0.5524 - accuracy: 0.8374 - sparse_levenshtein_v1: 0.1615\n",
      "[I 2023-09-01 18:05:23,092] Trial 57 finished with value: 0.1615348607301712 and parameters: {'attention_heads': 7, 'learning_rate': 0.0015705910070382125, 'drop_out_out_decoder': 0.25, 'encoder_kernel_size': 8}. Best is trial 55 with value: 0.13852393627166748.\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 0.6017 - accuracy: 0.8196 - sparse_levenshtein_v1: 0.1795\n",
      "[I 2023-09-01 18:07:37,975] Trial 58 finished with value: 0.1795375645160675 and parameters: {'attention_heads': 7, 'learning_rate': 0.0016269945966832192, 'drop_out_out_decoder': 0.45, 'encoder_kernel_size': 7}. Best is trial 55 with value: 0.13852393627166748.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5853 - accuracy: 0.8265 - sparse_levenshtein_v1: 0.1717\n",
      "[I 2023-09-01 18:09:52,366] Trial 59 finished with value: 0.1717456430196762 and parameters: {'attention_heads': 6, 'learning_rate': 0.001056186552366759, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 6}. Best is trial 55 with value: 0.13852393627166748.\n",
      "77/77 [==============================] - 1s 14ms/step - loss: 0.4588 - accuracy: 0.8674 - sparse_levenshtein_v1: 0.1320\n",
      "[I 2023-09-01 18:12:29,706] Trial 60 finished with value: 0.13200968503952026 and parameters: {'attention_heads': 8, 'learning_rate': 0.0021726204607152456, 'drop_out_out_decoder': 0.1, 'encoder_kernel_size': 8}. Best is trial 60 with value: 0.13200968503952026.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5117 - accuracy: 0.8512 - sparse_levenshtein_v1: 0.1479\n",
      "[I 2023-09-01 18:15:02,968] Trial 61 finished with value: 0.14792563021183014 and parameters: {'attention_heads': 8, 'learning_rate': 0.0016269878556570178, 'drop_out_out_decoder': 0.1, 'encoder_kernel_size': 8}. Best is trial 60 with value: 0.13200968503952026.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.4858 - accuracy: 0.8591 - sparse_levenshtein_v1: 0.1405\n",
      "[I 2023-09-01 18:17:26,330] Trial 62 finished with value: 0.1405499279499054 and parameters: {'attention_heads': 8, 'learning_rate': 0.0016324652585278673, 'drop_out_out_decoder': 0.1, 'encoder_kernel_size': 8}. Best is trial 60 with value: 0.13200968503952026.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.4985 - accuracy: 0.8548 - sparse_levenshtein_v1: 0.1446\n",
      "[I 2023-09-01 18:19:50,119] Trial 63 finished with value: 0.1445845067501068 and parameters: {'attention_heads': 8, 'learning_rate': 0.0022697685013833755, 'drop_out_out_decoder': 0.1, 'encoder_kernel_size': 8}. Best is trial 60 with value: 0.13200968503952026.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5090 - accuracy: 0.8520 - sparse_levenshtein_v1: 0.1477\n",
      "[I 2023-09-01 18:22:13,638] Trial 64 finished with value: 0.14774639904499054 and parameters: {'attention_heads': 8, 'learning_rate': 0.002229704382280487, 'drop_out_out_decoder': 0.1, 'encoder_kernel_size': 7}. Best is trial 60 with value: 0.13200968503952026.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5865 - accuracy: 0.8230 - sparse_levenshtein_v1: 0.1761\n",
      "[I 2023-09-01 18:24:36,814] Trial 65 finished with value: 0.17606306076049805 and parameters: {'attention_heads': 8, 'learning_rate': 0.0037992554079155907, 'drop_out_out_decoder': 0.05, 'encoder_kernel_size': 8}. Best is trial 60 with value: 0.13200968503952026.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.6600 - accuracy: 0.7992 - sparse_levenshtein_v1: 0.1999\n",
      "[I 2023-09-01 18:27:00,144] Trial 66 finished with value: 0.19992077350616455 and parameters: {'attention_heads': 8, 'learning_rate': 0.00548264266127171, 'drop_out_out_decoder': 0.1, 'encoder_kernel_size': 8}. Best is trial 60 with value: 0.13200968503952026.\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 0.5809 - accuracy: 0.8279 - sparse_levenshtein_v1: 0.1709\n",
      "[I 2023-09-01 18:29:15,303] Trial 67 finished with value: 0.17091697454452515 and parameters: {'attention_heads': 7, 'learning_rate': 0.0012382631791451443, 'drop_out_out_decoder': 0.05, 'encoder_kernel_size': 7}. Best is trial 60 with value: 0.13200968503952026.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.4667 - accuracy: 0.8653 - sparse_levenshtein_v1: 0.1341\n",
      "[I 2023-09-01 18:31:38,939] Trial 68 finished with value: 0.13407570123672485 and parameters: {'attention_heads': 8, 'learning_rate': 0.0025448058214776787, 'drop_out_out_decoder': 0.1, 'encoder_kernel_size': 8}. Best is trial 60 with value: 0.13200968503952026.\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 0.5274 - accuracy: 0.8452 - sparse_levenshtein_v1: 0.1540\n",
      "[I 2023-09-01 18:33:53,732] Trial 69 finished with value: 0.15402115881443024 and parameters: {'attention_heads': 7, 'learning_rate': 0.0024294852358706337, 'drop_out_out_decoder': 0.15000000000000002, 'encoder_kernel_size': 7}. Best is trial 60 with value: 0.13200968503952026.\n",
      "77/77 [==============================] - 1s 14ms/step - loss: 0.5643 - accuracy: 0.8336 - sparse_levenshtein_v1: 0.1645\n",
      "[I 2023-09-01 18:36:18,618] Trial 70 finished with value: 0.16446056962013245 and parameters: {'attention_heads': 8, 'learning_rate': 0.0008342088032677576, 'drop_out_out_decoder': 0.1, 'encoder_kernel_size': 5}. Best is trial 60 with value: 0.13200968503952026.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5179 - accuracy: 0.8481 - sparse_levenshtein_v1: 0.1514\n",
      "[I 2023-09-01 18:38:45,304] Trial 71 finished with value: 0.1514264941215515 and parameters: {'attention_heads': 8, 'learning_rate': 0.0017752409984927828, 'drop_out_out_decoder': 0.05, 'encoder_kernel_size': 8}. Best is trial 60 with value: 0.13200968503952026.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5922 - accuracy: 0.8219 - sparse_levenshtein_v1: 0.1772\n",
      "[I 2023-09-01 18:41:08,721] Trial 72 finished with value: 0.17724880576133728 and parameters: {'attention_heads': 8, 'learning_rate': 0.003562010530275835, 'drop_out_out_decoder': 0.1, 'encoder_kernel_size': 8}. Best is trial 60 with value: 0.13200968503952026.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.6759 - accuracy: 0.7947 - sparse_levenshtein_v1: 0.2047\n",
      "[I 2023-09-01 18:43:32,904] Trial 73 finished with value: 0.20469620823860168 and parameters: {'attention_heads': 8, 'learning_rate': 0.007615579194394512, 'drop_out_out_decoder': 0.05, 'encoder_kernel_size': 9}. Best is trial 60 with value: 0.13200968503952026.\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.5991 - accuracy: 0.8226 - sparse_levenshtein_v1: 0.1762\n",
      "[I 2023-09-01 18:44:57,468] Trial 74 finished with value: 0.17624492943286896 and parameters: {'attention_heads': 1, 'learning_rate': 0.00262195040312914, 'drop_out_out_decoder': 0.05, 'encoder_kernel_size': 8}. Best is trial 60 with value: 0.13200968503952026.\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 0.5253 - accuracy: 0.8462 - sparse_levenshtein_v1: 0.1529\n",
      "[I 2023-09-01 18:47:12,704] Trial 75 finished with value: 0.15290763974189758 and parameters: {'attention_heads': 7, 'learning_rate': 0.0013761417165406224, 'drop_out_out_decoder': 0.1, 'encoder_kernel_size': 9}. Best is trial 60 with value: 0.13200968503952026.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.6407 - accuracy: 0.8036 - sparse_levenshtein_v1: 0.1957\n",
      "[I 2023-09-01 18:49:17,949] Trial 76 finished with value: 0.1956912875175476 and parameters: {'attention_heads': 8, 'learning_rate': 0.005117704379040488, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 7}. Best is trial 60 with value: 0.13200968503952026.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.4819 - accuracy: 0.8596 - sparse_levenshtein_v1: 0.1397\n",
      "[I 2023-09-01 18:51:42,333] Trial 77 finished with value: 0.13973645865917206 and parameters: {'attention_heads': 8, 'learning_rate': 0.001938153696051516, 'drop_out_out_decoder': 0.15000000000000002, 'encoder_kernel_size': 9}. Best is trial 60 with value: 0.13200968503952026.\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 0.4484 - accuracy: 0.8705 - sparse_levenshtein_v1: 0.1288\n",
      "[I 2023-09-01 18:53:57,726] Trial 78 finished with value: 0.12880994379520416 and parameters: {'attention_heads': 7, 'learning_rate': 0.0020283022893461916, 'drop_out_out_decoder': 0.15000000000000002, 'encoder_kernel_size': 9}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 0.6096 - accuracy: 0.8166 - sparse_levenshtein_v1: 0.1825\n",
      "[I 2023-09-01 18:56:12,867] Trial 79 finished with value: 0.18246032297611237 and parameters: {'attention_heads': 7, 'learning_rate': 0.0010108314973152495, 'drop_out_out_decoder': 0.15000000000000002, 'encoder_kernel_size': 10}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 0.5594 - accuracy: 0.8359 - sparse_levenshtein_v1: 0.1633\n",
      "[I 2023-09-01 18:58:19,369] Trial 80 finished with value: 0.16329491138458252 and parameters: {'attention_heads': 6, 'learning_rate': 0.0014609263643148604, 'drop_out_out_decoder': 0.2, 'encoder_kernel_size': 9}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 0.4966 - accuracy: 0.8570 - sparse_levenshtein_v1: 0.1424\n",
      "[I 2023-09-01 19:00:34,652] Trial 81 finished with value: 0.14244432747364044 and parameters: {'attention_heads': 7, 'learning_rate': 0.0021116135482367225, 'drop_out_out_decoder': 0.15000000000000002, 'encoder_kernel_size': 9}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 0.5467 - accuracy: 0.8402 - sparse_levenshtein_v1: 0.1589\n",
      "[I 2023-09-01 19:02:49,659] Trial 82 finished with value: 0.15890632569789886 and parameters: {'attention_heads': 7, 'learning_rate': 0.0019463056107374378, 'drop_out_out_decoder': 0.15000000000000002, 'encoder_kernel_size': 10}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 0.5059 - accuracy: 0.8534 - sparse_levenshtein_v1: 0.1460\n",
      "[I 2023-09-01 19:05:04,982] Trial 83 finished with value: 0.14604635536670685 and parameters: {'attention_heads': 7, 'learning_rate': 0.002677994472255751, 'drop_out_out_decoder': 0.2, 'encoder_kernel_size': 9}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 0.6293 - accuracy: 0.8094 - sparse_levenshtein_v1: 0.1893\n",
      "[I 2023-09-01 19:07:19,621] Trial 84 finished with value: 0.1892886757850647 and parameters: {'attention_heads': 7, 'learning_rate': 0.0006639378900980183, 'drop_out_out_decoder': 0.15000000000000002, 'encoder_kernel_size': 10}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 0.4884 - accuracy: 0.8587 - sparse_levenshtein_v1: 0.1406\n",
      "[I 2023-09-01 19:09:26,368] Trial 85 finished with value: 0.14056509733200073 and parameters: {'attention_heads': 6, 'learning_rate': 0.0018188966087551704, 'drop_out_out_decoder': 0.15000000000000002, 'encoder_kernel_size': 9}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 0.5986 - accuracy: 0.8233 - sparse_levenshtein_v1: 0.1757\n",
      "[I 2023-09-01 19:11:33,147] Trial 86 finished with value: 0.17570891976356506 and parameters: {'attention_heads': 6, 'learning_rate': 0.0012281688037659145, 'drop_out_out_decoder': 0.25, 'encoder_kernel_size': 9}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 0.5414 - accuracy: 0.8413 - sparse_levenshtein_v1: 0.1580\n",
      "[I 2023-09-01 19:13:40,148] Trial 87 finished with value: 0.15800192952156067 and parameters: {'attention_heads': 6, 'learning_rate': 0.0017426600341069662, 'drop_out_out_decoder': 0.15000000000000002, 'encoder_kernel_size': 9}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 0.6327 - accuracy: 0.8077 - sparse_levenshtein_v1: 0.1911\n",
      "[I 2023-09-01 19:15:55,132] Trial 88 finished with value: 0.19111350178718567 and parameters: {'attention_heads': 7, 'learning_rate': 0.0009196298961077486, 'drop_out_out_decoder': 0.2, 'encoder_kernel_size': 9}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 0.6229 - accuracy: 0.8132 - sparse_levenshtein_v1: 0.1856\n",
      "[I 2023-09-01 19:18:10,728] Trial 89 finished with value: 0.18564490973949432 and parameters: {'attention_heads': 7, 'learning_rate': 0.0005346039752081162, 'drop_out_out_decoder': 0.15000000000000002, 'encoder_kernel_size': 9}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 0.5063 - accuracy: 0.8521 - sparse_levenshtein_v1: 0.1472\n",
      "[I 2023-09-01 19:20:16,671] Trial 90 finished with value: 0.14722280204296112 and parameters: {'attention_heads': 6, 'learning_rate': 0.0029212801080329277, 'drop_out_out_decoder': 0.15000000000000002, 'encoder_kernel_size': 8}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.4978 - accuracy: 0.8545 - sparse_levenshtein_v1: 0.1447\n",
      "[I 2023-09-01 19:22:40,731] Trial 91 finished with value: 0.14467419683933258 and parameters: {'attention_heads': 8, 'learning_rate': 0.002066095987712036, 'drop_out_out_decoder': 0.1, 'encoder_kernel_size': 10}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 0.5178 - accuracy: 0.8496 - sparse_levenshtein_v1: 0.1498\n",
      "[I 2023-09-01 19:24:56,068] Trial 92 finished with value: 0.149753600358963 and parameters: {'attention_heads': 7, 'learning_rate': 0.0013760563486881928, 'drop_out_out_decoder': 0.1, 'encoder_kernel_size': 9}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5124 - accuracy: 0.8497 - sparse_levenshtein_v1: 0.1496\n",
      "[I 2023-09-01 19:27:20,093] Trial 93 finished with value: 0.14960630238056183 and parameters: {'attention_heads': 8, 'learning_rate': 0.004415486784260122, 'drop_out_out_decoder': 0.1, 'encoder_kernel_size': 10}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 0.5176 - accuracy: 0.8479 - sparse_levenshtein_v1: 0.1514\n",
      "[I 2023-09-01 19:29:35,516] Trial 94 finished with value: 0.1513921171426773 and parameters: {'attention_heads': 7, 'learning_rate': 0.0020011490963364157, 'drop_out_out_decoder': 0.15000000000000002, 'encoder_kernel_size': 9}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5355 - accuracy: 0.8435 - sparse_levenshtein_v1: 0.1555\n",
      "[I 2023-09-01 19:31:59,239] Trial 95 finished with value: 0.15549184381961823 and parameters: {'attention_heads': 8, 'learning_rate': 0.0010558515444185372, 'drop_out_out_decoder': 0.2, 'encoder_kernel_size': 8}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 0.5713 - accuracy: 0.8307 - sparse_levenshtein_v1: 0.1684\n",
      "[I 2023-09-01 19:34:06,309] Trial 96 finished with value: 0.16843882203102112 and parameters: {'attention_heads': 6, 'learning_rate': 0.0026115692394230367, 'drop_out_out_decoder': 0.1, 'encoder_kernel_size': 9}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5333 - accuracy: 0.8429 - sparse_levenshtein_v1: 0.1563\n",
      "[I 2023-09-01 19:36:30,035] Trial 97 finished with value: 0.15631352365016937 and parameters: {'attention_heads': 8, 'learning_rate': 0.0015278027975890256, 'drop_out_out_decoder': 0.05, 'encoder_kernel_size': 10}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 0.5500 - accuracy: 0.8374 - sparse_levenshtein_v1: 0.1618\n",
      "[I 2023-09-01 19:38:44,849] Trial 98 finished with value: 0.16184663772583008 and parameters: {'attention_heads': 7, 'learning_rate': 0.0034173547865385774, 'drop_out_out_decoder': 0.15000000000000002, 'encoder_kernel_size': 8}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 8ms/step - loss: 0.6563 - accuracy: 0.8013 - sparse_levenshtein_v1: 0.1977\n",
      "[I 2023-09-01 19:40:13,146] Trial 99 finished with value: 0.19765281677246094 and parameters: {'attention_heads': 2, 'learning_rate': 0.0007758459306067082, 'drop_out_out_decoder': 0.05, 'encoder_kernel_size': 8}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 0.4799 - accuracy: 0.8608 - sparse_levenshtein_v1: 0.1387\n",
      "[I 2023-09-01 19:42:28,352] Trial 100 finished with value: 0.13871395587921143 and parameters: {'attention_heads': 7, 'learning_rate': 0.0021706846707944226, 'drop_out_out_decoder': 0.15000000000000002, 'encoder_kernel_size': 9}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 0.5406 - accuracy: 0.8411 - sparse_levenshtein_v1: 0.1579\n",
      "[I 2023-09-01 19:44:43,789] Trial 101 finished with value: 0.15791276097297668 and parameters: {'attention_heads': 7, 'learning_rate': 0.0022418725101687837, 'drop_out_out_decoder': 0.2, 'encoder_kernel_size': 9}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 0.5392 - accuracy: 0.8440 - sparse_levenshtein_v1: 0.1554\n",
      "[I 2023-09-01 19:46:50,889] Trial 102 finished with value: 0.1554202288389206 and parameters: {'attention_heads': 6, 'learning_rate': 0.0019193779419905306, 'drop_out_out_decoder': 0.15000000000000002, 'encoder_kernel_size': 9}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 0.5139 - accuracy: 0.8496 - sparse_levenshtein_v1: 0.1498\n",
      "[I 2023-09-01 19:49:06,148] Trial 103 finished with value: 0.14981110394001007 and parameters: {'attention_heads': 7, 'learning_rate': 0.0030099350604362433, 'drop_out_out_decoder': 0.15000000000000002, 'encoder_kernel_size': 10}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5134 - accuracy: 0.8502 - sparse_levenshtein_v1: 0.1490\n",
      "[I 2023-09-01 19:51:30,527] Trial 104 finished with value: 0.1490497589111328 and parameters: {'attention_heads': 8, 'learning_rate': 0.0016456216027370787, 'drop_out_out_decoder': 0.1, 'encoder_kernel_size': 9}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5533 - accuracy: 0.8380 - sparse_levenshtein_v1: 0.1613\n",
      "[I 2023-09-01 19:53:53,831] Trial 105 finished with value: 0.16134320199489594 and parameters: {'attention_heads': 8, 'learning_rate': 0.0012045316439174736, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 8}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 0.5983 - accuracy: 0.8209 - sparse_levenshtein_v1: 0.1782\n",
      "[I 2023-09-01 19:56:08,096] Trial 106 finished with value: 0.17822502553462982 and parameters: {'attention_heads': 7, 'learning_rate': 0.003981710500978599, 'drop_out_out_decoder': 0.1, 'encoder_kernel_size': 8}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5281 - accuracy: 0.8452 - sparse_levenshtein_v1: 0.1539\n",
      "[I 2023-09-01 19:58:32,502] Trial 107 finished with value: 0.15393424034118652 and parameters: {'attention_heads': 8, 'learning_rate': 0.002326540281331183, 'drop_out_out_decoder': 0.15000000000000002, 'encoder_kernel_size': 9}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 0.5596 - accuracy: 0.8360 - sparse_levenshtein_v1: 0.1630\n",
      "[I 2023-09-01 20:00:47,219] Trial 108 finished with value: 0.1630459725856781 and parameters: {'attention_heads': 7, 'learning_rate': 0.0013593675197138427, 'drop_out_out_decoder': 0.25, 'encoder_kernel_size': 8}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 11ms/step - loss: 0.5165 - accuracy: 0.8504 - sparse_levenshtein_v1: 0.1488\n",
      "[I 2023-09-01 20:02:54,188] Trial 109 finished with value: 0.1487887054681778 and parameters: {'attention_heads': 6, 'learning_rate': 0.0017361063409540496, 'drop_out_out_decoder': 0.1, 'encoder_kernel_size': 9}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5604 - accuracy: 0.8344 - sparse_levenshtein_v1: 0.1643\n",
      "[I 2023-09-01 20:05:18,375] Trial 110 finished with value: 0.16429254412651062 and parameters: {'attention_heads': 8, 'learning_rate': 0.0010111118384045297, 'drop_out_out_decoder': 0.05, 'encoder_kernel_size': 10}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.4833 - accuracy: 0.8589 - sparse_levenshtein_v1: 0.1404\n",
      "[I 2023-09-01 20:07:41,934] Trial 111 finished with value: 0.14036089181900024 and parameters: {'attention_heads': 8, 'learning_rate': 0.00236438401375015, 'drop_out_out_decoder': 0.1, 'encoder_kernel_size': 8}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.4828 - accuracy: 0.8595 - sparse_levenshtein_v1: 0.1402\n",
      "[I 2023-09-01 20:10:05,439] Trial 112 finished with value: 0.14016427099704742 and parameters: {'attention_heads': 8, 'learning_rate': 0.0025920477809409743, 'drop_out_out_decoder': 0.1, 'encoder_kernel_size': 8}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5184 - accuracy: 0.8480 - sparse_levenshtein_v1: 0.1513\n",
      "[I 2023-09-01 20:12:28,841] Trial 113 finished with value: 0.15129569172859192 and parameters: {'attention_heads': 8, 'learning_rate': 0.0028341687084962332, 'drop_out_out_decoder': 0.15000000000000002, 'encoder_kernel_size': 8}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5065 - accuracy: 0.8506 - sparse_levenshtein_v1: 0.1485\n",
      "[I 2023-09-01 20:14:52,835] Trial 114 finished with value: 0.14852291345596313 and parameters: {'attention_heads': 8, 'learning_rate': 0.0036389124914323204, 'drop_out_out_decoder': 0.1, 'encoder_kernel_size': 7}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5311 - accuracy: 0.8438 - sparse_levenshtein_v1: 0.1556\n",
      "[I 2023-09-01 20:17:16,965] Trial 115 finished with value: 0.1556302160024643 and parameters: {'attention_heads': 8, 'learning_rate': 0.0024954319791737583, 'drop_out_out_decoder': 0.15000000000000002, 'encoder_kernel_size': 7}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5698 - accuracy: 0.8284 - sparse_levenshtein_v1: 0.1709\n",
      "[I 2023-09-01 20:19:40,949] Trial 116 finished with value: 0.17093144357204437 and parameters: {'attention_heads': 8, 'learning_rate': 0.004660398420762363, 'drop_out_out_decoder': 0.2, 'encoder_kernel_size': 8}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.4873 - accuracy: 0.8579 - sparse_levenshtein_v1: 0.1413\n",
      "[I 2023-09-01 20:22:04,236] Trial 117 finished with value: 0.14132481813430786 and parameters: {'attention_heads': 8, 'learning_rate': 0.0018902900906553179, 'drop_out_out_decoder': 0.1, 'encoder_kernel_size': 8}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5128 - accuracy: 0.8497 - sparse_levenshtein_v1: 0.1495\n",
      "[I 2023-09-01 20:24:27,864] Trial 118 finished with value: 0.14949646592140198 and parameters: {'attention_heads': 8, 'learning_rate': 0.001999306599152814, 'drop_out_out_decoder': 0.1, 'encoder_kernel_size': 8}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5558 - accuracy: 0.8371 - sparse_levenshtein_v1: 0.1622\n",
      "[I 2023-09-01 20:26:52,232] Trial 119 finished with value: 0.16220802068710327 and parameters: {'attention_heads': 8, 'learning_rate': 0.003295262739075811, 'drop_out_out_decoder': 0.1, 'encoder_kernel_size': 9}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 0.5319 - accuracy: 0.8436 - sparse_levenshtein_v1: 0.1558\n",
      "[I 2023-09-01 20:29:15,169] Trial 120 finished with value: 0.1557544618844986 and parameters: {'attention_heads': 8, 'learning_rate': 0.0018355303972973247, 'drop_out_out_decoder': 0.1, 'encoder_kernel_size': 4}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 0.5139 - accuracy: 0.8522 - sparse_levenshtein_v1: 0.1471\n",
      "[I 2023-09-01 20:31:29,699] Trial 121 finished with value: 0.14705558121204376 and parameters: {'attention_heads': 7, 'learning_rate': 0.0014989886376820928, 'drop_out_out_decoder': 0.15000000000000002, 'encoder_kernel_size': 8}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5879 - accuracy: 0.8246 - sparse_levenshtein_v1: 0.1741\n",
      "[I 2023-09-01 20:33:53,257] Trial 122 finished with value: 0.17409560084342957 and parameters: {'attention_heads': 8, 'learning_rate': 0.001136676800715386, 'drop_out_out_decoder': 0.1, 'encoder_kernel_size': 8}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5040 - accuracy: 0.8533 - sparse_levenshtein_v1: 0.1461\n",
      "[I 2023-09-01 20:36:16,654] Trial 123 finished with value: 0.14611823856830597 and parameters: {'attention_heads': 8, 'learning_rate': 0.0025212982118758467, 'drop_out_out_decoder': 0.15000000000000002, 'encoder_kernel_size': 8}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 0.5542 - accuracy: 0.8365 - sparse_levenshtein_v1: 0.1628\n",
      "[I 2023-09-01 20:38:31,700] Trial 124 finished with value: 0.16278482973575592 and parameters: {'attention_heads': 7, 'learning_rate': 0.0014086997556990014, 'drop_out_out_decoder': 0.1, 'encoder_kernel_size': 7}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5680 - accuracy: 0.8307 - sparse_levenshtein_v1: 0.1683\n",
      "[I 2023-09-01 20:40:56,485] Trial 125 finished with value: 0.1683325618505478 and parameters: {'attention_heads': 8, 'learning_rate': 0.0030890389065107897, 'drop_out_out_decoder': 0.15000000000000002, 'encoder_kernel_size': 9}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 0.5445 - accuracy: 0.8405 - sparse_levenshtein_v1: 0.1587\n",
      "[I 2023-09-01 20:43:10,888] Trial 126 finished with value: 0.15865227580070496 and parameters: {'attention_heads': 7, 'learning_rate': 0.002273919873887551, 'drop_out_out_decoder': 0.2, 'encoder_kernel_size': 8}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.4979 - accuracy: 0.8555 - sparse_levenshtein_v1: 0.1438\n",
      "[I 2023-09-01 20:45:35,428] Trial 127 finished with value: 0.1437690556049347 and parameters: {'attention_heads': 8, 'learning_rate': 0.0017215263624997229, 'drop_out_out_decoder': 0.1, 'encoder_kernel_size': 9}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 0.6625 - accuracy: 0.7986 - sparse_levenshtein_v1: 0.2008\n",
      "[I 2023-09-01 20:47:49,759] Trial 128 finished with value: 0.2008422315120697 and parameters: {'attention_heads': 7, 'learning_rate': 0.003973762469829476, 'drop_out_out_decoder': 0.30000000000000004, 'encoder_kernel_size': 8}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5155 - accuracy: 0.8504 - sparse_levenshtein_v1: 0.1488\n",
      "[I 2023-09-01 20:50:13,194] Trial 129 finished with value: 0.14880867302417755 and parameters: {'attention_heads': 8, 'learning_rate': 0.0008847619661903291, 'drop_out_out_decoder': 0.05, 'encoder_kernel_size': 8}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 0.5376 - accuracy: 0.8433 - sparse_levenshtein_v1: 0.1559\n",
      "[I 2023-09-01 20:52:30,911] Trial 130 finished with value: 0.15592041611671448 and parameters: {'attention_heads': 7, 'learning_rate': 0.0012669813743130937, 'drop_out_out_decoder': 0.15000000000000002, 'encoder_kernel_size': 9}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.4724 - accuracy: 0.8632 - sparse_levenshtein_v1: 0.1360\n",
      "[I 2023-09-01 20:54:55,413] Trial 131 finished with value: 0.13601598143577576 and parameters: {'attention_heads': 8, 'learning_rate': 0.00217010008577117, 'drop_out_out_decoder': 0.05, 'encoder_kernel_size': 9}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.4537 - accuracy: 0.8680 - sparse_levenshtein_v1: 0.1313\n",
      "[I 2023-09-01 20:57:19,512] Trial 132 finished with value: 0.1312948614358902 and parameters: {'attention_heads': 8, 'learning_rate': 0.002078205200526824, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 9}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.4885 - accuracy: 0.8568 - sparse_levenshtein_v1: 0.1427\n",
      "[I 2023-09-01 20:59:43,931] Trial 133 finished with value: 0.14273136854171753 and parameters: {'attention_heads': 8, 'learning_rate': 0.0021076198459079056, 'drop_out_out_decoder': 0.05, 'encoder_kernel_size': 9}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5222 - accuracy: 0.8468 - sparse_levenshtein_v1: 0.1525\n",
      "[I 2023-09-01 21:02:08,177] Trial 134 finished with value: 0.15252386033535004 and parameters: {'attention_heads': 8, 'learning_rate': 0.0026594398696422722, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 9}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5323 - accuracy: 0.8437 - sparse_levenshtein_v1: 0.1555\n",
      "[I 2023-09-01 21:04:32,400] Trial 135 finished with value: 0.1554560363292694 and parameters: {'attention_heads': 8, 'learning_rate': 0.0017592774088121025, 'drop_out_out_decoder': 0.05, 'encoder_kernel_size': 9}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.4693 - accuracy: 0.8635 - sparse_levenshtein_v1: 0.1360\n",
      "[I 2023-09-01 21:06:56,711] Trial 136 finished with value: 0.13601168990135193 and parameters: {'attention_heads': 8, 'learning_rate': 0.002147589885111343, 'drop_out_out_decoder': 0.1, 'encoder_kernel_size': 9}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.6576 - accuracy: 0.7993 - sparse_levenshtein_v1: 0.1997\n",
      "[I 2023-09-01 21:09:20,941] Trial 137 finished with value: 0.19974400103092194 and parameters: {'attention_heads': 8, 'learning_rate': 0.005804970295680843, 'drop_out_out_decoder': 0.1, 'encoder_kernel_size': 9}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5025 - accuracy: 0.8527 - sparse_levenshtein_v1: 0.1466\n",
      "[I 2023-09-01 21:11:45,131] Trial 138 finished with value: 0.14663618803024292 and parameters: {'attention_heads': 8, 'learning_rate': 0.003078471483823401, 'drop_out_out_decoder': 0.1, 'encoder_kernel_size': 9}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.4883 - accuracy: 0.8577 - sparse_levenshtein_v1: 0.1416\n",
      "[I 2023-09-01 21:14:08,818] Trial 139 finished with value: 0.14158576726913452 and parameters: {'attention_heads': 8, 'learning_rate': 0.0025261028857748074, 'drop_out_out_decoder': 0.05, 'encoder_kernel_size': 8}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5054 - accuracy: 0.8520 - sparse_levenshtein_v1: 0.1470\n",
      "[I 2023-09-01 21:16:32,832] Trial 140 finished with value: 0.14704708755016327 and parameters: {'attention_heads': 8, 'learning_rate': 0.0015739086384718261, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 9}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.4882 - accuracy: 0.8570 - sparse_levenshtein_v1: 0.1423\n",
      "[I 2023-09-01 21:18:56,447] Trial 141 finished with value: 0.14230947196483612 and parameters: {'attention_heads': 8, 'learning_rate': 0.0025407742245191897, 'drop_out_out_decoder': 0.05, 'encoder_kernel_size': 8}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5277 - accuracy: 0.8447 - sparse_levenshtein_v1: 0.1550\n",
      "[I 2023-09-01 21:21:19,941] Trial 142 finished with value: 0.1549663245677948 and parameters: {'attention_heads': 8, 'learning_rate': 0.002062870532515824, 'drop_out_out_decoder': 0.05, 'encoder_kernel_size': 8}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5094 - accuracy: 0.8510 - sparse_levenshtein_v1: 0.1486\n",
      "[I 2023-09-01 21:23:43,416] Trial 143 finished with value: 0.1486072987318039 and parameters: {'attention_heads': 8, 'learning_rate': 0.003538535011869824, 'drop_out_out_decoder': 0.1, 'encoder_kernel_size': 8}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5304 - accuracy: 0.8431 - sparse_levenshtein_v1: 0.1563\n",
      "[I 2023-09-01 21:26:07,404] Trial 144 finished with value: 0.15633852779865265 and parameters: {'attention_heads': 8, 'learning_rate': 0.002859165442875543, 'drop_out_out_decoder': 0.1, 'encoder_kernel_size': 7}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5049 - accuracy: 0.8518 - sparse_levenshtein_v1: 0.1478\n",
      "[I 2023-09-01 21:28:31,633] Trial 145 finished with value: 0.147786483168602 and parameters: {'attention_heads': 8, 'learning_rate': 0.002332820322502931, 'drop_out_out_decoder': 0.05, 'encoder_kernel_size': 9}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.6434 - accuracy: 0.8034 - sparse_levenshtein_v1: 0.1958\n",
      "[I 2023-09-01 21:30:55,696] Trial 146 finished with value: 0.19579225778579712 and parameters: {'attention_heads': 8, 'learning_rate': 0.004463374242056577, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 9}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.4993 - accuracy: 0.8552 - sparse_levenshtein_v1: 0.1442\n",
      "[I 2023-09-01 21:33:19,302] Trial 147 finished with value: 0.14422397315502167 and parameters: {'attention_heads': 8, 'learning_rate': 0.0018582728096701309, 'drop_out_out_decoder': 0.1, 'encoder_kernel_size': 8}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5449 - accuracy: 0.8383 - sparse_levenshtein_v1: 0.1607\n",
      "[I 2023-09-01 21:35:43,388] Trial 148 finished with value: 0.16066640615463257 and parameters: {'attention_heads': 8, 'learning_rate': 0.0012039254357645397, 'drop_out_out_decoder': 0.05, 'encoder_kernel_size': 10}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5544 - accuracy: 0.8361 - sparse_levenshtein_v1: 0.1630\n",
      "[I 2023-09-01 21:38:07,024] Trial 149 finished with value: 0.16301679611206055 and parameters: {'attention_heads': 8, 'learning_rate': 0.0014709158364330378, 'drop_out_out_decoder': 0.1, 'encoder_kernel_size': 8}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.6238 - accuracy: 0.8098 - sparse_levenshtein_v1: 0.1894\n",
      "[I 2023-09-01 21:40:31,304] Trial 150 finished with value: 0.1894046515226364 and parameters: {'attention_heads': 8, 'learning_rate': 0.003287484950243073, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 9}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 14ms/step - loss: 0.4852 - accuracy: 0.8587 - sparse_levenshtein_v1: 0.1408\n",
      "[I 2023-09-01 21:43:06,552] Trial 151 finished with value: 0.14077229797840118 and parameters: {'attention_heads': 8, 'learning_rate': 0.002650449099512093, 'drop_out_out_decoder': 0.05, 'encoder_kernel_size': 8}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 14ms/step - loss: 0.5157 - accuracy: 0.8484 - sparse_levenshtein_v1: 0.1509\n",
      "[I 2023-09-01 21:45:43,584] Trial 152 finished with value: 0.1509072184562683 and parameters: {'attention_heads': 8, 'learning_rate': 0.002411082039408334, 'drop_out_out_decoder': 0.05, 'encoder_kernel_size': 8}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 15ms/step - loss: 0.4750 - accuracy: 0.8625 - sparse_levenshtein_v1: 0.1369\n",
      "[I 2023-09-01 21:48:29,031] Trial 153 finished with value: 0.13691040873527527 and parameters: {'attention_heads': 8, 'learning_rate': 0.0018630221606597702, 'drop_out_out_decoder': 0.05, 'encoder_kernel_size': 8}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 15ms/step - loss: 0.4802 - accuracy: 0.8618 - sparse_levenshtein_v1: 0.1377\n",
      "[I 2023-09-01 21:51:28,616] Trial 154 finished with value: 0.1377272754907608 and parameters: {'attention_heads': 8, 'learning_rate': 0.0019004156035149179, 'drop_out_out_decoder': 0.1, 'encoder_kernel_size': 8}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.5307 - accuracy: 0.8441 - sparse_levenshtein_v1: 0.1551\n",
      "[I 2023-09-01 21:54:15,498] Trial 155 finished with value: 0.15514686703681946 and parameters: {'attention_heads': 8, 'learning_rate': 0.0016593711293257784, 'drop_out_out_decoder': 0.1, 'encoder_kernel_size': 9}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.5538 - accuracy: 0.8363 - sparse_levenshtein_v1: 0.1627\n",
      "[I 2023-09-01 21:57:04,279] Trial 156 finished with value: 0.16265402734279633 and parameters: {'attention_heads': 8, 'learning_rate': 0.0012879472788559808, 'drop_out_out_decoder': 0.05, 'encoder_kernel_size': 8}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 15ms/step - loss: 0.4876 - accuracy: 0.8584 - sparse_levenshtein_v1: 0.1409\n",
      "[I 2023-09-01 21:59:53,871] Trial 157 finished with value: 0.14089557528495789 and parameters: {'attention_heads': 8, 'learning_rate': 0.0020179262790877093, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 9}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 14ms/step - loss: 0.5707 - accuracy: 0.8305 - sparse_levenshtein_v1: 0.1686\n",
      "[I 2023-09-01 22:02:35,820] Trial 158 finished with value: 0.168551504611969 and parameters: {'attention_heads': 8, 'learning_rate': 0.0010251617472193005, 'drop_out_out_decoder': 0.1, 'encoder_kernel_size': 7}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 14ms/step - loss: 0.5061 - accuracy: 0.8517 - sparse_levenshtein_v1: 0.1475\n",
      "[I 2023-09-01 22:05:16,139] Trial 159 finished with value: 0.1475481241941452 and parameters: {'attention_heads': 8, 'learning_rate': 0.0027685448535683652, 'drop_out_out_decoder': 0.15000000000000002, 'encoder_kernel_size': 9}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5384 - accuracy: 0.8415 - sparse_levenshtein_v1: 0.1577\n",
      "[I 2023-09-01 22:07:43,394] Trial 160 finished with value: 0.1576932966709137 and parameters: {'attention_heads': 8, 'learning_rate': 0.004043436611048626, 'drop_out_out_decoder': 0.1, 'encoder_kernel_size': 8}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.4636 - accuracy: 0.8649 - sparse_levenshtein_v1: 0.1346\n",
      "[I 2023-09-01 22:10:08,553] Trial 161 finished with value: 0.13457606732845306 and parameters: {'attention_heads': 8, 'learning_rate': 0.0021225622930373523, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 9}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.4948 - accuracy: 0.8554 - sparse_levenshtein_v1: 0.1441\n",
      "[I 2023-09-01 22:12:33,731] Trial 162 finished with value: 0.14411647617816925 and parameters: {'attention_heads': 8, 'learning_rate': 0.001634663875624421, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 9}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.4757 - accuracy: 0.8616 - sparse_levenshtein_v1: 0.1376\n",
      "[I 2023-09-01 22:14:59,059] Trial 163 finished with value: 0.1376313865184784 and parameters: {'attention_heads': 8, 'learning_rate': 0.0021639412882440453, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 9}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5116 - accuracy: 0.8497 - sparse_levenshtein_v1: 0.1497\n",
      "[I 2023-09-01 22:17:23,967] Trial 164 finished with value: 0.14973364770412445 and parameters: {'attention_heads': 8, 'learning_rate': 0.002177793799241504, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 9}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5173 - accuracy: 0.8477 - sparse_levenshtein_v1: 0.1517\n",
      "[I 2023-09-01 22:19:48,902] Trial 165 finished with value: 0.15170763432979584 and parameters: {'attention_heads': 8, 'learning_rate': 0.0014211798008204122, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 9}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5229 - accuracy: 0.8467 - sparse_levenshtein_v1: 0.1525\n",
      "[I 2023-09-01 22:22:13,123] Trial 166 finished with value: 0.15252099931240082 and parameters: {'attention_heads': 8, 'learning_rate': 0.0018431261084528839, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 10}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5477 - accuracy: 0.8377 - sparse_levenshtein_v1: 0.1613\n",
      "[I 2023-09-01 22:24:38,228] Trial 167 finished with value: 0.16131888329982758 and parameters: {'attention_heads': 8, 'learning_rate': 0.003227233900650369, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 9}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.4884 - accuracy: 0.8567 - sparse_levenshtein_v1: 0.1430\n",
      "[I 2023-09-01 22:27:03,281] Trial 168 finished with value: 0.14296092092990875 and parameters: {'attention_heads': 8, 'learning_rate': 0.0021591941208604025, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 9}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5680 - accuracy: 0.8323 - sparse_levenshtein_v1: 0.1665\n",
      "[I 2023-09-01 22:29:28,596] Trial 169 finished with value: 0.16653701663017273 and parameters: {'attention_heads': 8, 'learning_rate': 0.0011316084148957723, 'drop_out_out_decoder': 0.15000000000000002, 'encoder_kernel_size': 9}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.4996 - accuracy: 0.8548 - sparse_levenshtein_v1: 0.1442\n",
      "[I 2023-09-01 22:31:53,568] Trial 170 finished with value: 0.14418524503707886 and parameters: {'attention_heads': 8, 'learning_rate': 0.0015583650002395039, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 9}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.4809 - accuracy: 0.8601 - sparse_levenshtein_v1: 0.1396\n",
      "[I 2023-09-01 22:34:18,471] Trial 171 finished with value: 0.13962101936340332 and parameters: {'attention_heads': 8, 'learning_rate': 0.0027304399421639253, 'drop_out_out_decoder': 0.05, 'encoder_kernel_size': 8}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.4713 - accuracy: 0.8627 - sparse_levenshtein_v1: 0.1367\n",
      "[I 2023-09-01 22:36:42,864] Trial 172 finished with value: 0.13670885562896729 and parameters: {'attention_heads': 8, 'learning_rate': 0.002297883479977795, 'drop_out_out_decoder': 0.05, 'encoder_kernel_size': 8}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5265 - accuracy: 0.8449 - sparse_levenshtein_v1: 0.1543\n",
      "[I 2023-09-01 22:39:06,813] Trial 173 finished with value: 0.15430901944637299 and parameters: {'attention_heads': 8, 'learning_rate': 0.002895585421968237, 'drop_out_out_decoder': 0.05, 'encoder_kernel_size': 8}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.4880 - accuracy: 0.8571 - sparse_levenshtein_v1: 0.1424\n",
      "[I 2023-09-01 22:41:30,933] Trial 174 finished with value: 0.1423814743757248 and parameters: {'attention_heads': 8, 'learning_rate': 0.0023232168479149005, 'drop_out_out_decoder': 0.05, 'encoder_kernel_size': 8}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.6333 - accuracy: 0.8078 - sparse_levenshtein_v1: 0.1912\n",
      "[I 2023-09-01 22:43:54,896] Trial 175 finished with value: 0.19117394089698792 and parameters: {'attention_heads': 8, 'learning_rate': 0.0035391878291273875, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 8}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 4s 48ms/step - loss: 0.5020 - accuracy: 0.8524 - sparse_levenshtein_v1: 0.1468\n",
      "[I 2023-09-01 22:46:22,582] Trial 176 finished with value: 0.1468115746974945 and parameters: {'attention_heads': 8, 'learning_rate': 0.002086258228926414, 'drop_out_out_decoder': 0.05, 'encoder_kernel_size': 8}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 14ms/step - loss: 0.5889 - accuracy: 0.8229 - sparse_levenshtein_v1: 0.1765\n",
      "[I 2023-09-01 22:52:15,017] Trial 177 finished with value: 0.17645330727100372 and parameters: {'attention_heads': 8, 'learning_rate': 0.002762506758164428, 'drop_out_out_decoder': 0.5, 'encoder_kernel_size': 8}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 14ms/step - loss: 0.5279 - accuracy: 0.8446 - sparse_levenshtein_v1: 0.1547\n",
      "[I 2023-09-01 22:54:48,576] Trial 178 finished with value: 0.15467944741249084 and parameters: {'attention_heads': 8, 'learning_rate': 0.0017878829697992166, 'drop_out_out_decoder': 0.05, 'encoder_kernel_size': 8}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 14ms/step - loss: 0.6575 - accuracy: 0.7996 - sparse_levenshtein_v1: 0.1999\n",
      "[I 2023-09-01 22:57:22,976] Trial 179 finished with value: 0.1999482363462448 and parameters: {'attention_heads': 8, 'learning_rate': 0.005073391889780306, 'drop_out_out_decoder': 0.1, 'encoder_kernel_size': 8}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 14ms/step - loss: 0.4668 - accuracy: 0.8658 - sparse_levenshtein_v1: 0.1337\n",
      "[I 2023-09-01 22:59:57,235] Trial 180 finished with value: 0.13366694748401642 and parameters: {'attention_heads': 8, 'learning_rate': 0.0013441084636277877, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 7}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 14ms/step - loss: 0.4760 - accuracy: 0.8615 - sparse_levenshtein_v1: 0.1379\n",
      "[I 2023-09-01 23:02:30,874] Trial 181 finished with value: 0.1379261016845703 and parameters: {'attention_heads': 8, 'learning_rate': 0.0014019359750242898, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 6}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 14ms/step - loss: 0.4568 - accuracy: 0.8683 - sparse_levenshtein_v1: 0.1310\n",
      "[I 2023-09-01 23:05:05,200] Trial 182 finished with value: 0.1310426890850067 and parameters: {'attention_heads': 8, 'learning_rate': 0.0014286871057136023, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 5}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 2s 32ms/step - loss: 0.5697 - accuracy: 0.8317 - sparse_levenshtein_v1: 0.1669\n",
      "[I 2023-09-01 23:10:08,619] Trial 183 finished with value: 0.1668955534696579 and parameters: {'attention_heads': 8, 'learning_rate': 0.0013778450827253586, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 4}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5428 - accuracy: 0.8396 - sparse_levenshtein_v1: 0.1594\n",
      "[I 2023-09-01 23:13:05,932] Trial 184 finished with value: 0.1593906581401825 and parameters: {'attention_heads': 8, 'learning_rate': 0.0012991594629980089, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 5}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5478 - accuracy: 0.8388 - sparse_levenshtein_v1: 0.1598\n",
      "[I 2023-09-01 23:15:28,904] Trial 185 finished with value: 0.15977846086025238 and parameters: {'attention_heads': 8, 'learning_rate': 0.0008992775383023852, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 6}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.4865 - accuracy: 0.8579 - sparse_levenshtein_v1: 0.1415\n",
      "[I 2023-09-01 23:17:52,683] Trial 186 finished with value: 0.14149975776672363 and parameters: {'attention_heads': 8, 'learning_rate': 0.001888542904356914, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 5}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 14ms/step - loss: 0.4526 - accuracy: 0.8691 - sparse_levenshtein_v1: 0.1304\n",
      "[I 2023-09-01 23:20:18,441] Trial 187 finished with value: 0.1303894817829132 and parameters: {'attention_heads': 8, 'learning_rate': 0.0015326778327009814, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 6}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 14ms/step - loss: 0.4897 - accuracy: 0.8576 - sparse_levenshtein_v1: 0.1415\n",
      "[I 2023-09-01 23:23:01,909] Trial 188 finished with value: 0.14149412512779236 and parameters: {'attention_heads': 8, 'learning_rate': 0.0015159166595349936, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 5}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 14ms/step - loss: 0.5070 - accuracy: 0.8515 - sparse_levenshtein_v1: 0.1475\n",
      "[I 2023-09-01 23:25:39,278] Trial 189 finished with value: 0.14751145243644714 and parameters: {'attention_heads': 8, 'learning_rate': 0.0011115610517961239, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 6}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.6028 - accuracy: 0.8192 - sparse_levenshtein_v1: 0.1794\n",
      "[I 2023-09-01 23:28:14,224] Trial 190 finished with value: 0.17940539121627808 and parameters: {'attention_heads': 8, 'learning_rate': 0.0008080929371073255, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 6}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.4922 - accuracy: 0.8555 - sparse_levenshtein_v1: 0.1440\n",
      "[I 2023-09-01 23:30:37,288] Trial 191 finished with value: 0.14402863383293152 and parameters: {'attention_heads': 8, 'learning_rate': 0.0022249618836562373, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 6}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.4717 - accuracy: 0.8622 - sparse_levenshtein_v1: 0.1372\n",
      "[I 2023-09-01 23:33:01,074] Trial 192 finished with value: 0.13715441524982452 and parameters: {'attention_heads': 8, 'learning_rate': 0.0017934812629756376, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 7}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.4923 - accuracy: 0.8559 - sparse_levenshtein_v1: 0.1432\n",
      "[I 2023-09-01 23:35:25,045] Trial 193 finished with value: 0.14321306347846985 and parameters: {'attention_heads': 8, 'learning_rate': 0.0016412389422549897, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 7}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5734 - accuracy: 0.8287 - sparse_levenshtein_v1: 0.1707\n",
      "[I 2023-09-01 23:37:48,082] Trial 194 finished with value: 0.1707061529159546 and parameters: {'attention_heads': 8, 'learning_rate': 0.0013253048699133074, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 6}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.4940 - accuracy: 0.8557 - sparse_levenshtein_v1: 0.1436\n",
      "[I 2023-09-01 23:40:11,919] Trial 195 finished with value: 0.14357566833496094 and parameters: {'attention_heads': 8, 'learning_rate': 0.001891108586586853, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 7}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.4957 - accuracy: 0.8572 - sparse_levenshtein_v1: 0.1420\n",
      "[I 2023-09-01 23:42:34,750] Trial 196 finished with value: 0.14195220172405243 and parameters: {'attention_heads': 8, 'learning_rate': 0.001101042509323417, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 6}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5308 - accuracy: 0.8435 - sparse_levenshtein_v1: 0.1554\n",
      "[I 2023-09-01 23:44:58,477] Trial 197 finished with value: 0.15542510151863098 and parameters: {'attention_heads': 8, 'learning_rate': 0.001564371472135948, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 5}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.5558 - accuracy: 0.8350 - sparse_levenshtein_v1: 0.1636\n",
      "[I 2023-09-01 23:47:20,690] Trial 198 finished with value: 0.1635979562997818 and parameters: {'attention_heads': 8, 'learning_rate': 0.002104617945590743, 'drop_out_out_decoder': 0.0, 'encoder_kernel_size': 4}. Best is trial 78 with value: 0.12880994379520416.\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 0.4926 - accuracy: 0.8566 - sparse_levenshtein_v1: 0.1425\n",
      "[I 2023-09-01 23:49:35,724] Trial 199 finished with value: 0.14250344038009644 and parameters: {'attention_heads': 7, 'learning_rate': 0.0017770392814559564, 'drop_out_out_decoder': 0.05, 'encoder_kernel_size': 7}. Best is trial 78 with value: 0.12880994379520416.\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=TRIALS, gc_after_trial=True, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = study.best_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenTrial(number=78, state=TrialState.COMPLETE, values=[0.12880994379520416], datetime_start=datetime.datetime(2023, 9, 1, 18, 51, 42, 503274), datetime_complete=datetime.datetime(2023, 9, 1, 18, 53, 57, 726596), params={'attention_heads': 7, 'learning_rate': 0.0020283022893461916, 'drop_out_out_decoder': 0.15000000000000002, 'encoder_kernel_size': 9}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'attention_heads': IntDistribution(high=8, log=False, low=1, step=1), 'learning_rate': FloatDistribution(high=0.1, log=True, low=1e-05, step=None), 'drop_out_out_decoder': FloatDistribution(high=0.5, log=False, low=0.0, step=0.05), 'encoder_kernel_size': IntDistribution(high=12, log=False, low=3, step=1)}, trial_id=78, value=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: 1\n",
      "Model: \"finger_spelling_v2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " landmark_embedding_v2 (Lan  multiple                  75136     \n",
      " dmarkEmbeddingV2)                                               \n",
      "                                                                 \n",
      " basic_positional_embedding  multiple                  8064      \n",
      " s (BasicPositionalEmbeddin                                      \n",
      " gs)                                                             \n",
      "                                                                 \n",
      " transformer_encoder (Trans  multiple                  68944     \n",
      " formerEncoder)                                                  \n",
      "                                                                 \n",
      " transformer_decoder (Trans  multiple                  136406    \n",
      " formerDecoder)                                                  \n",
      "                                                                 \n",
      " dropout (Dropout)           multiple                  0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             multiple                  4030      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 292580 (1.12 MB)\n",
      "Trainable params: 292580 (1.12 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5000\n",
      "321/321 [==============================] - 15s 30ms/step - loss: 0.9245 - accuracy: 0.7491 - sparse_levenshtein_v1: 0.2424 - val_loss: 0.7713 - val_accuracy: 0.7718 - val_sparse_levenshtein_v1: 0.2253\n",
      "Epoch 2/5000\n",
      "321/321 [==============================] - 9s 27ms/step - loss: 0.7483 - accuracy: 0.7773 - sparse_levenshtein_v1: 0.2196 - val_loss: 0.7290 - val_accuracy: 0.7814 - val_sparse_levenshtein_v1: 0.2169\n",
      "Epoch 3/5000\n",
      "321/321 [==============================] - 9s 27ms/step - loss: 0.7175 - accuracy: 0.7845 - sparse_levenshtein_v1: 0.2138 - val_loss: 0.7100 - val_accuracy: 0.7861 - val_sparse_levenshtein_v1: 0.2128\n",
      "Epoch 4/5000\n",
      "321/321 [==============================] - 9s 27ms/step - loss: 0.7008 - accuracy: 0.7888 - sparse_levenshtein_v1: 0.2096 - val_loss: 0.6974 - val_accuracy: 0.7903 - val_sparse_levenshtein_v1: 0.2085\n",
      "Epoch 5/5000\n",
      "321/321 [==============================] - 9s 27ms/step - loss: 0.6889 - accuracy: 0.7917 - sparse_levenshtein_v1: 0.2069 - val_loss: 0.6869 - val_accuracy: 0.7919 - val_sparse_levenshtein_v1: 0.2070\n",
      "Epoch 6/5000\n",
      "321/321 [==============================] - 9s 27ms/step - loss: 0.6774 - accuracy: 0.7949 - sparse_levenshtein_v1: 0.2039 - val_loss: 0.6741 - val_accuracy: 0.7954 - val_sparse_levenshtein_v1: 0.2038\n",
      "Epoch 7/5000\n",
      "321/321 [==============================] - 9s 27ms/step - loss: 0.6696 - accuracy: 0.7969 - sparse_levenshtein_v1: 0.2019 - val_loss: 0.6755 - val_accuracy: 0.7950 - val_sparse_levenshtein_v1: 0.2040\n",
      "Epoch 8/5000\n",
      "321/321 [==============================] - 9s 27ms/step - loss: 0.6615 - accuracy: 0.7985 - sparse_levenshtein_v1: 0.2003 - val_loss: 0.6599 - val_accuracy: 0.7986 - val_sparse_levenshtein_v1: 0.2003\n",
      "Epoch 9/5000\n",
      "321/321 [==============================] - 9s 27ms/step - loss: 0.6527 - accuracy: 0.8009 - sparse_levenshtein_v1: 0.1979 - val_loss: 0.6534 - val_accuracy: 0.8015 - val_sparse_levenshtein_v1: 0.1973\n",
      "Epoch 10/5000\n",
      "321/321 [==============================] - 9s 27ms/step - loss: 0.6398 - accuracy: 0.8053 - sparse_levenshtein_v1: 0.1934 - val_loss: 0.6324 - val_accuracy: 0.8086 - val_sparse_levenshtein_v1: 0.1903\n",
      "Epoch 11/5000\n",
      "321/321 [==============================] - 9s 27ms/step - loss: 0.6235 - accuracy: 0.8110 - sparse_levenshtein_v1: 0.1877 - val_loss: 0.6197 - val_accuracy: 0.8126 - val_sparse_levenshtein_v1: 0.1864\n",
      "Epoch 12/5000\n",
      "321/321 [==============================] - 9s 27ms/step - loss: 0.6077 - accuracy: 0.8170 - sparse_levenshtein_v1: 0.1817 - val_loss: 0.6059 - val_accuracy: 0.8189 - val_sparse_levenshtein_v1: 0.1802\n",
      "Epoch 13/5000\n",
      "321/321 [==============================] - 9s 27ms/step - loss: 0.5868 - accuracy: 0.8252 - sparse_levenshtein_v1: 0.1735 - val_loss: 0.5729 - val_accuracy: 0.8308 - val_sparse_levenshtein_v1: 0.1682\n",
      "Epoch 14/5000\n",
      "321/321 [==============================] - 9s 27ms/step - loss: 0.5629 - accuracy: 0.8338 - sparse_levenshtein_v1: 0.1650 - val_loss: 0.5550 - val_accuracy: 0.8355 - val_sparse_levenshtein_v1: 0.1636\n",
      "Epoch 15/5000\n",
      "321/321 [==============================] - 9s 27ms/step - loss: 0.5426 - accuracy: 0.8407 - sparse_levenshtein_v1: 0.1582 - val_loss: 0.5265 - val_accuracy: 0.8462 - val_sparse_levenshtein_v1: 0.1526\n",
      "Epoch 16/5000\n",
      "321/321 [==============================] - 9s 27ms/step - loss: 0.5271 - accuracy: 0.8457 - sparse_levenshtein_v1: 0.1533 - val_loss: 0.5216 - val_accuracy: 0.8478 - val_sparse_levenshtein_v1: 0.1512\n",
      "Epoch 17/5000\n",
      "321/321 [==============================] - 9s 27ms/step - loss: 0.5173 - accuracy: 0.8489 - sparse_levenshtein_v1: 0.1502 - val_loss: 0.5119 - val_accuracy: 0.8504 - val_sparse_levenshtein_v1: 0.1489\n",
      "Epoch 18/5000\n",
      "321/321 [==============================] - 9s 27ms/step - loss: 0.5044 - accuracy: 0.8530 - sparse_levenshtein_v1: 0.1461 - val_loss: 0.5040 - val_accuracy: 0.8531 - val_sparse_levenshtein_v1: 0.1461\n",
      "Epoch 19/5000\n",
      "321/321 [==============================] - 9s 27ms/step - loss: 0.4960 - accuracy: 0.8555 - sparse_levenshtein_v1: 0.1438 - val_loss: 0.4883 - val_accuracy: 0.8573 - val_sparse_levenshtein_v1: 0.1418\n",
      "Epoch 20/5000\n",
      "321/321 [==============================] - 9s 27ms/step - loss: 0.4917 - accuracy: 0.8566 - sparse_levenshtein_v1: 0.1426 - val_loss: 0.4836 - val_accuracy: 0.8588 - val_sparse_levenshtein_v1: 0.1406\n",
      "Epoch 21/5000\n",
      "321/321 [==============================] - 9s 27ms/step - loss: 0.4842 - accuracy: 0.8588 - sparse_levenshtein_v1: 0.1404 - val_loss: 0.4791 - val_accuracy: 0.8604 - val_sparse_levenshtein_v1: 0.1389\n",
      "Epoch 22/5000\n",
      "321/321 [==============================] - 9s 27ms/step - loss: 0.4789 - accuracy: 0.8606 - sparse_levenshtein_v1: 0.1387 - val_loss: 0.4762 - val_accuracy: 0.8610 - val_sparse_levenshtein_v1: 0.1383\n",
      "Epoch 23/5000\n",
      "321/321 [==============================] - 9s 27ms/step - loss: 0.4748 - accuracy: 0.8618 - sparse_levenshtein_v1: 0.1376 - val_loss: 0.4706 - val_accuracy: 0.8632 - val_sparse_levenshtein_v1: 0.1361\n",
      "Epoch 24/5000\n",
      "321/321 [==============================] - 9s 27ms/step - loss: 0.4696 - accuracy: 0.8633 - sparse_levenshtein_v1: 0.1361 - val_loss: 0.4655 - val_accuracy: 0.8645 - val_sparse_levenshtein_v1: 0.1350\n",
      "Epoch 25/5000\n",
      "321/321 [==============================] - 9s 27ms/step - loss: 0.4635 - accuracy: 0.8652 - sparse_levenshtein_v1: 0.1342 - val_loss: 0.4654 - val_accuracy: 0.8640 - val_sparse_levenshtein_v1: 0.1356\n",
      "Epoch 26/5000\n",
      "321/321 [==============================] - 9s 27ms/step - loss: 0.4601 - accuracy: 0.8661 - sparse_levenshtein_v1: 0.1333 - val_loss: 0.4515 - val_accuracy: 0.8689 - val_sparse_levenshtein_v1: 0.1306\n",
      "Epoch 27/5000\n",
      "321/321 [==============================] - 9s 27ms/step - loss: 0.4581 - accuracy: 0.8666 - sparse_levenshtein_v1: 0.1329 - val_loss: 0.4549 - val_accuracy: 0.8675 - val_sparse_levenshtein_v1: 0.1320\n",
      "Epoch 28/5000\n",
      "321/321 [==============================] - 9s 27ms/step - loss: 0.4556 - accuracy: 0.8676 - sparse_levenshtein_v1: 0.1319 - val_loss: 0.4545 - val_accuracy: 0.8679 - val_sparse_levenshtein_v1: 0.1316\n",
      "Epoch 29/5000\n",
      "321/321 [==============================] - 9s 27ms/step - loss: 0.4486 - accuracy: 0.8697 - sparse_levenshtein_v1: 0.1297 - val_loss: 0.4531 - val_accuracy: 0.8677 - val_sparse_levenshtein_v1: 0.1318\n",
      "Epoch 30/5000\n",
      "321/321 [==============================] - 9s 27ms/step - loss: 0.4455 - accuracy: 0.8706 - sparse_levenshtein_v1: 0.1289 - val_loss: 0.4418 - val_accuracy: 0.8716 - val_sparse_levenshtein_v1: 0.1278\n",
      "Epoch 31/5000\n",
      "321/321 [==============================] - 9s 27ms/step - loss: 0.4425 - accuracy: 0.8716 - sparse_levenshtein_v1: 0.1279 - val_loss: 0.4480 - val_accuracy: 0.8701 - val_sparse_levenshtein_v1: 0.1294\n",
      "Epoch 32/5000\n",
      "321/321 [==============================] - 9s 27ms/step - loss: 0.4423 - accuracy: 0.8716 - sparse_levenshtein_v1: 0.1279 - val_loss: 0.4421 - val_accuracy: 0.8716 - val_sparse_levenshtein_v1: 0.1280\n",
      "Epoch 33/5000\n",
      "321/321 [==============================] - 9s 27ms/step - loss: 0.4380 - accuracy: 0.8729 - sparse_levenshtein_v1: 0.1267 - val_loss: 0.4414 - val_accuracy: 0.8718 - val_sparse_levenshtein_v1: 0.1277\n",
      "Epoch 34/5000\n",
      "321/321 [==============================] - 9s 27ms/step - loss: 0.4366 - accuracy: 0.8735 - sparse_levenshtein_v1: 0.1261 - val_loss: 0.4376 - val_accuracy: 0.8727 - val_sparse_levenshtein_v1: 0.1268\n",
      "Epoch 35/5000\n",
      "321/321 [==============================] - 9s 27ms/step - loss: 0.4382 - accuracy: 0.8729 - sparse_levenshtein_v1: 0.1267 - val_loss: 0.4451 - val_accuracy: 0.8709 - val_sparse_levenshtein_v1: 0.1285\n",
      "Epoch 36/5000\n",
      "321/321 [==============================] - 9s 27ms/step - loss: 0.4377 - accuracy: 0.8729 - sparse_levenshtein_v1: 0.1267 - val_loss: 0.4358 - val_accuracy: 0.8740 - val_sparse_levenshtein_v1: 0.1255\n",
      "Epoch 37/5000\n",
      "321/321 [==============================] - 9s 27ms/step - loss: 0.4354 - accuracy: 0.8738 - sparse_levenshtein_v1: 0.1257 - val_loss: 0.4279 - val_accuracy: 0.8760 - val_sparse_levenshtein_v1: 0.1235\n",
      "Epoch 38/5000\n",
      "321/321 [==============================] - 9s 27ms/step - loss: 0.4316 - accuracy: 0.8750 - sparse_levenshtein_v1: 0.1246 - val_loss: 0.4257 - val_accuracy: 0.8765 - val_sparse_levenshtein_v1: 0.1230\n",
      "Epoch 39/5000\n",
      "321/321 [==============================] - 9s 27ms/step - loss: 0.4275 - accuracy: 0.8762 - sparse_levenshtein_v1: 0.1234 - val_loss: 0.4373 - val_accuracy: 0.8731 - val_sparse_levenshtein_v1: 0.1267\n",
      "Epoch 40/5000\n",
      "321/321 [==============================] - 9s 27ms/step - loss: 0.4300 - accuracy: 0.8753 - sparse_levenshtein_v1: 0.1242 - val_loss: 0.4283 - val_accuracy: 0.8759 - val_sparse_levenshtein_v1: 0.1237\n",
      "Epoch 41/5000\n",
      "321/321 [==============================] - 9s 27ms/step - loss: 0.4295 - accuracy: 0.8755 - sparse_levenshtein_v1: 0.1241 - val_loss: 0.4251 - val_accuracy: 0.8772 - val_sparse_levenshtein_v1: 0.1224\n",
      "Epoch 42/5000\n",
      "321/321 [==============================] - 9s 29ms/step - loss: 0.4227 - accuracy: 0.8775 - sparse_levenshtein_v1: 0.1221 - val_loss: 0.4383 - val_accuracy: 0.8729 - val_sparse_levenshtein_v1: 0.1267\n",
      "Epoch 43/5000\n",
      "321/321 [==============================] - 10s 30ms/step - loss: 0.4231 - accuracy: 0.8773 - sparse_levenshtein_v1: 0.1223 - val_loss: 0.4288 - val_accuracy: 0.8758 - val_sparse_levenshtein_v1: 0.1240\n",
      "Epoch 44/5000\n",
      "321/321 [==============================] - 10s 30ms/step - loss: 0.4199 - accuracy: 0.8784 - sparse_levenshtein_v1: 0.1212 - val_loss: 0.4389 - val_accuracy: 0.8725 - val_sparse_levenshtein_v1: 0.1270\n",
      "Epoch 45/5000\n",
      "321/321 [==============================] - 10s 30ms/step - loss: 0.4256 - accuracy: 0.8765 - sparse_levenshtein_v1: 0.1231 - val_loss: 0.4291 - val_accuracy: 0.8760 - val_sparse_levenshtein_v1: 0.1234\n",
      "Epoch 46/5000\n",
      "321/321 [==============================] - 10s 30ms/step - loss: 0.4238 - accuracy: 0.8774 - sparse_levenshtein_v1: 0.1222 - val_loss: 0.4183 - val_accuracy: 0.8797 - val_sparse_levenshtein_v1: 0.1199\n",
      "Epoch 47/5000\n",
      "321/321 [==============================] - 10s 30ms/step - loss: 0.4247 - accuracy: 0.8771 - sparse_levenshtein_v1: 0.1226 - val_loss: 0.4304 - val_accuracy: 0.8756 - val_sparse_levenshtein_v1: 0.1239\n",
      "Epoch 48/5000\n",
      "321/321 [==============================] - 10s 30ms/step - loss: 0.4242 - accuracy: 0.8770 - sparse_levenshtein_v1: 0.1226 - val_loss: 0.4287 - val_accuracy: 0.8761 - val_sparse_levenshtein_v1: 0.1234\n",
      "Epoch 49/5000\n",
      "321/321 [==============================] - 10s 30ms/step - loss: 0.4219 - accuracy: 0.8778 - sparse_levenshtein_v1: 0.1218 - val_loss: 0.4178 - val_accuracy: 0.8793 - val_sparse_levenshtein_v1: 0.1204\n",
      "Epoch 50/5000\n",
      "321/321 [==============================] - 10s 30ms/step - loss: 0.4153 - accuracy: 0.8797 - sparse_levenshtein_v1: 0.1199 - val_loss: 0.4193 - val_accuracy: 0.8787 - val_sparse_levenshtein_v1: 0.1208\n",
      "Epoch 51/5000\n",
      "321/321 [==============================] - 10s 30ms/step - loss: 0.4184 - accuracy: 0.8787 - sparse_levenshtein_v1: 0.1210 - val_loss: 0.4185 - val_accuracy: 0.8790 - val_sparse_levenshtein_v1: 0.1207\n",
      "Epoch 52/5000\n",
      "321/321 [==============================] - 10s 30ms/step - loss: 0.4188 - accuracy: 0.8786 - sparse_levenshtein_v1: 0.1210 - val_loss: 0.4456 - val_accuracy: 0.8710 - val_sparse_levenshtein_v1: 0.1286\n",
      "Epoch 53/5000\n",
      "321/321 [==============================] - 10s 30ms/step - loss: 0.4177 - accuracy: 0.8790 - sparse_levenshtein_v1: 0.1206 - val_loss: 0.4200 - val_accuracy: 0.8784 - val_sparse_levenshtein_v1: 0.1212\n",
      "Epoch 54/5000\n",
      "321/321 [==============================] - 10s 30ms/step - loss: 0.4197 - accuracy: 0.8783 - sparse_levenshtein_v1: 0.1213 - val_loss: 0.4171 - val_accuracy: 0.8795 - val_sparse_levenshtein_v1: 0.1202\n",
      "Epoch 55/5000\n",
      "321/321 [==============================] - 10s 30ms/step - loss: 0.4170 - accuracy: 0.8792 - sparse_levenshtein_v1: 0.1204 - val_loss: 0.4179 - val_accuracy: 0.8792 - val_sparse_levenshtein_v1: 0.1205\n",
      "Epoch 56/5000\n",
      "321/321 [==============================] - 10s 30ms/step - loss: 0.4131 - accuracy: 0.8802 - sparse_levenshtein_v1: 0.1194 - val_loss: 0.4149 - val_accuracy: 0.8802 - val_sparse_levenshtein_v1: 0.1194\n",
      "Epoch 57/5000\n",
      "321/321 [==============================] - 10s 30ms/step - loss: 0.4150 - accuracy: 0.8796 - sparse_levenshtein_v1: 0.1200 - val_loss: 0.4152 - val_accuracy: 0.8798 - val_sparse_levenshtein_v1: 0.1199\n",
      "Epoch 58/5000\n",
      "321/321 [==============================] - 10s 30ms/step - loss: 0.4111 - accuracy: 0.8810 - sparse_levenshtein_v1: 0.1187 - val_loss: 0.4159 - val_accuracy: 0.8797 - val_sparse_levenshtein_v1: 0.1198\n",
      "Epoch 59/5000\n",
      "321/321 [==============================] - 10s 30ms/step - loss: 0.4091 - accuracy: 0.8812 - sparse_levenshtein_v1: 0.1184 - val_loss: 0.4124 - val_accuracy: 0.8808 - val_sparse_levenshtein_v1: 0.1188\n",
      "Epoch 60/5000\n",
      "321/321 [==============================] - 10s 30ms/step - loss: 0.4120 - accuracy: 0.8805 - sparse_levenshtein_v1: 0.1192 - val_loss: 0.4232 - val_accuracy: 0.8775 - val_sparse_levenshtein_v1: 0.1223\n",
      "Epoch 61/5000\n",
      "321/321 [==============================] - 10s 30ms/step - loss: 0.4118 - accuracy: 0.8806 - sparse_levenshtein_v1: 0.1191 - val_loss: 0.4140 - val_accuracy: 0.8806 - val_sparse_levenshtein_v1: 0.1190\n",
      "Epoch 62/5000\n",
      "321/321 [==============================] - 10s 30ms/step - loss: 0.4140 - accuracy: 0.8799 - sparse_levenshtein_v1: 0.1198 - val_loss: 0.4105 - val_accuracy: 0.8818 - val_sparse_levenshtein_v1: 0.1179\n",
      "Epoch 63/5000\n",
      "321/321 [==============================] - 10s 30ms/step - loss: 0.4166 - accuracy: 0.8791 - sparse_levenshtein_v1: 0.1205 - val_loss: 0.4425 - val_accuracy: 0.8717 - val_sparse_levenshtein_v1: 0.1280\n",
      "Epoch 64/5000\n",
      "321/321 [==============================] - 10s 30ms/step - loss: 0.4153 - accuracy: 0.8794 - sparse_levenshtein_v1: 0.1202 - val_loss: 0.4113 - val_accuracy: 0.8810 - val_sparse_levenshtein_v1: 0.1186\n",
      "Epoch 65/5000\n",
      "321/321 [==============================] - 10s 30ms/step - loss: 0.4085 - accuracy: 0.8816 - sparse_levenshtein_v1: 0.1181 - val_loss: 0.4035 - val_accuracy: 0.8836 - val_sparse_levenshtein_v1: 0.1160\n",
      "Epoch 66/5000\n",
      "321/321 [==============================] - 10s 30ms/step - loss: 0.4102 - accuracy: 0.8810 - sparse_levenshtein_v1: 0.1187 - val_loss: 0.4230 - val_accuracy: 0.8774 - val_sparse_levenshtein_v1: 0.1222\n",
      "Epoch 67/5000\n",
      "321/321 [==============================] - 10s 30ms/step - loss: 0.4093 - accuracy: 0.8814 - sparse_levenshtein_v1: 0.1182 - val_loss: 0.4163 - val_accuracy: 0.8795 - val_sparse_levenshtein_v1: 0.1201\n",
      "Epoch 68/5000\n",
      "321/321 [==============================] - 10s 30ms/step - loss: 0.4190 - accuracy: 0.8785 - sparse_levenshtein_v1: 0.1212 - val_loss: 0.4150 - val_accuracy: 0.8803 - val_sparse_levenshtein_v1: 0.1194\n",
      "Epoch 69/5000\n",
      "321/321 [==============================] - 10s 30ms/step - loss: 0.4142 - accuracy: 0.8798 - sparse_levenshtein_v1: 0.1198 - val_loss: 0.4384 - val_accuracy: 0.8734 - val_sparse_levenshtein_v1: 0.1261\n",
      "Epoch 70/5000\n",
      "321/321 [==============================] - 10s 30ms/step - loss: 0.4135 - accuracy: 0.8800 - sparse_levenshtein_v1: 0.1197 - val_loss: 0.4222 - val_accuracy: 0.8780 - val_sparse_levenshtein_v1: 0.1216\n",
      "Epoch 71/5000\n",
      "321/321 [==============================] - 10s 30ms/step - loss: 0.4083 - accuracy: 0.8815 - sparse_levenshtein_v1: 0.1181 - val_loss: 0.4125 - val_accuracy: 0.8806 - val_sparse_levenshtein_v1: 0.1191\n",
      "Epoch 72/5000\n",
      "321/321 [==============================] - 10s 30ms/step - loss: 0.4165 - accuracy: 0.8790 - sparse_levenshtein_v1: 0.1207 - val_loss: 0.4213 - val_accuracy: 0.8781 - val_sparse_levenshtein_v1: 0.1216\n",
      "Epoch 73/5000\n",
      "321/321 [==============================] - 10s 30ms/step - loss: 0.4087 - accuracy: 0.8814 - sparse_levenshtein_v1: 0.1183 - val_loss: 0.4136 - val_accuracy: 0.8802 - val_sparse_levenshtein_v1: 0.1193\n",
      "Epoch 74/5000\n",
      "321/321 [==============================] - 10s 30ms/step - loss: 0.4123 - accuracy: 0.8805 - sparse_levenshtein_v1: 0.1192 - val_loss: 0.4285 - val_accuracy: 0.8762 - val_sparse_levenshtein_v1: 0.1235\n",
      "Epoch 75/5000\n",
      "321/321 [==============================] - 10s 30ms/step - loss: 0.4129 - accuracy: 0.8801 - sparse_levenshtein_v1: 0.1196 - val_loss: 0.4391 - val_accuracy: 0.8733 - val_sparse_levenshtein_v1: 0.1262\n",
      "validation levenshtein distance: 0.12880994379520416\n",
      "Best hyperparameters: {'attention_heads': 7, 'learning_rate': 0.0020283022893461916, 'drop_out_out_decoder': 0.15000000000000002, 'encoder_kernel_size': 9}\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.4035 - accuracy: 0.8836 - sparse_levenshtein_v1: 0.1160\n",
      "Metrics in Validation: [0.4034680426120758, 0.8836108446121216, 0.11596878618001938]\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "trials = study.best_trials\n",
    "\n",
    "for index, trial in enumerate(trials):\n",
    "    print(f\"Best model: {index+1}\")\n",
    "\n",
    "    model = build_prod_transformer_model_v2(trial=trial)\n",
    "\n",
    "    model.build([(None, MAX_LENGHT_SOURCE, int(FEATURE_COLUMNS.shape[0]/2)), (None, TARGET_MAX_LENGHT)])\n",
    "\n",
    "    print(model.summary())\n",
    "    model.fit(train_dataset, validation_data=val_dataset, epochs=EPOCHS, callbacks=get_predefine_callbacks(model_name=MODEL_NAME, patience=10))\n",
    "   \n",
    "    print('validation levenshtein distance: {}'.format(trial.value))\n",
    "    print(\"Best hyperparameters: {}\".format(trial.params))\n",
    "\n",
    "    model.load_weights(f\"../best_model/prototype/{MODEL_NAME}\")\n",
    "\n",
    "    print(f\"Metrics in Validation: {model.evaluate(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/prod_v2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/prod_v2/assets\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "\n",
    "model.save(f\"../models/{MODEL_NAME}\", save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.export_tf_lite.model import TFLiteModel\n",
    "\n",
    "tflitemodel_base = TFLiteModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'outputs': <tf.Tensor: shape=(18, 59), dtype=float32, numpy=\n",
       " array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflitemodel_base(tf.random.uniform(shape=[128, 104]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpgbn00xou/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpgbn00xou/assets\n"
     ]
    }
   ],
   "source": [
    "keras_model_converter = tf.lite.TFLiteConverter.from_keras_model(tflitemodel_base)\n",
    "keras_model_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "keras_model_converter.allow_custom_ops = True\n",
    "keras_model_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "tflite_model = keras_model_converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../models/model.tflite\", \"wb\") as f:    \n",
    "    f.write(tflite_model)\n",
    "    \n",
    "infargs = {\"selected_columns\" : list(FEATURE_COLUMNS)}\n",
    "\n",
    "with open(\"../models/inference_args.json\", \"w\") as json_file:\n",
    "    json.dump(infargs, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: ../models/model.tflite (deflated 78%)\n",
      "updating: ../models/inference_args.json (deflated 84%)\n"
     ]
    }
   ],
   "source": [
    "!zip submission.zip  '../models/model.tflite' '../models/inference_args.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from batch 1\n",
    "\n",
    "source_batch, target_batch = next(iter(val_dataset))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated:  784-084-0800\n",
      "target:  www.empresadelimpieza.org\n",
      "generated:  885 co 10\n",
      "target:  5 north 3029th road\n",
      "generated:  808 dowe\n",
      "target:  seller-online.net/shonanholic\n",
      "generated:  958-588-1601\n",
      "target:  eduardbatlle.cat\n",
      "generated:  4088 dee 8849\n",
      "target:  867-401-2165\n",
      "generated:  9848 480\n",
      "target:  35 davidoff drive\n",
      "generated:  4888 484\n",
      "target:  856-675-9951\n",
      "generated:  458-848-2488\n",
      "target:  621919 101da jimmerson\n",
      "generated:  788 7849\n",
      "target:  1150 apostol\n",
      "generated:  778-888-8101\n",
      "target:  992-961-3811\n",
      "generated:  8984 848\n",
      "target:  320846 skaggs st\n",
      "generated:  beende con\n",
      "target:  401 nw via della ct\n",
      "generated:  medie\n",
      "target:  3091 485\n",
      "generated:  8888 888\n",
      "target:  +30-1286-33-43-8907\n",
      "generated:  meffree be\n",
      "target:  5459 wells cemetery no 1\n",
      "generated:  8888 888\n",
      "target:  +32-8177-3222-69\n",
      "generated:  6787 core\n",
      "target:  www.driverscape.com\n",
      "generated:  400-048-0001\n",
      "target:  7011 henry circle\n",
      "generated:  5888 co 10\n",
      "target:  9365 carmel airport\n",
      "generated:  858-888-3161\n",
      "target:  177 kipawa court\n",
      "generated:  484\n",
      "target:  norberto wolf\n",
      "generated:  890-898-3848\n",
      "target:  thelil.bakerdelhi\n",
      "generated:  4898 co 10\n",
      "target:  jeannine gibbs\n",
      "generated:  498-888-4810\n",
      "target:  509-105-9769\n",
      "generated:  7788 80\n",
      "target:  brooks crawford\n",
      "generated:  8998 co 10\n",
      "target:  395-311-7741\n",
      "generated:  849-078-1008\n",
      "target:  /specialtoolboodyrepair\n",
      "generated:  805 co 108\n",
      "target:  www.skisises.it/2021-02-24\n",
      "generated:  4858 carring\n",
      "target:  9848 dep trail 1\n",
      "generated:  +48-84-84-88\n",
      "target:  +47-75-68390\n",
      "generated:  7848 4889\n",
      "target:  16 jim cleveland\n",
      "generated:  5858 co 108\n",
      "target:  +43-7440-569-5526\n",
      "generated:  784-088-0888\n",
      "target:  www.e-pro.fr/jewelry+suitcase\n",
      "generated:  898 fie 808\n",
      "target:  kenyatta zamora\n",
      "generated:  8878 beel\n",
      "target:  1391 deboa\n",
      "generated:  858-888-8880\n",
      "target:  +57-3080-0923\n",
      "generated:  787-888-2888\n",
      "target:  138-877-9522\n",
      "generated:  m88980\n",
      "target:  chauncey harding\n",
      "generated:  888 e\n",
      "target:  kristy vargas\n",
      "generated:  980-488-1088\n",
      "target:  /la-fondazione-carpinetum\n",
      "generated:  878 88\n",
      "target:  +41-935-856-113\n",
      "generated:  989-848-2988\n",
      "target:  alexandria virginia\n",
      "generated:  eima core\n",
      "target:  766-005-3175\n",
      "generated:  4968 ferfer\n",
      "target:  9055 west highway 158\n",
      "generated:  8888 708\n",
      "target:  7810 e revis bluff rd\n",
      "generated:  7 dirop\n",
      "target:  974-070-5788\n",
      "generated:  898 feon\n",
      "target:  www.neviscycles.com/5799\n",
      "generated:  808-898-2048\n",
      "target:  7285 petruzzello\n",
      "generated:  4888 e\n",
      "target:  87 binacos\n",
      "generated:  neer\n",
      "target:  brokkeng\n",
      "generated:  888-858-1100\n",
      "target:  atmtamilnovels.com/66073\n",
      "generated:  4888 co 8\n",
      "target:  6087 nelson prater cemetery\n",
      "generated:  nice co roa\n",
      "target:  382 pamona ct\n",
      "generated:  9878 7044\n",
      "target:  1641 north 45th dale\n",
      "generated:  8308 co 109\n",
      "target:  32_32737/alternatore/\n",
      "generated:  888-988-0010\n",
      "target:  699-721-6795\n",
      "generated:  8588 ferrin\n",
      "target:  www.apexchauffage.be/\n",
      "generated:  748-488-2488\n",
      "target:  515405 ashley lauren dr\n",
      "generated:  9788 e 818\n",
      "target:  580-661-4773\n",
      "generated:  781 denie\n",
      "target:  +680-225-9053\n",
      "generated:  edee farmie\n",
      "target:  www.pedersgatautvikling.no\n",
      "generated:  7888 88\n",
      "target:  785 nfs 200 road\n",
      "generated:  4888 88\n",
      "target:  813-583-6467\n",
      "generated:  8888 88\n",
      "target:  tyra arias\n",
      "generated:  749-888-4888\n",
      "target:  www.najacedojave.com\n",
      "generated:  nie 8859\n",
      "target:  chemical-name-for-cialis\n",
      "generated:  4888 e 804\n",
      "target:  creator.nightcafe.studio\n",
      "generated:  8888888\n",
      "target:  968-022-6134\n",
      "generated:  488-848-0498\n",
      "target:  www.elteclado.com.ar/16878\n",
      "generated:  898-980-3888\n",
      "target:  www.imei.info/video-card\n",
      "generated:  8588 barra court\n",
      "target:  webmail.moyolegalpractice.com\n",
      "generated:  8989 bill\n",
      "target:  412-228-8214\n",
      "generated:  9888 48\n",
      "target:  lawanda olsen\n",
      "generated:  848-888-1088\n",
      "target:  http://www.glocktalk.com/\n",
      "generated:  8488 80\n",
      "target:  92 field avenue\n",
      "generated:  9808 mie\n",
      "target:  3848 ranch 1430\n",
      "generated:  8888 fe\n",
      "target:  +290-0095-38\n",
      "generated:  7888 bee\n",
      "target:  www.vins-gites-stoll.fr/\n",
      "generated:  8880 e\n",
      "target:  2137 ragersville\n",
      "generated:  marie fie\n",
      "target:  151-015-5105\n",
      "generated:  deferc\n",
      "target:  460 youts\n",
      "generated:  8888 488\n",
      "target:  www.reciproqueservices.fr\n",
      "generated:  808 e\n",
      "target:  tammi fields\n",
      "generated:  787 bee\n",
      "target:  +963-160-152-38-39-157\n",
      "generated:  498-828-4780\n",
      "target:  solvesy.net/4454087/swoop\n",
      "generated:  828-807-9788\n",
      "target:  417-147-0769\n",
      "generated:  8788 78\n",
      "target:  www.webseolink.com\n",
      "generated:  480-048-2089\n",
      "target:  kylene galindo\n",
      "generated:  888 co 10\n",
      "target:  www.doncastercivictrust.org.uk\n",
      "generated:  804 n 804\n",
      "target:  3301 south 201st court\n",
      "generated:  406-888-5988\n",
      "target:  907 cll alto de cuba b\n",
      "generated:  782-889-8989\n",
      "target:  sothysinstitut/\n",
      "generated:  408-808-8488\n",
      "target:  6152 north 13th circle\n",
      "generated:  8888 808\n",
      "target:  454 faucher\n",
      "generated:  8810 1049\n",
      "target:  401224 old co road 95\n",
      "generated:  9488 e\n",
      "target:  graciela bullock\n",
      "generated:  800 4889\n",
      "target:  615-001-9753\n",
      "generated:  818-848-8488\n",
      "target:  1161 white columns\n",
      "generated:  4888 e 804\n",
      "target:  +376-0061-59-607-769-84\n",
      "generated:  458-888-2889\n",
      "target:  digitalarchives.tw/enchroma\n",
      "generated:  808-898-3168\n",
      "target:  +995-98-98-4886\n",
      "generated:  8104 8078\n",
      "target:  sambatech.com/~auerbach\n",
      "generated:  4880 north\n",
      "target:  hacksauction.com/app-voicetra\n",
      "generated:  macie bell\n",
      "target:  8126 randamar\n",
      "generated:  merie delle\n",
      "target:  918 pumpkin ts\n",
      "generated:  9480 carl\n",
      "target:  686-431-4451\n",
      "generated:  488-988-4488\n",
      "target:  marti-soler/freitassene\n",
      "generated:  8978 east\n",
      "target:  tammy douglas\n",
      "generated:  708 4898\n",
      "target:  41052/michio_kaku/\n",
      "generated:  enford deeef\n",
      "target:  tr.auto-data.org/\n",
      "generated:  8888 bee\n",
      "target:  +355-346-9310-430-2613\n",
      "generated:  580 co 10\n",
      "target:  +82-65-43-00-96\n",
      "generated:  859-588-2488\n",
      "target:  704-090-3742\n",
      "generated:  8858 be\n",
      "target:  summer holt\n",
      "generated:  480 4089\n",
      "target:  westhoekverbeeldt.be/animenyc/\n",
      "generated:  498 domfe\n",
      "target:  523-644-0366\n",
      "generated:  800 1048\n",
      "target:  9042 twp rd 6504\n",
      "generated:  0780 10\n",
      "target:  pete powers\n",
      "generated:  890 e\n",
      "target:  620-780-5529\n",
      "generated:  888 80\n",
      "target:  282 marsh trail\n",
      "generated:  858 co 180\n",
      "target:  +227-68-5351-925-615-83\n",
      "generated:  1888 bee\n",
      "target:  www.ruiouwir.co\n",
      "generated:  sonor dar\n",
      "target:  v-is-for-vanya\n",
      "generated:  809 e\n",
      "target:  627 deranger\n",
      "generated:  898-848-8488\n",
      "target:  4396 north halter road\n",
      "generated:  888 ee\n",
      "target:  jaron bradford\n",
      "generated:  4888 48\n",
      "target:  8 hepburn exd\n",
      "generated:  709-808-8088\n",
      "target:  4820 north duncan creek court\n"
     ]
    }
   ],
   "source": [
    "REQUIRED_SIGNATURE = \"serving_default\"\n",
    "REQUIRED_OUTPUT = \"outputs\"\n",
    "\n",
    "interpreter = tf.lite.Interpreter(\"../models/model.tflite\")\n",
    "\n",
    "with open (\"../data/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n",
    "    character_map = json.load(f)\n",
    "\n",
    "rev_character_map = {j:i for i,j in character_map.items()}\n",
    "found_signatures = list(interpreter.get_signature_list().keys())\n",
    "\n",
    "if REQUIRED_SIGNATURE not in found_signatures:\n",
    "    raise KernelEvalException('Required input signature not found.')\n",
    "\n",
    "prediction_fn = interpreter.get_signature_runner(REQUIRED_SIGNATURE)\n",
    "\n",
    "prediction_str = \"\"\n",
    "for source_element, target_element in zip(source_batch, target_batch):\n",
    "    output = prediction_fn(inputs=source_element)\n",
    "\n",
    "    print(\"generated: \", \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output[REQUIRED_OUTPUT], axis=1)]))\n",
    "    print(\"target: \", \"\".join([rev_character_map.get(s, \"\") for s in target_element.numpy()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'serving_default_inputs:0',\n",
       "  'index': 0,\n",
       "  'shape': array([128,  52], dtype=int32),\n",
       "  'shape_signature': array([ -1, 104], dtype=int32),\n",
       "  'dtype': numpy.float32,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
       "   'zero_points': array([], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpreter.get_input_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fingerspelling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
