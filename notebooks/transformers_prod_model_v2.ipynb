{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "\n",
    "from src.constants import TARGET_MAX_LENGHT, MAX_LENGHT_SOURCE, FEATURE_COLUMNS\n",
    "from src.data_utils.dataset import build_datset_train_val, VOCAB_SIZE, LHAND_IDX, LHAND_IDX, start_token_idx, end_token_idx, pre_process, pad_token_idx\n",
    "from src.prod_models.builder import build_prod_transformer_model_v2\n",
    "from src.callbacks import get_predefine_callbacks\n",
    "import optuna\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "TRIALS = 80\n",
    "EPOCHS = 5000\n",
    "EPOCHS_PER_TRIAL = 10\n",
    "BATCH_SIZE = 128\n",
    "TRAIN_SPLIT = 0.8\n",
    "MODEL_NAME = \"prod_v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train split: 28160 | val split: 6656\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset = build_datset_train_val(split=TRAIN_SPLIT, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    tf.keras.backend.clear_session()\n",
    "    model = build_prod_transformer_model_v2(trial=trial)\n",
    "    model.build([(None, MAX_LENGHT_SOURCE, int(FEATURE_COLUMNS.shape[0]/2)), (None, TARGET_MAX_LENGHT)])\n",
    "    model.fit(train_dataset, validation_data=val_dataset, epochs=EPOCHS_PER_TRIAL, callbacks=get_predefine_callbacks(model_name=MODEL_NAME, patience=3), verbose=0)\n",
    "    levenshtein = model.evaluate(val_dataset)[-1]\n",
    "\n",
    "    return  levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-24 16:09:13,866] A new study created in memory with name: no-name-dd6f015f-36a4-4f77-a50c-9eb5d9ad08b8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36ff362dfb7f478d982186d9fd83520a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77/77 [==============================] - 1s 13ms/step - loss: 0.7873 - accuracy: 0.7697 - sparse_levenshtein_v1: 0.2278\n",
      "[I 2023-08-24 16:10:54,821] Trial 0 finished with value: 0.22779132425785065 and parameters: {'attention_heads': 3, 'learning_rate': 8.487681652646925e-05, 'drop_out_out_decoder': 0.15000000000000002, 'dense_layers': 4, 'drop_out': 0.05, 'encoder_kernel_size': 9}. Best is trial 0 with value: 0.22779132425785065.\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.7068 - accuracy: 0.7880 - sparse_levenshtein_v1: 0.2105\n",
      "[I 2023-08-24 16:13:00,316] Trial 1 finished with value: 0.21052058041095734 and parameters: {'attention_heads': 5, 'learning_rate': 0.0008058347570333092, 'drop_out_out_decoder': 0.1, 'dense_layers': 3, 'drop_out': 0.0, 'encoder_kernel_size': 12}. Best is trial 1 with value: 0.21052058041095734.\n",
      "77/77 [==============================] - 2s 21ms/step - loss: 0.7722 - accuracy: 0.7716 - sparse_levenshtein_v1: 0.2248\n",
      "[I 2023-08-24 16:14:52,580] Trial 2 finished with value: 0.22480256855487823 and parameters: {'attention_heads': 8, 'learning_rate': 0.00907519569562131, 'drop_out_out_decoder': 0.1, 'dense_layers': 1, 'drop_out': 0.30000000000000004, 'encoder_kernel_size': 8}. Best is trial 1 with value: 0.21052058041095734.\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 0.7465 - accuracy: 0.7782 - sparse_levenshtein_v1: 0.2199\n",
      "[I 2023-08-24 16:16:18,662] Trial 3 finished with value: 0.21985526382923126 and parameters: {'attention_heads': 2, 'learning_rate': 0.0032972422796030032, 'drop_out_out_decoder': 0.05, 'dense_layers': 4, 'drop_out': 0.0, 'encoder_kernel_size': 4}. Best is trial 1 with value: 0.21052058041095734.\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.8986 - accuracy: 0.7557 - sparse_levenshtein_v1: 0.2384\n",
      "[I 2023-08-24 16:18:35,804] Trial 4 finished with value: 0.23836621642112732 and parameters: {'attention_heads': 6, 'learning_rate': 1.2353470333124664e-05, 'drop_out_out_decoder': 0.05, 'dense_layers': 2, 'drop_out': 0.30000000000000004, 'encoder_kernel_size': 5}. Best is trial 1 with value: 0.21052058041095734.\n",
      "77/77 [==============================] - 2s 20ms/step - loss: 0.8398 - accuracy: 0.7655 - sparse_levenshtein_v1: 0.2292\n",
      "[I 2023-08-24 16:21:15,625] Trial 5 finished with value: 0.22923095524311066 and parameters: {'attention_heads': 8, 'learning_rate': 2.216739253955281e-05, 'drop_out_out_decoder': 0.15000000000000002, 'dense_layers': 1, 'drop_out': 0.4, 'encoder_kernel_size': 12}. Best is trial 1 with value: 0.21052058041095734.\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.8648 - accuracy: 0.7450 - sparse_levenshtein_v1: 0.2479\n",
      "[I 2023-08-24 16:22:26,371] Trial 6 finished with value: 0.2478945255279541 and parameters: {'attention_heads': 1, 'learning_rate': 0.0001245880503488796, 'drop_out_out_decoder': 0.05, 'dense_layers': 1, 'drop_out': 0.45, 'encoder_kernel_size': 11}. Best is trial 1 with value: 0.21052058041095734.\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 4.1226 - accuracy: 0.7186 - sparse_levenshtein_v1: 0.2788\n",
      "[I 2023-08-24 16:23:20,392] Trial 7 finished with value: 0.2787582278251648 and parameters: {'attention_heads': 5, 'learning_rate': 0.031151791851898807, 'drop_out_out_decoder': 0.30000000000000004, 'dense_layers': 4, 'drop_out': 0.45, 'encoder_kernel_size': 8}. Best is trial 1 with value: 0.21052058041095734.\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.7098 - accuracy: 0.7869 - sparse_levenshtein_v1: 0.2120\n",
      "[I 2023-08-24 16:25:27,681] Trial 8 finished with value: 0.212024986743927 and parameters: {'attention_heads': 5, 'learning_rate': 0.0013975444022210892, 'drop_out_out_decoder': 0.5, 'dense_layers': 5, 'drop_out': 0.05, 'encoder_kernel_size': 11}. Best is trial 1 with value: 0.21052058041095734.\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 0.8734 - accuracy: 0.7539 - sparse_levenshtein_v1: 0.2401\n",
      "[I 2023-08-24 16:26:55,348] Trial 9 finished with value: 0.24013186991214752 and parameters: {'attention_heads': 2, 'learning_rate': 2.4924589521627035e-05, 'drop_out_out_decoder': 0.5, 'dense_layers': 5, 'drop_out': 0.1, 'encoder_kernel_size': 7}. Best is trial 1 with value: 0.21052058041095734.\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.7208 - accuracy: 0.7836 - sparse_levenshtein_v1: 0.2148\n",
      "[I 2023-08-24 16:29:07,202] Trial 10 finished with value: 0.21476219594478607 and parameters: {'attention_heads': 6, 'learning_rate': 0.0002609806921076431, 'drop_out_out_decoder': 0.30000000000000004, 'dense_layers': 3, 'drop_out': 0.15000000000000002, 'encoder_kernel_size': 10}. Best is trial 1 with value: 0.21052058041095734.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.7251 - accuracy: 0.7841 - sparse_levenshtein_v1: 0.2140\n",
      "[I 2023-08-24 16:30:47,399] Trial 11 finished with value: 0.21400688588619232 and parameters: {'attention_heads': 4, 'learning_rate': 0.0010097659021990355, 'drop_out_out_decoder': 0.5, 'dense_layers': 3, 'drop_out': 0.2, 'encoder_kernel_size': 12}. Best is trial 1 with value: 0.21052058041095734.\n",
      "77/77 [==============================] - 1s 14ms/step - loss: 0.7071 - accuracy: 0.7876 - sparse_levenshtein_v1: 0.2110\n",
      "[I 2023-08-24 16:32:38,991] Trial 12 finished with value: 0.21104077994823456 and parameters: {'attention_heads': 5, 'learning_rate': 0.0007888329906625834, 'drop_out_out_decoder': 0.4, 'dense_layers': 5, 'drop_out': 0.0, 'encoder_kernel_size': 10}. Best is trial 1 with value: 0.21052058041095734.\n",
      "77/77 [==============================] - 1s 15ms/step - loss: 0.7091 - accuracy: 0.7866 - sparse_levenshtein_v1: 0.2123\n",
      "[I 2023-08-24 16:34:37,129] Trial 13 finished with value: 0.2123194932937622 and parameters: {'attention_heads': 6, 'learning_rate': 0.0004989588411361277, 'drop_out_out_decoder': 0.4, 'dense_layers': 2, 'drop_out': 0.0, 'encoder_kernel_size': 10}. Best is trial 1 with value: 0.21052058041095734.\n",
      "77/77 [==============================] - 1s 14ms/step - loss: 0.7779 - accuracy: 0.7709 - sparse_levenshtein_v1: 0.2261\n",
      "[I 2023-08-24 16:36:19,758] Trial 14 finished with value: 0.22611753642559052 and parameters: {'attention_heads': 4, 'learning_rate': 0.003368778268788615, 'drop_out_out_decoder': 0.2, 'dense_layers': 2, 'drop_out': 0.15000000000000002, 'encoder_kernel_size': 6}. Best is trial 1 with value: 0.21052058041095734.\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.7177 - accuracy: 0.7846 - sparse_levenshtein_v1: 0.2138\n",
      "[I 2023-08-24 16:38:33,588] Trial 15 finished with value: 0.2138357013463974 and parameters: {'attention_heads': 6, 'learning_rate': 0.0004030865224447137, 'drop_out_out_decoder': 0.4, 'dense_layers': 4, 'drop_out': 0.0, 'encoder_kernel_size': 10}. Best is trial 1 with value: 0.21052058041095734.\n",
      "77/77 [==============================] - 1s 19ms/step - loss: 0.8914 - accuracy: 0.7397 - sparse_levenshtein_v1: 0.2547\n",
      "[I 2023-08-24 16:41:04,892] Trial 16 finished with value: 0.2547399401664734 and parameters: {'attention_heads': 7, 'learning_rate': 0.06547332284167556, 'drop_out_out_decoder': 0.4, 'dense_layers': 5, 'drop_out': 0.1, 'encoder_kernel_size': 12}. Best is trial 1 with value: 0.21052058041095734.\n",
      "77/77 [==============================] - 1s 13ms/step - loss: 0.7460 - accuracy: 0.7788 - sparse_levenshtein_v1: 0.2197\n",
      "[I 2023-08-24 16:42:41,034] Trial 17 finished with value: 0.21970239281654358 and parameters: {'attention_heads': 3, 'learning_rate': 0.002068575188140045, 'drop_out_out_decoder': 0.25, 'dense_layers': 3, 'drop_out': 0.25, 'encoder_kernel_size': 9}. Best is trial 1 with value: 0.21052058041095734.\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.7134 - accuracy: 0.7859 - sparse_levenshtein_v1: 0.2133\n",
      "[I 2023-08-24 16:44:46,348] Trial 18 finished with value: 0.21331267058849335 and parameters: {'attention_heads': 5, 'learning_rate': 0.0005927063564847215, 'drop_out_out_decoder': 0.0, 'dense_layers': 4, 'drop_out': 0.1, 'encoder_kernel_size': 11}. Best is trial 1 with value: 0.21052058041095734.\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.7284 - accuracy: 0.7812 - sparse_levenshtein_v1: 0.2173\n",
      "[I 2023-08-24 16:47:08,679] Trial 19 finished with value: 0.21727125346660614 and parameters: {'attention_heads': 7, 'learning_rate': 0.00583382038496176, 'drop_out_out_decoder': 0.35000000000000003, 'dense_layers': 2, 'drop_out': 0.35000000000000003, 'encoder_kernel_size': 3}. Best is trial 1 with value: 0.21052058041095734.\n",
      "77/77 [==============================] - 1s 14ms/step - loss: 0.7408 - accuracy: 0.7802 - sparse_levenshtein_v1: 0.2183\n",
      "[I 2023-08-24 16:48:58,334] Trial 20 finished with value: 0.21826346218585968 and parameters: {'attention_heads': 4, 'learning_rate': 0.0010377512017733747, 'drop_out_out_decoder': 0.2, 'dense_layers': 3, 'drop_out': 0.2, 'encoder_kernel_size': 9}. Best is trial 1 with value: 0.21052058041095734.\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.7074 - accuracy: 0.7877 - sparse_levenshtein_v1: 0.2107\n",
      "[I 2023-08-24 16:51:04,383] Trial 21 finished with value: 0.21068724989891052 and parameters: {'attention_heads': 5, 'learning_rate': 0.0012609313933807473, 'drop_out_out_decoder': 0.5, 'dense_layers': 5, 'drop_out': 0.05, 'encoder_kernel_size': 11}. Best is trial 1 with value: 0.21052058041095734.\n",
      "77/77 [==============================] - 1s 14ms/step - loss: 0.7405 - accuracy: 0.7788 - sparse_levenshtein_v1: 0.2192\n",
      "[I 2023-08-24 16:52:59,909] Trial 22 finished with value: 0.21924598515033722 and parameters: {'attention_heads': 5, 'learning_rate': 0.0002306042008538333, 'drop_out_out_decoder': 0.45, 'dense_layers': 5, 'drop_out': 0.05, 'encoder_kernel_size': 11}. Best is trial 1 with value: 0.21052058041095734.\n",
      "77/77 [==============================] - 1s 12ms/step - loss: 0.7438 - accuracy: 0.7788 - sparse_levenshtein_v1: 0.2211\n",
      "[I 2023-08-24 16:54:31,287] Trial 23 finished with value: 0.2211243212223053 and parameters: {'attention_heads': 3, 'learning_rate': 0.0009522061673714165, 'drop_out_out_decoder': 0.45, 'dense_layers': 5, 'drop_out': 0.0, 'encoder_kernel_size': 12}. Best is trial 1 with value: 0.21052058041095734.\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.6967 - accuracy: 0.7900 - sparse_levenshtein_v1: 0.2086\n",
      "[I 2023-08-24 16:56:45,244] Trial 24 finished with value: 0.2086203396320343 and parameters: {'attention_heads': 7, 'learning_rate': 0.002214948656389387, 'drop_out_out_decoder': 0.35000000000000003, 'dense_layers': 4, 'drop_out': 0.05, 'encoder_kernel_size': 11}. Best is trial 24 with value: 0.2086203396320343.\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.7907 - accuracy: 0.7677 - sparse_levenshtein_v1: 0.2284\n",
      "[I 2023-08-24 16:58:58,977] Trial 25 finished with value: 0.22836491465568542 and parameters: {'attention_heads': 7, 'learning_rate': 0.00957485018233192, 'drop_out_out_decoder': 0.30000000000000004, 'dense_layers': 4, 'drop_out': 0.15000000000000002, 'encoder_kernel_size': 11}. Best is trial 24 with value: 0.2086203396320343.\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.6755 - accuracy: 0.7959 - sparse_levenshtein_v1: 0.2036\n",
      "[I 2023-08-24 17:01:13,420] Trial 26 finished with value: 0.2035754919052124 and parameters: {'attention_heads': 7, 'learning_rate': 0.002169271146757282, 'drop_out_out_decoder': 0.25, 'dense_layers': 4, 'drop_out': 0.05, 'encoder_kernel_size': 12}. Best is trial 26 with value: 0.2035754919052124.\n",
      "77/77 [==============================] - 2s 21ms/step - loss: 0.6625 - accuracy: 0.7995 - sparse_levenshtein_v1: 0.1989\n",
      "[I 2023-08-24 17:03:43,533] Trial 27 finished with value: 0.198923721909523 and parameters: {'attention_heads': 8, 'learning_rate': 0.0023787403204474544, 'drop_out_out_decoder': 0.25, 'dense_layers': 3, 'drop_out': 0.1, 'encoder_kernel_size': 12}. Best is trial 27 with value: 0.198923721909523.\n",
      "77/77 [==============================] - 2s 21ms/step - loss: 0.6992 - accuracy: 0.7899 - sparse_levenshtein_v1: 0.2092\n",
      "[I 2023-08-24 17:06:24,229] Trial 28 finished with value: 0.20917685329914093 and parameters: {'attention_heads': 8, 'learning_rate': 0.0031792544641010855, 'drop_out_out_decoder': 0.25, 'dense_layers': 4, 'drop_out': 0.1, 'encoder_kernel_size': 12}. Best is trial 27 with value: 0.198923721909523.\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 1.0490 - accuracy: 0.7218 - sparse_levenshtein_v1: 0.2782\n",
      "[I 2023-08-24 17:08:48,583] Trial 29 finished with value: 0.27822574973106384 and parameters: {'attention_heads': 7, 'learning_rate': 0.016053124414368715, 'drop_out_out_decoder': 0.2, 'dense_layers': 3, 'drop_out': 0.2, 'encoder_kernel_size': 9}. Best is trial 27 with value: 0.198923721909523.\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.6532 - accuracy: 0.8038 - sparse_levenshtein_v1: 0.1947\n",
      "[I 2023-08-24 17:11:09,613] Trial 30 finished with value: 0.19471584260463715 and parameters: {'attention_heads': 8, 'learning_rate': 0.0020961301897376326, 'drop_out_out_decoder': 0.35000000000000003, 'dense_layers': 4, 'drop_out': 0.05, 'encoder_kernel_size': 7}. Best is trial 30 with value: 0.19471584260463715.\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.6778 - accuracy: 0.7949 - sparse_levenshtein_v1: 0.2042\n",
      "[I 2023-08-24 17:13:29,331] Trial 31 finished with value: 0.20424507558345795 and parameters: {'attention_heads': 8, 'learning_rate': 0.0021446374141029377, 'drop_out_out_decoder': 0.35000000000000003, 'dense_layers': 4, 'drop_out': 0.05, 'encoder_kernel_size': 6}. Best is trial 30 with value: 0.19471584260463715.\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.7213 - accuracy: 0.7846 - sparse_levenshtein_v1: 0.2142\n",
      "[I 2023-08-24 17:15:49,824] Trial 32 finished with value: 0.2142220288515091 and parameters: {'attention_heads': 8, 'learning_rate': 0.005313844178667594, 'drop_out_out_decoder': 0.35000000000000003, 'dense_layers': 4, 'drop_out': 0.1, 'encoder_kernel_size': 6}. Best is trial 30 with value: 0.19471584260463715.\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.6488 - accuracy: 0.8032 - sparse_levenshtein_v1: 0.1955\n",
      "[I 2023-08-24 17:18:09,933] Trial 33 finished with value: 0.19546842575073242 and parameters: {'attention_heads': 8, 'learning_rate': 0.0021728610573955166, 'drop_out_out_decoder': 0.30000000000000004, 'dense_layers': 3, 'drop_out': 0.05, 'encoder_kernel_size': 7}. Best is trial 30 with value: 0.19471584260463715.\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.7085 - accuracy: 0.7886 - sparse_levenshtein_v1: 0.2097\n",
      "[I 2023-08-24 17:20:29,605] Trial 34 finished with value: 0.20973336696624756 and parameters: {'attention_heads': 8, 'learning_rate': 0.005005468394031055, 'drop_out_out_decoder': 0.25, 'dense_layers': 3, 'drop_out': 0.15000000000000002, 'encoder_kernel_size': 7}. Best is trial 30 with value: 0.19471584260463715.\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.6449 - accuracy: 0.8059 - sparse_levenshtein_v1: 0.1933\n",
      "[I 2023-08-24 17:22:49,580] Trial 35 finished with value: 0.19326549768447876 and parameters: {'attention_heads': 8, 'learning_rate': 0.0018992122460740161, 'drop_out_out_decoder': 0.15000000000000002, 'dense_layers': 3, 'drop_out': 0.05, 'encoder_kernel_size': 8}. Best is trial 35 with value: 0.19326549768447876.\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.6673 - accuracy: 0.7974 - sparse_levenshtein_v1: 0.2016\n",
      "[I 2023-08-24 17:29:31,623] Trial 36 finished with value: 0.20156873762607574 and parameters: {'attention_heads': 8, 'learning_rate': 0.0014349818864996364, 'drop_out_out_decoder': 0.15000000000000002, 'dense_layers': 2, 'drop_out': 0.1, 'encoder_kernel_size': 8}. Best is trial 35 with value: 0.19326549768447876.\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.7352 - accuracy: 0.7792 - sparse_levenshtein_v1: 0.2182\n",
      "[I 2023-08-24 17:31:24,294] Trial 37 finished with value: 0.21822047233581543 and parameters: {'attention_heads': 8, 'learning_rate': 0.008338202635505403, 'drop_out_out_decoder': 0.1, 'dense_layers': 3, 'drop_out': 0.0, 'encoder_kernel_size': 5}. Best is trial 35 with value: 0.19326549768447876.\n",
      "77/77 [==============================] - 7s 90ms/step - loss: 0.6818 - accuracy: 0.7939 - sparse_levenshtein_v1: 0.2050\n",
      "[I 2023-08-24 17:36:56,665] Trial 38 finished with value: 0.20504873991012573 and parameters: {'attention_heads': 7, 'learning_rate': 0.0036526521462905857, 'drop_out_out_decoder': 0.30000000000000004, 'dense_layers': 3, 'drop_out': 0.05, 'encoder_kernel_size': 7}. Best is trial 35 with value: 0.19326549768447876.\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.8088 - accuracy: 0.7640 - sparse_levenshtein_v1: 0.2321\n",
      "[I 2023-08-24 17:39:25,154] Trial 39 finished with value: 0.23208384215831757 and parameters: {'attention_heads': 8, 'learning_rate': 0.011065777304682653, 'drop_out_out_decoder': 0.15000000000000002, 'dense_layers': 2, 'drop_out': 0.25, 'encoder_kernel_size': 5}. Best is trial 35 with value: 0.19326549768447876.\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.8886 - accuracy: 0.7413 - sparse_levenshtein_v1: 0.2477\n",
      "[I 2023-08-24 17:40:51,188] Trial 40 finished with value: 0.24774758517742157 and parameters: {'attention_heads': 8, 'learning_rate': 0.01764888508610782, 'drop_out_out_decoder': 0.2, 'dense_layers': 2, 'drop_out': 0.15000000000000002, 'encoder_kernel_size': 8}. Best is trial 35 with value: 0.19326549768447876.\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.6527 - accuracy: 0.8027 - sparse_levenshtein_v1: 0.1959\n",
      "[I 2023-08-24 17:43:09,517] Trial 41 finished with value: 0.19590257108211517 and parameters: {'attention_heads': 8, 'learning_rate': 0.001558654629400422, 'drop_out_out_decoder': 0.15000000000000002, 'dense_layers': 2, 'drop_out': 0.1, 'encoder_kernel_size': 8}. Best is trial 35 with value: 0.19326549768447876.\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6662 - accuracy: 0.7988 - sparse_levenshtein_v1: 0.2001\n",
      "[I 2023-08-24 17:45:17,586] Trial 42 finished with value: 0.20007441937923431 and parameters: {'attention_heads': 7, 'learning_rate': 0.001677249272424316, 'drop_out_out_decoder': 0.1, 'dense_layers': 1, 'drop_out': 0.1, 'encoder_kernel_size': 7}. Best is trial 35 with value: 0.19326549768447876.\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.6849 - accuracy: 0.7928 - sparse_levenshtein_v1: 0.2063\n",
      "[I 2023-08-24 17:47:36,934] Trial 43 finished with value: 0.20625092089176178 and parameters: {'attention_heads': 8, 'learning_rate': 0.000707180815075947, 'drop_out_out_decoder': 0.15000000000000002, 'dense_layers': 3, 'drop_out': 0.05, 'encoder_kernel_size': 8}. Best is trial 35 with value: 0.19326549768447876.\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6277 - accuracy: 0.8125 - sparse_levenshtein_v1: 0.1862\n",
      "[I 2023-08-24 17:49:45,893] Trial 44 finished with value: 0.18621140718460083 and parameters: {'attention_heads': 7, 'learning_rate': 0.0027196542256953437, 'drop_out_out_decoder': 0.05, 'dense_layers': 3, 'drop_out': 0.0, 'encoder_kernel_size': 6}. Best is trial 44 with value: 0.18621140718460083.\n",
      "77/77 [==============================] - 1s 15ms/step - loss: 0.6767 - accuracy: 0.7945 - sparse_levenshtein_v1: 0.2039\n",
      "[I 2023-08-24 17:51:42,207] Trial 45 finished with value: 0.20392008125782013 and parameters: {'attention_heads': 6, 'learning_rate': 0.00136850704132045, 'drop_out_out_decoder': 0.0, 'dense_layers': 2, 'drop_out': 0.0, 'encoder_kernel_size': 6}. Best is trial 44 with value: 0.18621140718460083.\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.7027 - accuracy: 0.7893 - sparse_levenshtein_v1: 0.2088\n",
      "[I 2023-08-24 17:53:52,423] Trial 46 finished with value: 0.20879904925823212 and parameters: {'attention_heads': 7, 'learning_rate': 0.003891226546758803, 'drop_out_out_decoder': 0.05, 'dense_layers': 3, 'drop_out': 0.0, 'encoder_kernel_size': 7}. Best is trial 44 with value: 0.18621140718460083.\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.6493 - accuracy: 0.8070 - sparse_levenshtein_v1: 0.1910\n",
      "[I 2023-08-24 17:56:08,789] Trial 47 finished with value: 0.19104282557964325 and parameters: {'attention_heads': 8, 'learning_rate': 0.0007642632989895296, 'drop_out_out_decoder': 0.05, 'dense_layers': 1, 'drop_out': 0.0, 'encoder_kernel_size': 5}. Best is trial 44 with value: 0.18621140718460083.\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6988 - accuracy: 0.7905 - sparse_levenshtein_v1: 0.2083\n",
      "[I 2023-08-24 17:58:14,785] Trial 48 finished with value: 0.20828066766262054 and parameters: {'attention_heads': 7, 'learning_rate': 0.0007555887052956466, 'drop_out_out_decoder': 0.05, 'dense_layers': 1, 'drop_out': 0.5, 'encoder_kernel_size': 4}. Best is trial 44 with value: 0.18621140718460083.\n",
      "77/77 [==============================] - 1s 15ms/step - loss: 0.6952 - accuracy: 0.7922 - sparse_levenshtein_v1: 0.2060\n",
      "[I 2023-08-24 18:00:09,684] Trial 49 finished with value: 0.20600353181362152 and parameters: {'attention_heads': 6, 'learning_rate': 0.0004936079480731381, 'drop_out_out_decoder': 0.0, 'dense_layers': 1, 'drop_out': 0.0, 'encoder_kernel_size': 5}. Best is trial 44 with value: 0.18621140718460083.\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.6638 - accuracy: 0.7985 - sparse_levenshtein_v1: 0.2004\n",
      "[I 2023-08-24 18:02:28,037] Trial 50 finished with value: 0.2004062831401825 and parameters: {'attention_heads': 8, 'learning_rate': 0.0028515151428348214, 'drop_out_out_decoder': 0.1, 'dense_layers': 3, 'drop_out': 0.0, 'encoder_kernel_size': 6}. Best is trial 44 with value: 0.18621140718460083.\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.6423 - accuracy: 0.8062 - sparse_levenshtein_v1: 0.1923\n",
      "[I 2023-08-24 18:04:45,201] Trial 51 finished with value: 0.1922668069601059 and parameters: {'attention_heads': 8, 'learning_rate': 0.0015620859544219312, 'drop_out_out_decoder': 0.1, 'dense_layers': 1, 'drop_out': 0.05, 'encoder_kernel_size': 8}. Best is trial 44 with value: 0.18621140718460083.\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.6592 - accuracy: 0.8019 - sparse_levenshtein_v1: 0.1960\n",
      "[I 2023-08-24 18:07:00,539] Trial 52 finished with value: 0.19604671001434326 and parameters: {'attention_heads': 8, 'learning_rate': 0.0009557159717601105, 'drop_out_out_decoder': 0.1, 'dense_layers': 1, 'drop_out': 0.05, 'encoder_kernel_size': 4}. Best is trial 44 with value: 0.18621140718460083.\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.6698 - accuracy: 0.7962 - sparse_levenshtein_v1: 0.2024\n",
      "[I 2023-08-24 18:09:18,003] Trial 53 finished with value: 0.2023688107728958 and parameters: {'attention_heads': 8, 'learning_rate': 0.0017402632152629953, 'drop_out_out_decoder': 0.05, 'dense_layers': 1, 'drop_out': 0.0, 'encoder_kernel_size': 7}. Best is trial 44 with value: 0.18621140718460083.\n",
      "77/77 [==============================] - 1s 9ms/step - loss: 0.8440 - accuracy: 0.7513 - sparse_levenshtein_v1: 0.2455\n",
      "[I 2023-08-24 18:10:06,301] Trial 54 finished with value: 0.24551938474178314 and parameters: {'attention_heads': 1, 'learning_rate': 0.002817141428372487, 'drop_out_out_decoder': 0.05, 'dense_layers': 1, 'drop_out': 0.05, 'encoder_kernel_size': 9}. Best is trial 44 with value: 0.18621140718460083.\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6873 - accuracy: 0.7932 - sparse_levenshtein_v1: 0.2056\n",
      "[I 2023-08-24 18:12:14,404] Trial 55 finished with value: 0.20563705265522003 and parameters: {'attention_heads': 7, 'learning_rate': 0.0009443933512458861, 'drop_out_out_decoder': 0.1, 'dense_layers': 2, 'drop_out': 0.30000000000000004, 'encoder_kernel_size': 6}. Best is trial 44 with value: 0.18621140718460083.\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.6880 - accuracy: 0.7930 - sparse_levenshtein_v1: 0.2050\n",
      "[I 2023-08-24 18:14:30,671] Trial 56 finished with value: 0.20503981411457062 and parameters: {'attention_heads': 8, 'learning_rate': 0.0004187755274309928, 'drop_out_out_decoder': 0.0, 'dense_layers': 1, 'drop_out': 0.05, 'encoder_kernel_size': 5}. Best is trial 44 with value: 0.18621140718460083.\n",
      "77/77 [==============================] - 1s 10ms/step - loss: 0.7645 - accuracy: 0.7754 - sparse_levenshtein_v1: 0.2232\n",
      "[I 2023-08-24 18:15:47,986] Trial 57 finished with value: 0.2232068032026291 and parameters: {'attention_heads': 2, 'learning_rate': 0.00437985247304182, 'drop_out_out_decoder': 0.05, 'dense_layers': 3, 'drop_out': 0.0, 'encoder_kernel_size': 7}. Best is trial 44 with value: 0.18621140718460083.\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.7013 - accuracy: 0.7895 - sparse_levenshtein_v1: 0.2094\n",
      "[I 2023-08-24 18:17:58,447] Trial 58 finished with value: 0.20940467715263367 and parameters: {'attention_heads': 7, 'learning_rate': 0.0011463153567356783, 'drop_out_out_decoder': 0.30000000000000004, 'dense_layers': 3, 'drop_out': 0.35000000000000003, 'encoder_kernel_size': 8}. Best is trial 44 with value: 0.18621140718460083.\n",
      "77/77 [==============================] - 1s 15ms/step - loss: 0.6954 - accuracy: 0.7907 - sparse_levenshtein_v1: 0.2076\n",
      "[I 2023-08-24 18:19:57,551] Trial 59 finished with value: 0.20758727192878723 and parameters: {'attention_heads': 6, 'learning_rate': 0.0006116723103748294, 'drop_out_out_decoder': 0.1, 'dense_layers': 2, 'drop_out': 0.05, 'encoder_kernel_size': 9}. Best is trial 44 with value: 0.18621140718460083.\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.7141 - accuracy: 0.7854 - sparse_levenshtein_v1: 0.2131\n",
      "[I 2023-08-24 18:22:04,262] Trial 60 finished with value: 0.21311113238334656 and parameters: {'attention_heads': 7, 'learning_rate': 0.0065412457072462436, 'drop_out_out_decoder': 0.05, 'dense_layers': 1, 'drop_out': 0.05, 'encoder_kernel_size': 6}. Best is trial 44 with value: 0.18621140718460083.\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.6653 - accuracy: 0.7982 - sparse_levenshtein_v1: 0.2006\n",
      "[I 2023-08-24 18:24:23,289] Trial 61 finished with value: 0.20059247314929962 and parameters: {'attention_heads': 8, 'learning_rate': 0.0014907828348909337, 'drop_out_out_decoder': 0.15000000000000002, 'dense_layers': 2, 'drop_out': 0.1, 'encoder_kernel_size': 8}. Best is trial 44 with value: 0.18621140718460083.\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.6409 - accuracy: 0.8087 - sparse_levenshtein_v1: 0.1901\n",
      "[I 2023-08-24 18:26:41,382] Trial 62 finished with value: 0.19005092978477478 and parameters: {'attention_heads': 8, 'learning_rate': 0.0017241755388332245, 'drop_out_out_decoder': 0.15000000000000002, 'dense_layers': 2, 'drop_out': 0.0, 'encoder_kernel_size': 7}. Best is trial 44 with value: 0.18621140718460083.\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.6626 - accuracy: 0.8013 - sparse_levenshtein_v1: 0.1974\n",
      "[I 2023-08-24 18:28:58,829] Trial 63 finished with value: 0.19744661450386047 and parameters: {'attention_heads': 8, 'learning_rate': 0.003230725531468061, 'drop_out_out_decoder': 0.2, 'dense_layers': 1, 'drop_out': 0.0, 'encoder_kernel_size': 7}. Best is trial 44 with value: 0.18621140718460083.\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.6668 - accuracy: 0.7965 - sparse_levenshtein_v1: 0.2026\n",
      "[I 2023-08-24 18:31:17,356] Trial 64 finished with value: 0.20257975161075592 and parameters: {'attention_heads': 8, 'learning_rate': 0.00228129678920664, 'drop_out_out_decoder': 0.1, 'dense_layers': 2, 'drop_out': 0.0, 'encoder_kernel_size': 7}. Best is trial 44 with value: 0.18621140718460083.\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6696 - accuracy: 0.7990 - sparse_levenshtein_v1: 0.1995\n",
      "[I 2023-08-24 18:33:25,271] Trial 65 finished with value: 0.19948209822177887 and parameters: {'attention_heads': 7, 'learning_rate': 0.0010495232184416041, 'drop_out_out_decoder': 0.0, 'dense_layers': 3, 'drop_out': 0.05, 'encoder_kernel_size': 4}. Best is trial 44 with value: 0.18621140718460083.\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.6597 - accuracy: 0.8013 - sparse_levenshtein_v1: 0.1969\n",
      "[I 2023-08-24 18:35:44,347] Trial 66 finished with value: 0.19689004123210907 and parameters: {'attention_heads': 8, 'learning_rate': 0.0017596850375875257, 'drop_out_out_decoder': 0.35000000000000003, 'dense_layers': 3, 'drop_out': 0.0, 'encoder_kernel_size': 6}. Best is trial 44 with value: 0.18621140718460083.\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.6771 - accuracy: 0.7949 - sparse_levenshtein_v1: 0.2044\n",
      "[I 2023-08-24 18:38:05,231] Trial 67 finished with value: 0.20443905889987946 and parameters: {'attention_heads': 8, 'learning_rate': 0.002731799455231807, 'drop_out_out_decoder': 0.2, 'dense_layers': 4, 'drop_out': 0.0, 'encoder_kernel_size': 8}. Best is trial 44 with value: 0.18621140718460083.\n",
      "77/77 [==============================] - 1s 16ms/step - loss: 0.6837 - accuracy: 0.7932 - sparse_levenshtein_v1: 0.2054\n",
      "[I 2023-08-24 18:40:12,326] Trial 68 finished with value: 0.20543695986270905 and parameters: {'attention_heads': 7, 'learning_rate': 0.004106026439342762, 'drop_out_out_decoder': 0.30000000000000004, 'dense_layers': 1, 'drop_out': 0.05, 'encoder_kernel_size': 5}. Best is trial 44 with value: 0.18621140718460083.\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.6576 - accuracy: 0.8030 - sparse_levenshtein_v1: 0.1955\n",
      "[I 2023-08-24 18:42:33,527] Trial 69 finished with value: 0.19553251564502716 and parameters: {'attention_heads': 8, 'learning_rate': 0.0012315414589724495, 'drop_out_out_decoder': 0.15000000000000002, 'dense_layers': 4, 'drop_out': 0.05, 'encoder_kernel_size': 8}. Best is trial 44 with value: 0.18621140718460083.\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.6900 - accuracy: 0.7920 - sparse_levenshtein_v1: 0.2064\n",
      "[I 2023-08-24 18:44:53,565] Trial 70 finished with value: 0.20640525221824646 and parameters: {'attention_heads': 8, 'learning_rate': 0.0007329478216928115, 'drop_out_out_decoder': 0.4, 'dense_layers': 3, 'drop_out': 0.1, 'encoder_kernel_size': 7}. Best is trial 44 with value: 0.18621140718460083.\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.6653 - accuracy: 0.7982 - sparse_levenshtein_v1: 0.2002\n",
      "[I 2023-08-24 18:47:15,306] Trial 71 finished with value: 0.20023195445537567 and parameters: {'attention_heads': 8, 'learning_rate': 0.0012182797903032127, 'drop_out_out_decoder': 0.15000000000000002, 'dense_layers': 4, 'drop_out': 0.05, 'encoder_kernel_size': 8}. Best is trial 44 with value: 0.18621140718460083.\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.6537 - accuracy: 0.8048 - sparse_levenshtein_v1: 0.1942\n",
      "[I 2023-08-24 18:49:37,524] Trial 72 finished with value: 0.1942177563905716 and parameters: {'attention_heads': 8, 'learning_rate': 0.0019681639578021624, 'drop_out_out_decoder': 0.15000000000000002, 'dense_layers': 4, 'drop_out': 0.0, 'encoder_kernel_size': 9}. Best is trial 44 with value: 0.18621140718460083.\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.6465 - accuracy: 0.8078 - sparse_levenshtein_v1: 0.1910\n",
      "[I 2023-08-24 18:51:50,319] Trial 73 finished with value: 0.19104500114917755 and parameters: {'attention_heads': 7, 'learning_rate': 0.001907470176570914, 'drop_out_out_decoder': 0.1, 'dense_layers': 4, 'drop_out': 0.0, 'encoder_kernel_size': 9}. Best is trial 44 with value: 0.18621140718460083.\n",
      "77/77 [==============================] - 5s 66ms/step - loss: 0.6850 - accuracy: 0.7933 - sparse_levenshtein_v1: 0.2050\n",
      "[I 2023-08-24 18:57:00,609] Trial 74 finished with value: 0.2050178349018097 and parameters: {'attention_heads': 7, 'learning_rate': 0.0018471957940136851, 'drop_out_out_decoder': 0.1, 'dense_layers': 4, 'drop_out': 0.0, 'encoder_kernel_size': 9}. Best is trial 44 with value: 0.18621140718460083.\n",
      "77/77 [==============================] - 1s 17ms/step - loss: 0.6920 - accuracy: 0.7917 - sparse_levenshtein_v1: 0.2070\n",
      "[I 2023-08-24 19:02:04,574] Trial 75 finished with value: 0.20695647597312927 and parameters: {'attention_heads': 7, 'learning_rate': 0.0023559352667728785, 'drop_out_out_decoder': 0.05, 'dense_layers': 5, 'drop_out': 0.0, 'encoder_kernel_size': 10}. Best is trial 44 with value: 0.18621140718460083.\n",
      "77/77 [==============================] - 1s 15ms/step - loss: 0.7143 - accuracy: 0.7861 - sparse_levenshtein_v1: 0.2123\n",
      "[I 2023-08-24 19:04:05,710] Trial 76 finished with value: 0.21229560673236847 and parameters: {'attention_heads': 6, 'learning_rate': 0.003330138173796813, 'drop_out_out_decoder': 0.1, 'dense_layers': 4, 'drop_out': 0.0, 'encoder_kernel_size': 10}. Best is trial 44 with value: 0.18621140718460083.\n",
      "77/77 [==============================] - 1s 18ms/step - loss: 0.7088 - accuracy: 0.7878 - sparse_levenshtein_v1: 0.2115\n",
      "[I 2023-08-24 19:06:27,833] Trial 77 finished with value: 0.21150128543376923 and parameters: {'attention_heads': 8, 'learning_rate': 0.004729998114665757, 'drop_out_out_decoder': 0.2, 'dense_layers': 4, 'drop_out': 0.0, 'encoder_kernel_size': 9}. Best is trial 44 with value: 0.18621140718460083.\n",
      "77/77 [==============================] - 1s 19ms/step - loss: 0.6757 - accuracy: 0.7958 - sparse_levenshtein_v1: 0.2023\n",
      "[I 2023-08-24 19:08:44,180] Trial 78 finished with value: 0.2022959440946579 and parameters: {'attention_heads': 7, 'learning_rate': 0.000888697552873505, 'drop_out_out_decoder': 0.15000000000000002, 'dense_layers': 4, 'drop_out': 0.0, 'encoder_kernel_size': 9}. Best is trial 44 with value: 0.18621140718460083.\n",
      "77/77 [==============================] - 2s 20ms/step - loss: 0.6523 - accuracy: 0.8055 - sparse_levenshtein_v1: 0.1929\n",
      "[I 2023-08-24 19:11:22,718] Trial 79 finished with value: 0.1929413229227066 and parameters: {'attention_heads': 8, 'learning_rate': 0.001517488964823808, 'drop_out_out_decoder': 0.1, 'dense_layers': 5, 'drop_out': 0.0, 'encoder_kernel_size': 8}. Best is trial 44 with value: 0.18621140718460083.\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=TRIALS, gc_after_trial=True, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = study.best_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenTrial(number=44, state=TrialState.COMPLETE, values=[0.18621140718460083], datetime_start=datetime.datetime(2023, 8, 24, 17, 47, 37, 114347), datetime_complete=datetime.datetime(2023, 8, 24, 17, 49, 45, 892933), params={'attention_heads': 7, 'learning_rate': 0.0027196542256953437, 'drop_out_out_decoder': 0.05, 'dense_layers': 3, 'drop_out': 0.0, 'encoder_kernel_size': 6}, user_attrs={}, system_attrs={}, intermediate_values={}, distributions={'attention_heads': IntDistribution(high=8, log=False, low=1, step=1), 'learning_rate': FloatDistribution(high=0.1, log=True, low=1e-05, step=None), 'drop_out_out_decoder': FloatDistribution(high=0.5, log=False, low=0.0, step=0.05), 'dense_layers': IntDistribution(high=5, log=False, low=1, step=1), 'drop_out': FloatDistribution(high=0.5, log=False, low=0.0, step=0.05), 'encoder_kernel_size': IntDistribution(high=12, log=False, low=3, step=1)}, trial_id=44, value=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trials[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: 1\n",
      "Model: \"finger_spelling_v2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " landmark_embedding_v2 (Lan  multiple                  147200    \n",
      " dmarkEmbeddingV2)                                               \n",
      "                                                                 \n",
      " basic_positional_embedding  multiple                  16128     \n",
      " s (BasicPositionalEmbeddin                                      \n",
      " gs)                                                             \n",
      "                                                                 \n",
      " transformer_encoder (Trans  multiple                  233992    \n",
      " formerEncoder)                                                  \n",
      "                                                                 \n",
      " transformer_decoder (Trans  multiple                  467343    \n",
      " formerDecoder)                                                  \n",
      "                                                                 \n",
      " dropout (Dropout)           multiple                  0         \n",
      "                                                                 \n",
      " sequential_2 (Sequential)   (None, 64, 42)            10836     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         multiple                  0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             multiple                  2666      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 878165 (3.35 MB)\n",
      "Trainable params: 878165 (3.35 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5000\n",
      "321/321 [==============================] - 21s 47ms/step - loss: 1.5653 - accuracy: 0.6668 - sparse_levenshtein_v1: 0.3262 - val_loss: 1.0115 - val_accuracy: 0.7410 - val_sparse_levenshtein_v1: 0.2531\n",
      "Epoch 2/5000\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 1.0430 - accuracy: 0.7351 - sparse_levenshtein_v1: 0.2520 - val_loss: 0.8934 - val_accuracy: 0.7571 - val_sparse_levenshtein_v1: 0.2386\n",
      "Epoch 3/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.9400 - accuracy: 0.7456 - sparse_levenshtein_v1: 0.2425 - val_loss: 0.8514 - val_accuracy: 0.7613 - val_sparse_levenshtein_v1: 0.2347\n",
      "Epoch 4/5000\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.8956 - accuracy: 0.7520 - sparse_levenshtein_v1: 0.2381 - val_loss: 0.8284 - val_accuracy: 0.7633 - val_sparse_levenshtein_v1: 0.2334\n",
      "Epoch 5/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.8690 - accuracy: 0.7561 - sparse_levenshtein_v1: 0.2355 - val_loss: 0.8123 - val_accuracy: 0.7656 - val_sparse_levenshtein_v1: 0.2323\n",
      "Epoch 6/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.8483 - accuracy: 0.7592 - sparse_levenshtein_v1: 0.2334 - val_loss: 0.7991 - val_accuracy: 0.7688 - val_sparse_levenshtein_v1: 0.2291\n",
      "Epoch 7/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.8319 - accuracy: 0.7619 - sparse_levenshtein_v1: 0.2315 - val_loss: 0.7877 - val_accuracy: 0.7698 - val_sparse_levenshtein_v1: 0.2275\n",
      "Epoch 8/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.8184 - accuracy: 0.7646 - sparse_levenshtein_v1: 0.2293 - val_loss: 0.7793 - val_accuracy: 0.7724 - val_sparse_levenshtein_v1: 0.2252\n",
      "Epoch 9/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.8064 - accuracy: 0.7671 - sparse_levenshtein_v1: 0.2272 - val_loss: 0.7703 - val_accuracy: 0.7737 - val_sparse_levenshtein_v1: 0.2238\n",
      "Epoch 10/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.7963 - accuracy: 0.7689 - sparse_levenshtein_v1: 0.2257 - val_loss: 0.7639 - val_accuracy: 0.7751 - val_sparse_levenshtein_v1: 0.2229\n",
      "Epoch 11/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.7872 - accuracy: 0.7708 - sparse_levenshtein_v1: 0.2243 - val_loss: 0.7577 - val_accuracy: 0.7759 - val_sparse_levenshtein_v1: 0.2218\n",
      "Epoch 12/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.7804 - accuracy: 0.7719 - sparse_levenshtein_v1: 0.2232 - val_loss: 0.7540 - val_accuracy: 0.7766 - val_sparse_levenshtein_v1: 0.2216\n",
      "Epoch 13/5000\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.7732 - accuracy: 0.7733 - sparse_levenshtein_v1: 0.2221 - val_loss: 0.7512 - val_accuracy: 0.7770 - val_sparse_levenshtein_v1: 0.2208\n",
      "Epoch 14/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.7673 - accuracy: 0.7743 - sparse_levenshtein_v1: 0.2212 - val_loss: 0.7463 - val_accuracy: 0.7778 - val_sparse_levenshtein_v1: 0.2202\n",
      "Epoch 15/5000\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.7620 - accuracy: 0.7755 - sparse_levenshtein_v1: 0.2202 - val_loss: 0.7413 - val_accuracy: 0.7789 - val_sparse_levenshtein_v1: 0.2193\n",
      "Epoch 16/5000\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.7577 - accuracy: 0.7764 - sparse_levenshtein_v1: 0.2194 - val_loss: 0.7387 - val_accuracy: 0.7793 - val_sparse_levenshtein_v1: 0.2189\n",
      "Epoch 17/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.7538 - accuracy: 0.7772 - sparse_levenshtein_v1: 0.2186 - val_loss: 0.7373 - val_accuracy: 0.7799 - val_sparse_levenshtein_v1: 0.2180\n",
      "Epoch 18/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.7497 - accuracy: 0.7781 - sparse_levenshtein_v1: 0.2179 - val_loss: 0.7334 - val_accuracy: 0.7804 - val_sparse_levenshtein_v1: 0.2176\n",
      "Epoch 19/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.7467 - accuracy: 0.7788 - sparse_levenshtein_v1: 0.2175 - val_loss: 0.7314 - val_accuracy: 0.7809 - val_sparse_levenshtein_v1: 0.2175\n",
      "Epoch 20/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.7433 - accuracy: 0.7794 - sparse_levenshtein_v1: 0.2168 - val_loss: 0.7300 - val_accuracy: 0.7815 - val_sparse_levenshtein_v1: 0.2171\n",
      "Epoch 21/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.7403 - accuracy: 0.7798 - sparse_levenshtein_v1: 0.2165 - val_loss: 0.7262 - val_accuracy: 0.7824 - val_sparse_levenshtein_v1: 0.2160\n",
      "Epoch 22/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.7376 - accuracy: 0.7807 - sparse_levenshtein_v1: 0.2157 - val_loss: 0.7244 - val_accuracy: 0.7828 - val_sparse_levenshtein_v1: 0.2153\n",
      "Epoch 23/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.7349 - accuracy: 0.7811 - sparse_levenshtein_v1: 0.2153 - val_loss: 0.7220 - val_accuracy: 0.7837 - val_sparse_levenshtein_v1: 0.2143\n",
      "Epoch 24/5000\n",
      "321/321 [==============================] - 13s 39ms/step - loss: 0.7320 - accuracy: 0.7820 - sparse_levenshtein_v1: 0.2145 - val_loss: 0.7203 - val_accuracy: 0.7841 - val_sparse_levenshtein_v1: 0.2146\n",
      "Epoch 25/5000\n",
      "321/321 [==============================] - 13s 39ms/step - loss: 0.7285 - accuracy: 0.7830 - sparse_levenshtein_v1: 0.2135 - val_loss: 0.7159 - val_accuracy: 0.7856 - val_sparse_levenshtein_v1: 0.2126\n",
      "Epoch 26/5000\n",
      "321/321 [==============================] - 13s 39ms/step - loss: 0.7257 - accuracy: 0.7838 - sparse_levenshtein_v1: 0.2126 - val_loss: 0.7121 - val_accuracy: 0.7868 - val_sparse_levenshtein_v1: 0.2112\n",
      "Epoch 27/5000\n",
      "321/321 [==============================] - 12s 39ms/step - loss: 0.7228 - accuracy: 0.7845 - sparse_levenshtein_v1: 0.2121 - val_loss: 0.7122 - val_accuracy: 0.7866 - val_sparse_levenshtein_v1: 0.2115\n",
      "Epoch 28/5000\n",
      "321/321 [==============================] - 13s 39ms/step - loss: 0.7195 - accuracy: 0.7857 - sparse_levenshtein_v1: 0.2110 - val_loss: 0.7079 - val_accuracy: 0.7885 - val_sparse_levenshtein_v1: 0.2094\n",
      "Epoch 29/5000\n",
      "321/321 [==============================] - 13s 39ms/step - loss: 0.7164 - accuracy: 0.7865 - sparse_levenshtein_v1: 0.2101 - val_loss: 0.7046 - val_accuracy: 0.7894 - val_sparse_levenshtein_v1: 0.2087\n",
      "Epoch 30/5000\n",
      "321/321 [==============================] - 13s 39ms/step - loss: 0.7138 - accuracy: 0.7876 - sparse_levenshtein_v1: 0.2091 - val_loss: 0.7013 - val_accuracy: 0.7905 - val_sparse_levenshtein_v1: 0.2074\n",
      "Epoch 31/5000\n",
      "321/321 [==============================] - 12s 39ms/step - loss: 0.7108 - accuracy: 0.7886 - sparse_levenshtein_v1: 0.2082 - val_loss: 0.7015 - val_accuracy: 0.7902 - val_sparse_levenshtein_v1: 0.2081\n",
      "Epoch 32/5000\n",
      "321/321 [==============================] - 13s 39ms/step - loss: 0.7075 - accuracy: 0.7897 - sparse_levenshtein_v1: 0.2070 - val_loss: 0.6971 - val_accuracy: 0.7920 - val_sparse_levenshtein_v1: 0.2062\n",
      "Epoch 33/5000\n",
      "321/321 [==============================] - 16s 50ms/step - loss: 0.7046 - accuracy: 0.7907 - sparse_levenshtein_v1: 0.2060 - val_loss: 0.6930 - val_accuracy: 0.7934 - val_sparse_levenshtein_v1: 0.2046\n",
      "Epoch 34/5000\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.7016 - accuracy: 0.7919 - sparse_levenshtein_v1: 0.2048 - val_loss: 0.6896 - val_accuracy: 0.7945 - val_sparse_levenshtein_v1: 0.2034\n",
      "Epoch 35/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.6985 - accuracy: 0.7928 - sparse_levenshtein_v1: 0.2038 - val_loss: 0.6880 - val_accuracy: 0.7950 - val_sparse_levenshtein_v1: 0.2030\n",
      "Epoch 36/5000\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6952 - accuracy: 0.7941 - sparse_levenshtein_v1: 0.2025 - val_loss: 0.6828 - val_accuracy: 0.7969 - val_sparse_levenshtein_v1: 0.2008\n",
      "Epoch 37/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.6925 - accuracy: 0.7951 - sparse_levenshtein_v1: 0.2016 - val_loss: 0.6817 - val_accuracy: 0.7977 - val_sparse_levenshtein_v1: 0.2002\n",
      "Epoch 38/5000\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6893 - accuracy: 0.7963 - sparse_levenshtein_v1: 0.2003 - val_loss: 0.6773 - val_accuracy: 0.7992 - val_sparse_levenshtein_v1: 0.1984\n",
      "Epoch 39/5000\n",
      "321/321 [==============================] - 13s 41ms/step - loss: 0.6865 - accuracy: 0.7972 - sparse_levenshtein_v1: 0.1994 - val_loss: 0.6746 - val_accuracy: 0.8004 - val_sparse_levenshtein_v1: 0.1971\n",
      "Epoch 40/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.6833 - accuracy: 0.7984 - sparse_levenshtein_v1: 0.1982 - val_loss: 0.6707 - val_accuracy: 0.8015 - val_sparse_levenshtein_v1: 0.1964\n",
      "Epoch 41/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.6803 - accuracy: 0.7996 - sparse_levenshtein_v1: 0.1969 - val_loss: 0.6681 - val_accuracy: 0.8026 - val_sparse_levenshtein_v1: 0.1951\n",
      "Epoch 42/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.6775 - accuracy: 0.8008 - sparse_levenshtein_v1: 0.1957 - val_loss: 0.6655 - val_accuracy: 0.8040 - val_sparse_levenshtein_v1: 0.1936\n",
      "Epoch 43/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.6747 - accuracy: 0.8019 - sparse_levenshtein_v1: 0.1946 - val_loss: 0.6624 - val_accuracy: 0.8054 - val_sparse_levenshtein_v1: 0.1919\n",
      "Epoch 44/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.6719 - accuracy: 0.8029 - sparse_levenshtein_v1: 0.1936 - val_loss: 0.6591 - val_accuracy: 0.8059 - val_sparse_levenshtein_v1: 0.1915\n",
      "Epoch 45/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.6690 - accuracy: 0.8041 - sparse_levenshtein_v1: 0.1923 - val_loss: 0.6569 - val_accuracy: 0.8071 - val_sparse_levenshtein_v1: 0.1899\n",
      "Epoch 46/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.6659 - accuracy: 0.8053 - sparse_levenshtein_v1: 0.1913 - val_loss: 0.6538 - val_accuracy: 0.8079 - val_sparse_levenshtein_v1: 0.1894\n",
      "Epoch 47/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.6635 - accuracy: 0.8062 - sparse_levenshtein_v1: 0.1903 - val_loss: 0.6521 - val_accuracy: 0.8085 - val_sparse_levenshtein_v1: 0.1890\n",
      "Epoch 48/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.6610 - accuracy: 0.8071 - sparse_levenshtein_v1: 0.1894 - val_loss: 0.6497 - val_accuracy: 0.8095 - val_sparse_levenshtein_v1: 0.1878\n",
      "Epoch 49/5000\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6586 - accuracy: 0.8079 - sparse_levenshtein_v1: 0.1886 - val_loss: 0.6465 - val_accuracy: 0.8112 - val_sparse_levenshtein_v1: 0.1859\n",
      "Epoch 50/5000\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6561 - accuracy: 0.8090 - sparse_levenshtein_v1: 0.1876 - val_loss: 0.6445 - val_accuracy: 0.8115 - val_sparse_levenshtein_v1: 0.1855\n",
      "Epoch 51/5000\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6537 - accuracy: 0.8097 - sparse_levenshtein_v1: 0.1867 - val_loss: 0.6414 - val_accuracy: 0.8128 - val_sparse_levenshtein_v1: 0.1843\n",
      "Epoch 52/5000\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6509 - accuracy: 0.8107 - sparse_levenshtein_v1: 0.1858 - val_loss: 0.6404 - val_accuracy: 0.8134 - val_sparse_levenshtein_v1: 0.1837\n",
      "Epoch 53/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.6485 - accuracy: 0.8117 - sparse_levenshtein_v1: 0.1849 - val_loss: 0.6375 - val_accuracy: 0.8148 - val_sparse_levenshtein_v1: 0.1824\n",
      "Epoch 54/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.6465 - accuracy: 0.8125 - sparse_levenshtein_v1: 0.1841 - val_loss: 0.6360 - val_accuracy: 0.8149 - val_sparse_levenshtein_v1: 0.1822\n",
      "Epoch 55/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.6442 - accuracy: 0.8133 - sparse_levenshtein_v1: 0.1832 - val_loss: 0.6333 - val_accuracy: 0.8158 - val_sparse_levenshtein_v1: 0.1813\n",
      "Epoch 56/5000\n",
      "321/321 [==============================] - 13s 42ms/step - loss: 0.6415 - accuracy: 0.8144 - sparse_levenshtein_v1: 0.1823 - val_loss: 0.6291 - val_accuracy: 0.8176 - val_sparse_levenshtein_v1: 0.1797\n",
      "Epoch 57/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.6394 - accuracy: 0.8150 - sparse_levenshtein_v1: 0.1816 - val_loss: 0.6279 - val_accuracy: 0.8182 - val_sparse_levenshtein_v1: 0.1790\n",
      "Epoch 58/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.6369 - accuracy: 0.8158 - sparse_levenshtein_v1: 0.1808 - val_loss: 0.6255 - val_accuracy: 0.8191 - val_sparse_levenshtein_v1: 0.1780\n",
      "Epoch 59/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.6342 - accuracy: 0.8170 - sparse_levenshtein_v1: 0.1797 - val_loss: 0.6234 - val_accuracy: 0.8197 - val_sparse_levenshtein_v1: 0.1777\n",
      "Epoch 60/5000\n",
      "321/321 [==============================] - 14s 45ms/step - loss: 0.6320 - accuracy: 0.8176 - sparse_levenshtein_v1: 0.1792 - val_loss: 0.6216 - val_accuracy: 0.8205 - val_sparse_levenshtein_v1: 0.1768\n",
      "Epoch 61/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.6293 - accuracy: 0.8187 - sparse_levenshtein_v1: 0.1781 - val_loss: 0.6185 - val_accuracy: 0.8213 - val_sparse_levenshtein_v1: 0.1759\n",
      "Epoch 62/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.6270 - accuracy: 0.8195 - sparse_levenshtein_v1: 0.1773 - val_loss: 0.6150 - val_accuracy: 0.8225 - val_sparse_levenshtein_v1: 0.1751\n",
      "Epoch 63/5000\n",
      "321/321 [==============================] - 14s 45ms/step - loss: 0.6247 - accuracy: 0.8205 - sparse_levenshtein_v1: 0.1764 - val_loss: 0.6124 - val_accuracy: 0.8235 - val_sparse_levenshtein_v1: 0.1740\n",
      "Epoch 64/5000\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6218 - accuracy: 0.8216 - sparse_levenshtein_v1: 0.1753 - val_loss: 0.6091 - val_accuracy: 0.8248 - val_sparse_levenshtein_v1: 0.1726\n",
      "Epoch 65/5000\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.6189 - accuracy: 0.8225 - sparse_levenshtein_v1: 0.1745 - val_loss: 0.6068 - val_accuracy: 0.8257 - val_sparse_levenshtein_v1: 0.1718\n",
      "Epoch 66/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.6160 - accuracy: 0.8237 - sparse_levenshtein_v1: 0.1735 - val_loss: 0.6057 - val_accuracy: 0.8264 - val_sparse_levenshtein_v1: 0.1711\n",
      "Epoch 67/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.6130 - accuracy: 0.8246 - sparse_levenshtein_v1: 0.1725 - val_loss: 0.6018 - val_accuracy: 0.8276 - val_sparse_levenshtein_v1: 0.1701\n",
      "Epoch 68/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.6096 - accuracy: 0.8259 - sparse_levenshtein_v1: 0.1713 - val_loss: 0.5990 - val_accuracy: 0.8287 - val_sparse_levenshtein_v1: 0.1691\n",
      "Epoch 69/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.6070 - accuracy: 0.8270 - sparse_levenshtein_v1: 0.1703 - val_loss: 0.5945 - val_accuracy: 0.8300 - val_sparse_levenshtein_v1: 0.1677\n",
      "Epoch 70/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.6032 - accuracy: 0.8282 - sparse_levenshtein_v1: 0.1691 - val_loss: 0.5908 - val_accuracy: 0.8315 - val_sparse_levenshtein_v1: 0.1662\n",
      "Epoch 71/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.6000 - accuracy: 0.8293 - sparse_levenshtein_v1: 0.1681 - val_loss: 0.5873 - val_accuracy: 0.8328 - val_sparse_levenshtein_v1: 0.1650\n",
      "Epoch 72/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.5964 - accuracy: 0.8307 - sparse_levenshtein_v1: 0.1669 - val_loss: 0.5839 - val_accuracy: 0.8335 - val_sparse_levenshtein_v1: 0.1643\n",
      "Epoch 73/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.5930 - accuracy: 0.8319 - sparse_levenshtein_v1: 0.1657 - val_loss: 0.5822 - val_accuracy: 0.8346 - val_sparse_levenshtein_v1: 0.1636\n",
      "Epoch 74/5000\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5889 - accuracy: 0.8334 - sparse_levenshtein_v1: 0.1644 - val_loss: 0.5771 - val_accuracy: 0.8364 - val_sparse_levenshtein_v1: 0.1617\n",
      "Epoch 75/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.5850 - accuracy: 0.8347 - sparse_levenshtein_v1: 0.1631 - val_loss: 0.5717 - val_accuracy: 0.8381 - val_sparse_levenshtein_v1: 0.1602\n",
      "Epoch 76/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.5811 - accuracy: 0.8360 - sparse_levenshtein_v1: 0.1618 - val_loss: 0.5683 - val_accuracy: 0.8393 - val_sparse_levenshtein_v1: 0.1590\n",
      "Epoch 77/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.5774 - accuracy: 0.8375 - sparse_levenshtein_v1: 0.1605 - val_loss: 0.5651 - val_accuracy: 0.8404 - val_sparse_levenshtein_v1: 0.1580\n",
      "Epoch 78/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.5732 - accuracy: 0.8388 - sparse_levenshtein_v1: 0.1591 - val_loss: 0.5601 - val_accuracy: 0.8420 - val_sparse_levenshtein_v1: 0.1563\n",
      "Epoch 79/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.5696 - accuracy: 0.8399 - sparse_levenshtein_v1: 0.1581 - val_loss: 0.5573 - val_accuracy: 0.8430 - val_sparse_levenshtein_v1: 0.1554\n",
      "Epoch 80/5000\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5656 - accuracy: 0.8415 - sparse_levenshtein_v1: 0.1567 - val_loss: 0.5522 - val_accuracy: 0.8449 - val_sparse_levenshtein_v1: 0.1536\n",
      "Epoch 81/5000\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5620 - accuracy: 0.8428 - sparse_levenshtein_v1: 0.1554 - val_loss: 0.5502 - val_accuracy: 0.8454 - val_sparse_levenshtein_v1: 0.1532\n",
      "Epoch 82/5000\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5581 - accuracy: 0.8440 - sparse_levenshtein_v1: 0.1543 - val_loss: 0.5450 - val_accuracy: 0.8472 - val_sparse_levenshtein_v1: 0.1515\n",
      "Epoch 83/5000\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5547 - accuracy: 0.8452 - sparse_levenshtein_v1: 0.1531 - val_loss: 0.5415 - val_accuracy: 0.8484 - val_sparse_levenshtein_v1: 0.1503\n",
      "Epoch 84/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.5509 - accuracy: 0.8466 - sparse_levenshtein_v1: 0.1518 - val_loss: 0.5400 - val_accuracy: 0.8488 - val_sparse_levenshtein_v1: 0.1499\n",
      "Epoch 85/5000\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5476 - accuracy: 0.8475 - sparse_levenshtein_v1: 0.1509 - val_loss: 0.5356 - val_accuracy: 0.8501 - val_sparse_levenshtein_v1: 0.1486\n",
      "Epoch 86/5000\n",
      "321/321 [==============================] - 13s 40ms/step - loss: 0.5438 - accuracy: 0.8488 - sparse_levenshtein_v1: 0.1497 - val_loss: 0.5327 - val_accuracy: 0.8512 - val_sparse_levenshtein_v1: 0.1478\n",
      "Epoch 87/5000\n",
      "321/321 [==============================] - 12s 39ms/step - loss: 0.5407 - accuracy: 0.8496 - sparse_levenshtein_v1: 0.1489 - val_loss: 0.5278 - val_accuracy: 0.8526 - val_sparse_levenshtein_v1: 0.1463\n",
      "Epoch 88/5000\n",
      "321/321 [==============================] - 12s 39ms/step - loss: 0.5377 - accuracy: 0.8507 - sparse_levenshtein_v1: 0.1478 - val_loss: 0.5260 - val_accuracy: 0.8533 - val_sparse_levenshtein_v1: 0.1455\n",
      "Epoch 89/5000\n",
      "321/321 [==============================] - 12s 39ms/step - loss: 0.5342 - accuracy: 0.8520 - sparse_levenshtein_v1: 0.1466 - val_loss: 0.5213 - val_accuracy: 0.8549 - val_sparse_levenshtein_v1: 0.1442\n",
      "Epoch 90/5000\n",
      "321/321 [==============================] - 12s 38ms/step - loss: 0.5307 - accuracy: 0.8532 - sparse_levenshtein_v1: 0.1454 - val_loss: 0.5177 - val_accuracy: 0.8559 - val_sparse_levenshtein_v1: 0.1430\n",
      "Epoch 91/5000\n",
      "321/321 [==============================] - 12s 39ms/step - loss: 0.5277 - accuracy: 0.8540 - sparse_levenshtein_v1: 0.1447 - val_loss: 0.5154 - val_accuracy: 0.8568 - val_sparse_levenshtein_v1: 0.1423\n",
      "Epoch 92/5000\n",
      "321/321 [==============================] - 12s 39ms/step - loss: 0.5250 - accuracy: 0.8550 - sparse_levenshtein_v1: 0.1436 - val_loss: 0.5135 - val_accuracy: 0.8575 - val_sparse_levenshtein_v1: 0.1416\n",
      "Epoch 93/5000\n",
      "321/321 [==============================] - 12s 39ms/step - loss: 0.5220 - accuracy: 0.8559 - sparse_levenshtein_v1: 0.1428 - val_loss: 0.5099 - val_accuracy: 0.8587 - val_sparse_levenshtein_v1: 0.1404\n",
      "Epoch 94/5000\n",
      "321/321 [==============================] - 12s 39ms/step - loss: 0.5194 - accuracy: 0.8567 - sparse_levenshtein_v1: 0.1420 - val_loss: 0.5078 - val_accuracy: 0.8593 - val_sparse_levenshtein_v1: 0.1397\n",
      "Epoch 95/5000\n",
      "321/321 [==============================] - 12s 39ms/step - loss: 0.5165 - accuracy: 0.8576 - sparse_levenshtein_v1: 0.1411 - val_loss: 0.5045 - val_accuracy: 0.8600 - val_sparse_levenshtein_v1: 0.1390\n",
      "Epoch 96/5000\n",
      "321/321 [==============================] - 12s 39ms/step - loss: 0.5135 - accuracy: 0.8586 - sparse_levenshtein_v1: 0.1402 - val_loss: 0.5025 - val_accuracy: 0.8610 - val_sparse_levenshtein_v1: 0.1381\n",
      "Epoch 97/5000\n",
      "321/321 [==============================] - 12s 39ms/step - loss: 0.5111 - accuracy: 0.8593 - sparse_levenshtein_v1: 0.1395 - val_loss: 0.5015 - val_accuracy: 0.8614 - val_sparse_levenshtein_v1: 0.1377\n",
      "Epoch 98/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.5092 - accuracy: 0.8600 - sparse_levenshtein_v1: 0.1388 - val_loss: 0.4986 - val_accuracy: 0.8623 - val_sparse_levenshtein_v1: 0.1369\n",
      "Epoch 99/5000\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5067 - accuracy: 0.8609 - sparse_levenshtein_v1: 0.1380 - val_loss: 0.4955 - val_accuracy: 0.8633 - val_sparse_levenshtein_v1: 0.1358\n",
      "Epoch 100/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.5042 - accuracy: 0.8616 - sparse_levenshtein_v1: 0.1372 - val_loss: 0.4940 - val_accuracy: 0.8637 - val_sparse_levenshtein_v1: 0.1355\n",
      "Epoch 101/5000\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.5020 - accuracy: 0.8622 - sparse_levenshtein_v1: 0.1366 - val_loss: 0.4930 - val_accuracy: 0.8641 - val_sparse_levenshtein_v1: 0.1351\n",
      "Epoch 102/5000\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4998 - accuracy: 0.8629 - sparse_levenshtein_v1: 0.1360 - val_loss: 0.4919 - val_accuracy: 0.8640 - val_sparse_levenshtein_v1: 0.1351\n",
      "Epoch 103/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4978 - accuracy: 0.8635 - sparse_levenshtein_v1: 0.1354 - val_loss: 0.4883 - val_accuracy: 0.8657 - val_sparse_levenshtein_v1: 0.1336\n",
      "Epoch 104/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4960 - accuracy: 0.8642 - sparse_levenshtein_v1: 0.1348 - val_loss: 0.4873 - val_accuracy: 0.8656 - val_sparse_levenshtein_v1: 0.1335\n",
      "Epoch 105/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4938 - accuracy: 0.8648 - sparse_levenshtein_v1: 0.1342 - val_loss: 0.4861 - val_accuracy: 0.8661 - val_sparse_levenshtein_v1: 0.1331\n",
      "Epoch 106/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4922 - accuracy: 0.8653 - sparse_levenshtein_v1: 0.1336 - val_loss: 0.4836 - val_accuracy: 0.8672 - val_sparse_levenshtein_v1: 0.1321\n",
      "Epoch 107/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4902 - accuracy: 0.8660 - sparse_levenshtein_v1: 0.1330 - val_loss: 0.4822 - val_accuracy: 0.8675 - val_sparse_levenshtein_v1: 0.1318\n",
      "Epoch 108/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4884 - accuracy: 0.8666 - sparse_levenshtein_v1: 0.1324 - val_loss: 0.4815 - val_accuracy: 0.8681 - val_sparse_levenshtein_v1: 0.1311\n",
      "Epoch 109/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4870 - accuracy: 0.8670 - sparse_levenshtein_v1: 0.1319 - val_loss: 0.4790 - val_accuracy: 0.8685 - val_sparse_levenshtein_v1: 0.1308\n",
      "Epoch 110/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4850 - accuracy: 0.8675 - sparse_levenshtein_v1: 0.1315 - val_loss: 0.4786 - val_accuracy: 0.8686 - val_sparse_levenshtein_v1: 0.1304\n",
      "Epoch 111/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4833 - accuracy: 0.8682 - sparse_levenshtein_v1: 0.1309 - val_loss: 0.4774 - val_accuracy: 0.8691 - val_sparse_levenshtein_v1: 0.1301\n",
      "Epoch 112/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4819 - accuracy: 0.8686 - sparse_levenshtein_v1: 0.1305 - val_loss: 0.4764 - val_accuracy: 0.8694 - val_sparse_levenshtein_v1: 0.1298\n",
      "Epoch 113/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4806 - accuracy: 0.8689 - sparse_levenshtein_v1: 0.1301 - val_loss: 0.4759 - val_accuracy: 0.8695 - val_sparse_levenshtein_v1: 0.1296\n",
      "Epoch 114/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4788 - accuracy: 0.8695 - sparse_levenshtein_v1: 0.1295 - val_loss: 0.4736 - val_accuracy: 0.8704 - val_sparse_levenshtein_v1: 0.1287\n",
      "Epoch 115/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4774 - accuracy: 0.8698 - sparse_levenshtein_v1: 0.1292 - val_loss: 0.4729 - val_accuracy: 0.8706 - val_sparse_levenshtein_v1: 0.1285\n",
      "Epoch 116/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4760 - accuracy: 0.8704 - sparse_levenshtein_v1: 0.1287 - val_loss: 0.4710 - val_accuracy: 0.8713 - val_sparse_levenshtein_v1: 0.1279\n",
      "Epoch 117/5000\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4746 - accuracy: 0.8707 - sparse_levenshtein_v1: 0.1284 - val_loss: 0.4696 - val_accuracy: 0.8716 - val_sparse_levenshtein_v1: 0.1276\n",
      "Epoch 118/5000\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4731 - accuracy: 0.8711 - sparse_levenshtein_v1: 0.1280 - val_loss: 0.4692 - val_accuracy: 0.8719 - val_sparse_levenshtein_v1: 0.1272\n",
      "Epoch 119/5000\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4714 - accuracy: 0.8718 - sparse_levenshtein_v1: 0.1273 - val_loss: 0.4681 - val_accuracy: 0.8720 - val_sparse_levenshtein_v1: 0.1271\n",
      "Epoch 120/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4707 - accuracy: 0.8721 - sparse_levenshtein_v1: 0.1270 - val_loss: 0.4667 - val_accuracy: 0.8729 - val_sparse_levenshtein_v1: 0.1263\n",
      "Epoch 121/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4689 - accuracy: 0.8724 - sparse_levenshtein_v1: 0.1267 - val_loss: 0.4665 - val_accuracy: 0.8728 - val_sparse_levenshtein_v1: 0.1263\n",
      "Epoch 122/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4682 - accuracy: 0.8727 - sparse_levenshtein_v1: 0.1264 - val_loss: 0.4655 - val_accuracy: 0.8732 - val_sparse_levenshtein_v1: 0.1260\n",
      "Epoch 123/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4666 - accuracy: 0.8733 - sparse_levenshtein_v1: 0.1259 - val_loss: 0.4640 - val_accuracy: 0.8737 - val_sparse_levenshtein_v1: 0.1255\n",
      "Epoch 124/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4653 - accuracy: 0.8736 - sparse_levenshtein_v1: 0.1256 - val_loss: 0.4633 - val_accuracy: 0.8738 - val_sparse_levenshtein_v1: 0.1253\n",
      "Epoch 125/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4639 - accuracy: 0.8741 - sparse_levenshtein_v1: 0.1251 - val_loss: 0.4626 - val_accuracy: 0.8740 - val_sparse_levenshtein_v1: 0.1252\n",
      "Epoch 126/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4629 - accuracy: 0.8744 - sparse_levenshtein_v1: 0.1248 - val_loss: 0.4630 - val_accuracy: 0.8739 - val_sparse_levenshtein_v1: 0.1254\n",
      "Epoch 127/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4620 - accuracy: 0.8747 - sparse_levenshtein_v1: 0.1244 - val_loss: 0.4606 - val_accuracy: 0.8747 - val_sparse_levenshtein_v1: 0.1246\n",
      "Epoch 128/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4608 - accuracy: 0.8751 - sparse_levenshtein_v1: 0.1241 - val_loss: 0.4599 - val_accuracy: 0.8750 - val_sparse_levenshtein_v1: 0.1243\n",
      "Epoch 129/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4598 - accuracy: 0.8754 - sparse_levenshtein_v1: 0.1238 - val_loss: 0.4602 - val_accuracy: 0.8749 - val_sparse_levenshtein_v1: 0.1243\n",
      "Epoch 130/5000\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4588 - accuracy: 0.8757 - sparse_levenshtein_v1: 0.1235 - val_loss: 0.4593 - val_accuracy: 0.8752 - val_sparse_levenshtein_v1: 0.1239\n",
      "Epoch 131/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4576 - accuracy: 0.8760 - sparse_levenshtein_v1: 0.1232 - val_loss: 0.4589 - val_accuracy: 0.8755 - val_sparse_levenshtein_v1: 0.1237\n",
      "Epoch 132/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4562 - accuracy: 0.8763 - sparse_levenshtein_v1: 0.1229 - val_loss: 0.4572 - val_accuracy: 0.8757 - val_sparse_levenshtein_v1: 0.1236\n",
      "Epoch 133/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4555 - accuracy: 0.8765 - sparse_levenshtein_v1: 0.1227 - val_loss: 0.4565 - val_accuracy: 0.8761 - val_sparse_levenshtein_v1: 0.1231\n",
      "Epoch 134/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4544 - accuracy: 0.8769 - sparse_levenshtein_v1: 0.1223 - val_loss: 0.4559 - val_accuracy: 0.8762 - val_sparse_levenshtein_v1: 0.1231\n",
      "Epoch 135/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4538 - accuracy: 0.8770 - sparse_levenshtein_v1: 0.1222 - val_loss: 0.4561 - val_accuracy: 0.8762 - val_sparse_levenshtein_v1: 0.1230\n",
      "Epoch 136/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4525 - accuracy: 0.8774 - sparse_levenshtein_v1: 0.1218 - val_loss: 0.4544 - val_accuracy: 0.8768 - val_sparse_levenshtein_v1: 0.1224\n",
      "Epoch 137/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4517 - accuracy: 0.8778 - sparse_levenshtein_v1: 0.1215 - val_loss: 0.4548 - val_accuracy: 0.8766 - val_sparse_levenshtein_v1: 0.1227\n",
      "Epoch 138/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4509 - accuracy: 0.8780 - sparse_levenshtein_v1: 0.1212 - val_loss: 0.4534 - val_accuracy: 0.8771 - val_sparse_levenshtein_v1: 0.1222\n",
      "Epoch 139/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4499 - accuracy: 0.8783 - sparse_levenshtein_v1: 0.1210 - val_loss: 0.4531 - val_accuracy: 0.8770 - val_sparse_levenshtein_v1: 0.1223\n",
      "Epoch 140/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4489 - accuracy: 0.8786 - sparse_levenshtein_v1: 0.1207 - val_loss: 0.4512 - val_accuracy: 0.8777 - val_sparse_levenshtein_v1: 0.1215\n",
      "Epoch 141/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4479 - accuracy: 0.8787 - sparse_levenshtein_v1: 0.1205 - val_loss: 0.4523 - val_accuracy: 0.8775 - val_sparse_levenshtein_v1: 0.1217\n",
      "Epoch 142/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4469 - accuracy: 0.8790 - sparse_levenshtein_v1: 0.1202 - val_loss: 0.4512 - val_accuracy: 0.8781 - val_sparse_levenshtein_v1: 0.1212\n",
      "Epoch 143/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4462 - accuracy: 0.8793 - sparse_levenshtein_v1: 0.1199 - val_loss: 0.4506 - val_accuracy: 0.8781 - val_sparse_levenshtein_v1: 0.1211\n",
      "Epoch 144/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4453 - accuracy: 0.8797 - sparse_levenshtein_v1: 0.1196 - val_loss: 0.4513 - val_accuracy: 0.8782 - val_sparse_levenshtein_v1: 0.1210\n",
      "Epoch 145/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4443 - accuracy: 0.8798 - sparse_levenshtein_v1: 0.1194 - val_loss: 0.4515 - val_accuracy: 0.8779 - val_sparse_levenshtein_v1: 0.1214\n",
      "Epoch 146/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4438 - accuracy: 0.8799 - sparse_levenshtein_v1: 0.1193 - val_loss: 0.4495 - val_accuracy: 0.8786 - val_sparse_levenshtein_v1: 0.1206\n",
      "Epoch 147/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4427 - accuracy: 0.8803 - sparse_levenshtein_v1: 0.1190 - val_loss: 0.4480 - val_accuracy: 0.8790 - val_sparse_levenshtein_v1: 0.1202\n",
      "Epoch 148/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4422 - accuracy: 0.8803 - sparse_levenshtein_v1: 0.1189 - val_loss: 0.4480 - val_accuracy: 0.8790 - val_sparse_levenshtein_v1: 0.1203\n",
      "Epoch 149/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4411 - accuracy: 0.8808 - sparse_levenshtein_v1: 0.1185 - val_loss: 0.4477 - val_accuracy: 0.8792 - val_sparse_levenshtein_v1: 0.1201\n",
      "Epoch 150/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4403 - accuracy: 0.8810 - sparse_levenshtein_v1: 0.1183 - val_loss: 0.4467 - val_accuracy: 0.8795 - val_sparse_levenshtein_v1: 0.1197\n",
      "Epoch 151/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4393 - accuracy: 0.8812 - sparse_levenshtein_v1: 0.1181 - val_loss: 0.4475 - val_accuracy: 0.8793 - val_sparse_levenshtein_v1: 0.1200\n",
      "Epoch 152/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4389 - accuracy: 0.8815 - sparse_levenshtein_v1: 0.1178 - val_loss: 0.4463 - val_accuracy: 0.8795 - val_sparse_levenshtein_v1: 0.1198\n",
      "Epoch 153/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4383 - accuracy: 0.8814 - sparse_levenshtein_v1: 0.1179 - val_loss: 0.4470 - val_accuracy: 0.8795 - val_sparse_levenshtein_v1: 0.1198\n",
      "Epoch 154/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4369 - accuracy: 0.8818 - sparse_levenshtein_v1: 0.1175 - val_loss: 0.4441 - val_accuracy: 0.8801 - val_sparse_levenshtein_v1: 0.1192\n",
      "Epoch 155/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4366 - accuracy: 0.8820 - sparse_levenshtein_v1: 0.1174 - val_loss: 0.4446 - val_accuracy: 0.8802 - val_sparse_levenshtein_v1: 0.1190\n",
      "Epoch 156/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4355 - accuracy: 0.8823 - sparse_levenshtein_v1: 0.1171 - val_loss: 0.4440 - val_accuracy: 0.8804 - val_sparse_levenshtein_v1: 0.1189\n",
      "Epoch 157/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4353 - accuracy: 0.8825 - sparse_levenshtein_v1: 0.1169 - val_loss: 0.4448 - val_accuracy: 0.8800 - val_sparse_levenshtein_v1: 0.1193\n",
      "Epoch 158/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4341 - accuracy: 0.8826 - sparse_levenshtein_v1: 0.1167 - val_loss: 0.4429 - val_accuracy: 0.8807 - val_sparse_levenshtein_v1: 0.1186\n",
      "Epoch 159/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4339 - accuracy: 0.8829 - sparse_levenshtein_v1: 0.1164 - val_loss: 0.4443 - val_accuracy: 0.8806 - val_sparse_levenshtein_v1: 0.1187\n",
      "Epoch 160/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4331 - accuracy: 0.8831 - sparse_levenshtein_v1: 0.1163 - val_loss: 0.4433 - val_accuracy: 0.8806 - val_sparse_levenshtein_v1: 0.1187\n",
      "Epoch 161/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4327 - accuracy: 0.8831 - sparse_levenshtein_v1: 0.1162 - val_loss: 0.4421 - val_accuracy: 0.8809 - val_sparse_levenshtein_v1: 0.1184\n",
      "Epoch 162/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4318 - accuracy: 0.8834 - sparse_levenshtein_v1: 0.1160 - val_loss: 0.4413 - val_accuracy: 0.8813 - val_sparse_levenshtein_v1: 0.1180\n",
      "Epoch 163/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4314 - accuracy: 0.8835 - sparse_levenshtein_v1: 0.1159 - val_loss: 0.4419 - val_accuracy: 0.8811 - val_sparse_levenshtein_v1: 0.1182\n",
      "Epoch 164/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4308 - accuracy: 0.8836 - sparse_levenshtein_v1: 0.1158 - val_loss: 0.4408 - val_accuracy: 0.8817 - val_sparse_levenshtein_v1: 0.1176\n",
      "Epoch 165/5000\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4302 - accuracy: 0.8840 - sparse_levenshtein_v1: 0.1154 - val_loss: 0.4404 - val_accuracy: 0.8815 - val_sparse_levenshtein_v1: 0.1178\n",
      "Epoch 166/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4297 - accuracy: 0.8838 - sparse_levenshtein_v1: 0.1155 - val_loss: 0.4401 - val_accuracy: 0.8817 - val_sparse_levenshtein_v1: 0.1175\n",
      "Epoch 167/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4288 - accuracy: 0.8844 - sparse_levenshtein_v1: 0.1150 - val_loss: 0.4400 - val_accuracy: 0.8818 - val_sparse_levenshtein_v1: 0.1175\n",
      "Epoch 168/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4281 - accuracy: 0.8845 - sparse_levenshtein_v1: 0.1149 - val_loss: 0.4408 - val_accuracy: 0.8815 - val_sparse_levenshtein_v1: 0.1179\n",
      "Epoch 169/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4278 - accuracy: 0.8844 - sparse_levenshtein_v1: 0.1150 - val_loss: 0.4402 - val_accuracy: 0.8819 - val_sparse_levenshtein_v1: 0.1174\n",
      "Epoch 170/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4274 - accuracy: 0.8846 - sparse_levenshtein_v1: 0.1148 - val_loss: 0.4401 - val_accuracy: 0.8817 - val_sparse_levenshtein_v1: 0.1176\n",
      "Epoch 171/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4265 - accuracy: 0.8849 - sparse_levenshtein_v1: 0.1145 - val_loss: 0.4400 - val_accuracy: 0.8818 - val_sparse_levenshtein_v1: 0.1175\n",
      "Epoch 172/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4256 - accuracy: 0.8852 - sparse_levenshtein_v1: 0.1142 - val_loss: 0.4398 - val_accuracy: 0.8820 - val_sparse_levenshtein_v1: 0.1173\n",
      "Epoch 173/5000\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4253 - accuracy: 0.8852 - sparse_levenshtein_v1: 0.1143 - val_loss: 0.4382 - val_accuracy: 0.8824 - val_sparse_levenshtein_v1: 0.1169\n",
      "Epoch 174/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4249 - accuracy: 0.8853 - sparse_levenshtein_v1: 0.1141 - val_loss: 0.4378 - val_accuracy: 0.8825 - val_sparse_levenshtein_v1: 0.1167\n",
      "Epoch 175/5000\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4241 - accuracy: 0.8855 - sparse_levenshtein_v1: 0.1139 - val_loss: 0.4373 - val_accuracy: 0.8825 - val_sparse_levenshtein_v1: 0.1168\n",
      "Epoch 176/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4233 - accuracy: 0.8857 - sparse_levenshtein_v1: 0.1137 - val_loss: 0.4372 - val_accuracy: 0.8827 - val_sparse_levenshtein_v1: 0.1167\n",
      "Epoch 177/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4229 - accuracy: 0.8860 - sparse_levenshtein_v1: 0.1134 - val_loss: 0.4364 - val_accuracy: 0.8829 - val_sparse_levenshtein_v1: 0.1163\n",
      "Epoch 178/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4227 - accuracy: 0.8859 - sparse_levenshtein_v1: 0.1135 - val_loss: 0.4370 - val_accuracy: 0.8828 - val_sparse_levenshtein_v1: 0.1166\n",
      "Epoch 179/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4219 - accuracy: 0.8862 - sparse_levenshtein_v1: 0.1133 - val_loss: 0.4357 - val_accuracy: 0.8829 - val_sparse_levenshtein_v1: 0.1164\n",
      "Epoch 180/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4217 - accuracy: 0.8863 - sparse_levenshtein_v1: 0.1131 - val_loss: 0.4370 - val_accuracy: 0.8827 - val_sparse_levenshtein_v1: 0.1167\n",
      "Epoch 181/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4210 - accuracy: 0.8863 - sparse_levenshtein_v1: 0.1132 - val_loss: 0.4351 - val_accuracy: 0.8833 - val_sparse_levenshtein_v1: 0.1162\n",
      "Epoch 182/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4207 - accuracy: 0.8863 - sparse_levenshtein_v1: 0.1131 - val_loss: 0.4342 - val_accuracy: 0.8833 - val_sparse_levenshtein_v1: 0.1161\n",
      "Epoch 183/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4203 - accuracy: 0.8867 - sparse_levenshtein_v1: 0.1127 - val_loss: 0.4346 - val_accuracy: 0.8834 - val_sparse_levenshtein_v1: 0.1159\n",
      "Epoch 184/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4198 - accuracy: 0.8868 - sparse_levenshtein_v1: 0.1127 - val_loss: 0.4346 - val_accuracy: 0.8835 - val_sparse_levenshtein_v1: 0.1158\n",
      "Epoch 185/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4195 - accuracy: 0.8871 - sparse_levenshtein_v1: 0.1123 - val_loss: 0.4338 - val_accuracy: 0.8836 - val_sparse_levenshtein_v1: 0.1158\n",
      "Epoch 186/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4190 - accuracy: 0.8871 - sparse_levenshtein_v1: 0.1124 - val_loss: 0.4335 - val_accuracy: 0.8838 - val_sparse_levenshtein_v1: 0.1154\n",
      "Epoch 187/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4185 - accuracy: 0.8871 - sparse_levenshtein_v1: 0.1124 - val_loss: 0.4339 - val_accuracy: 0.8839 - val_sparse_levenshtein_v1: 0.1154\n",
      "Epoch 188/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4178 - accuracy: 0.8874 - sparse_levenshtein_v1: 0.1120 - val_loss: 0.4340 - val_accuracy: 0.8837 - val_sparse_levenshtein_v1: 0.1157\n",
      "Epoch 189/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4172 - accuracy: 0.8875 - sparse_levenshtein_v1: 0.1119 - val_loss: 0.4327 - val_accuracy: 0.8842 - val_sparse_levenshtein_v1: 0.1151\n",
      "Epoch 190/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4170 - accuracy: 0.8875 - sparse_levenshtein_v1: 0.1120 - val_loss: 0.4345 - val_accuracy: 0.8836 - val_sparse_levenshtein_v1: 0.1158\n",
      "Epoch 191/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4164 - accuracy: 0.8878 - sparse_levenshtein_v1: 0.1117 - val_loss: 0.4339 - val_accuracy: 0.8840 - val_sparse_levenshtein_v1: 0.1153\n",
      "Epoch 192/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4163 - accuracy: 0.8877 - sparse_levenshtein_v1: 0.1118 - val_loss: 0.4327 - val_accuracy: 0.8842 - val_sparse_levenshtein_v1: 0.1150\n",
      "Epoch 193/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4154 - accuracy: 0.8879 - sparse_levenshtein_v1: 0.1116 - val_loss: 0.4323 - val_accuracy: 0.8841 - val_sparse_levenshtein_v1: 0.1152\n",
      "Epoch 194/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4148 - accuracy: 0.8881 - sparse_levenshtein_v1: 0.1113 - val_loss: 0.4323 - val_accuracy: 0.8847 - val_sparse_levenshtein_v1: 0.1146\n",
      "Epoch 195/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4149 - accuracy: 0.8883 - sparse_levenshtein_v1: 0.1112 - val_loss: 0.4324 - val_accuracy: 0.8843 - val_sparse_levenshtein_v1: 0.1150\n",
      "Epoch 196/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4140 - accuracy: 0.8884 - sparse_levenshtein_v1: 0.1111 - val_loss: 0.4322 - val_accuracy: 0.8844 - val_sparse_levenshtein_v1: 0.1148\n",
      "Epoch 197/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4137 - accuracy: 0.8884 - sparse_levenshtein_v1: 0.1110 - val_loss: 0.4322 - val_accuracy: 0.8844 - val_sparse_levenshtein_v1: 0.1149\n",
      "Epoch 198/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4136 - accuracy: 0.8885 - sparse_levenshtein_v1: 0.1110 - val_loss: 0.4304 - val_accuracy: 0.8846 - val_sparse_levenshtein_v1: 0.1147\n",
      "Epoch 199/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4131 - accuracy: 0.8886 - sparse_levenshtein_v1: 0.1109 - val_loss: 0.4314 - val_accuracy: 0.8846 - val_sparse_levenshtein_v1: 0.1146\n",
      "Epoch 200/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4127 - accuracy: 0.8886 - sparse_levenshtein_v1: 0.1109 - val_loss: 0.4309 - val_accuracy: 0.8846 - val_sparse_levenshtein_v1: 0.1148\n",
      "Epoch 201/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4118 - accuracy: 0.8889 - sparse_levenshtein_v1: 0.1105 - val_loss: 0.4315 - val_accuracy: 0.8847 - val_sparse_levenshtein_v1: 0.1147\n",
      "Epoch 202/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4118 - accuracy: 0.8890 - sparse_levenshtein_v1: 0.1105 - val_loss: 0.4307 - val_accuracy: 0.8846 - val_sparse_levenshtein_v1: 0.1146\n",
      "Epoch 203/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4116 - accuracy: 0.8890 - sparse_levenshtein_v1: 0.1105 - val_loss: 0.4319 - val_accuracy: 0.8847 - val_sparse_levenshtein_v1: 0.1145\n",
      "Epoch 204/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4115 - accuracy: 0.8890 - sparse_levenshtein_v1: 0.1105 - val_loss: 0.4306 - val_accuracy: 0.8851 - val_sparse_levenshtein_v1: 0.1141\n",
      "Epoch 205/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4109 - accuracy: 0.8893 - sparse_levenshtein_v1: 0.1102 - val_loss: 0.4310 - val_accuracy: 0.8850 - val_sparse_levenshtein_v1: 0.1143\n",
      "Epoch 206/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4101 - accuracy: 0.8892 - sparse_levenshtein_v1: 0.1102 - val_loss: 0.4315 - val_accuracy: 0.8846 - val_sparse_levenshtein_v1: 0.1148\n",
      "Epoch 207/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4099 - accuracy: 0.8895 - sparse_levenshtein_v1: 0.1100 - val_loss: 0.4310 - val_accuracy: 0.8849 - val_sparse_levenshtein_v1: 0.1144\n",
      "Epoch 208/5000\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4096 - accuracy: 0.8896 - sparse_levenshtein_v1: 0.1099 - val_loss: 0.4296 - val_accuracy: 0.8854 - val_sparse_levenshtein_v1: 0.1140\n",
      "Epoch 209/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4091 - accuracy: 0.8898 - sparse_levenshtein_v1: 0.1097 - val_loss: 0.4299 - val_accuracy: 0.8854 - val_sparse_levenshtein_v1: 0.1140\n",
      "Epoch 210/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4090 - accuracy: 0.8898 - sparse_levenshtein_v1: 0.1097 - val_loss: 0.4288 - val_accuracy: 0.8855 - val_sparse_levenshtein_v1: 0.1138\n",
      "Epoch 211/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4081 - accuracy: 0.8899 - sparse_levenshtein_v1: 0.1096 - val_loss: 0.4289 - val_accuracy: 0.8852 - val_sparse_levenshtein_v1: 0.1141\n",
      "Epoch 212/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4087 - accuracy: 0.8899 - sparse_levenshtein_v1: 0.1096 - val_loss: 0.4282 - val_accuracy: 0.8854 - val_sparse_levenshtein_v1: 0.1140\n",
      "Epoch 213/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4081 - accuracy: 0.8899 - sparse_levenshtein_v1: 0.1096 - val_loss: 0.4286 - val_accuracy: 0.8854 - val_sparse_levenshtein_v1: 0.1141\n",
      "Epoch 214/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4078 - accuracy: 0.8900 - sparse_levenshtein_v1: 0.1095 - val_loss: 0.4287 - val_accuracy: 0.8855 - val_sparse_levenshtein_v1: 0.1138\n",
      "Epoch 215/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4075 - accuracy: 0.8902 - sparse_levenshtein_v1: 0.1093 - val_loss: 0.4279 - val_accuracy: 0.8857 - val_sparse_levenshtein_v1: 0.1137\n",
      "Epoch 216/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4068 - accuracy: 0.8903 - sparse_levenshtein_v1: 0.1092 - val_loss: 0.4284 - val_accuracy: 0.8857 - val_sparse_levenshtein_v1: 0.1135\n",
      "Epoch 217/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4065 - accuracy: 0.8904 - sparse_levenshtein_v1: 0.1091 - val_loss: 0.4293 - val_accuracy: 0.8852 - val_sparse_levenshtein_v1: 0.1141\n",
      "Epoch 218/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4061 - accuracy: 0.8906 - sparse_levenshtein_v1: 0.1090 - val_loss: 0.4281 - val_accuracy: 0.8856 - val_sparse_levenshtein_v1: 0.1136\n",
      "Epoch 219/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4059 - accuracy: 0.8905 - sparse_levenshtein_v1: 0.1089 - val_loss: 0.4292 - val_accuracy: 0.8854 - val_sparse_levenshtein_v1: 0.1140\n",
      "Epoch 220/5000\n",
      "321/321 [==============================] - 14s 44ms/step - loss: 0.4052 - accuracy: 0.8909 - sparse_levenshtein_v1: 0.1087 - val_loss: 0.4273 - val_accuracy: 0.8862 - val_sparse_levenshtein_v1: 0.1131\n",
      "Epoch 221/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4050 - accuracy: 0.8909 - sparse_levenshtein_v1: 0.1086 - val_loss: 0.4268 - val_accuracy: 0.8862 - val_sparse_levenshtein_v1: 0.1131\n",
      "Epoch 222/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4051 - accuracy: 0.8907 - sparse_levenshtein_v1: 0.1088 - val_loss: 0.4267 - val_accuracy: 0.8865 - val_sparse_levenshtein_v1: 0.1128\n",
      "Epoch 223/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4042 - accuracy: 0.8910 - sparse_levenshtein_v1: 0.1085 - val_loss: 0.4271 - val_accuracy: 0.8860 - val_sparse_levenshtein_v1: 0.1133\n",
      "Epoch 224/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4044 - accuracy: 0.8910 - sparse_levenshtein_v1: 0.1085 - val_loss: 0.4262 - val_accuracy: 0.8864 - val_sparse_levenshtein_v1: 0.1129\n",
      "Epoch 225/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4039 - accuracy: 0.8911 - sparse_levenshtein_v1: 0.1085 - val_loss: 0.4272 - val_accuracy: 0.8860 - val_sparse_levenshtein_v1: 0.1133\n",
      "Epoch 226/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4036 - accuracy: 0.8912 - sparse_levenshtein_v1: 0.1084 - val_loss: 0.4269 - val_accuracy: 0.8860 - val_sparse_levenshtein_v1: 0.1133\n",
      "Epoch 227/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4032 - accuracy: 0.8914 - sparse_levenshtein_v1: 0.1081 - val_loss: 0.4263 - val_accuracy: 0.8860 - val_sparse_levenshtein_v1: 0.1134\n",
      "Epoch 228/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4033 - accuracy: 0.8913 - sparse_levenshtein_v1: 0.1082 - val_loss: 0.4283 - val_accuracy: 0.8857 - val_sparse_levenshtein_v1: 0.1138\n",
      "Epoch 229/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4033 - accuracy: 0.8914 - sparse_levenshtein_v1: 0.1081 - val_loss: 0.4260 - val_accuracy: 0.8864 - val_sparse_levenshtein_v1: 0.1130\n",
      "Epoch 230/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4026 - accuracy: 0.8915 - sparse_levenshtein_v1: 0.1081 - val_loss: 0.4284 - val_accuracy: 0.8857 - val_sparse_levenshtein_v1: 0.1136\n",
      "Epoch 231/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4021 - accuracy: 0.8916 - sparse_levenshtein_v1: 0.1079 - val_loss: 0.4275 - val_accuracy: 0.8861 - val_sparse_levenshtein_v1: 0.1132\n",
      "Epoch 232/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4020 - accuracy: 0.8915 - sparse_levenshtein_v1: 0.1080 - val_loss: 0.4262 - val_accuracy: 0.8863 - val_sparse_levenshtein_v1: 0.1131\n",
      "Epoch 233/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4021 - accuracy: 0.8916 - sparse_levenshtein_v1: 0.1079 - val_loss: 0.4266 - val_accuracy: 0.8861 - val_sparse_levenshtein_v1: 0.1132\n",
      "Epoch 234/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4016 - accuracy: 0.8918 - sparse_levenshtein_v1: 0.1078 - val_loss: 0.4261 - val_accuracy: 0.8866 - val_sparse_levenshtein_v1: 0.1128\n",
      "Epoch 235/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4009 - accuracy: 0.8920 - sparse_levenshtein_v1: 0.1076 - val_loss: 0.4264 - val_accuracy: 0.8864 - val_sparse_levenshtein_v1: 0.1130\n",
      "Epoch 236/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4007 - accuracy: 0.8920 - sparse_levenshtein_v1: 0.1075 - val_loss: 0.4282 - val_accuracy: 0.8862 - val_sparse_levenshtein_v1: 0.1132\n",
      "Epoch 237/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4012 - accuracy: 0.8918 - sparse_levenshtein_v1: 0.1077 - val_loss: 0.4270 - val_accuracy: 0.8863 - val_sparse_levenshtein_v1: 0.1131\n",
      "Epoch 238/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4010 - accuracy: 0.8920 - sparse_levenshtein_v1: 0.1076 - val_loss: 0.4272 - val_accuracy: 0.8861 - val_sparse_levenshtein_v1: 0.1133\n",
      "Epoch 239/5000\n",
      "321/321 [==============================] - 14s 43ms/step - loss: 0.4001 - accuracy: 0.8922 - sparse_levenshtein_v1: 0.1074 - val_loss: 0.4272 - val_accuracy: 0.8860 - val_sparse_levenshtein_v1: 0.1134\n",
      "validation levenshtein distance: 0.18621140718460083\n",
      "Best hyperparameters: {'attention_heads': 7, 'learning_rate': 0.0027196542256953437, 'drop_out_out_decoder': 0.05, 'dense_layers': 3, 'drop_out': 0.0, 'encoder_kernel_size': 6}\n",
      "77/77 [==============================] - 1s 19ms/step - loss: 0.4260 - accuracy: 0.8864 - sparse_levenshtein_v1: 0.1130\n",
      "Metrics in Validation: [0.42600005865097046, 0.8863524794578552, 0.11301431804895401]\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "trials = study.best_trials\n",
    "\n",
    "for index, trial in enumerate(trials):\n",
    "    print(f\"Best model: {index+1}\")\n",
    "\n",
    "    model = build_prod_transformer_model_v2(trial=None)\n",
    "\n",
    "    model.build([(None, MAX_LENGHT_SOURCE, int(FEATURE_COLUMNS.shape[0]/2)), (None, TARGET_MAX_LENGHT)])\n",
    "\n",
    "    print(model.summary())\n",
    "    model.fit(train_dataset, validation_data=val_dataset, epochs=EPOCHS, callbacks=get_predefine_callbacks(model_name=MODEL_NAME, patience=10))\n",
    "   \n",
    "    print('validation levenshtein distance: {}'.format(trial.value))\n",
    "    print(\"Best hyperparameters: {}\".format(trial.params))\n",
    "\n",
    "    model.load_weights(f\"../best_model/prototype/{MODEL_NAME}\")\n",
    "\n",
    "    print(f\"Metrics in Validation: {model.evaluate(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_utils.dataset import char_to_num, num_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_sequence = [char_to_num[w] for w in [\"<\"]]\n",
    "\n",
    "# for batch_index, batch in enumerate(val_dataset):\n",
    "#     batch = batch[0]\n",
    "\n",
    "#     sources = batch[0] #batch[\"source\"]\n",
    "#     targets = batch[1] #batch[\"target\"]\n",
    "    \n",
    "#     print(sources.shape)\n",
    "#     print(targets.shape)\n",
    "\n",
    "#     for index_sample, (source, target) in enumerate(zip(sources, targets)):\n",
    "#         source = tf.expand_dims(source, axis=0)\n",
    "#         target_sequence = [char_to_num[w] for w in [\"<\"]]\n",
    "#         y_true = \"\".join([num_to_char[w] for w in target.numpy()])\n",
    "    \n",
    "#         for i in range(TARGET_MAX_LENGHT):\n",
    "#             next_token = tf.expand_dims(tf.pad(tf.constant(target_sequence),\n",
    "#              [[0, TARGET_MAX_LENGHT-len(target_sequence)]],\n",
    "#               mode='CONSTANT',\n",
    "#                constant_values=pad_token_idx,\n",
    "#                 name=None),\n",
    "#                  axis=0)\n",
    "\n",
    "#             print(\"next target sequence to predict: \", next_token)\n",
    "#             y_pred = model((source, next_token))\n",
    "\n",
    "#             y_pred = tf.cast(tf.argmax(y_pred, axis=2), dtype=tf.int32)\n",
    "\n",
    "#             print(\"argmax:\", y_pred)\n",
    "\n",
    "#             mask = tf.not_equal(y_pred, pad_token_idx)\n",
    "#             next_token = y_pred[mask][-1].numpy()\n",
    "\n",
    "#             target_sequence.append(next_token)\n",
    "\n",
    "#             print(\"sequence so far: \", \"\".join([num_to_char[w] for w in target_sequence]))\n",
    "#             print(\"Label: \", y_true)\n",
    "\n",
    "#             if num_to_char[next_token]==\">\":\n",
    "#                 break\n",
    "\n",
    "#         print(f\"================================={index_sample}=========================================\")\n",
    "#         if index_sample==1:\n",
    "#             break\n",
    "\n",
    "#     if batch_index==1:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"finger_spelling_v2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " landmark_embedding_v2 (Lan  multiple                  147200    \n",
      " dmarkEmbeddingV2)                                               \n",
      "                                                                 \n",
      " basic_positional_embedding  multiple                  16128     \n",
      " s (BasicPositionalEmbeddin                                      \n",
      " gs)                                                             \n",
      "                                                                 \n",
      " transformer_encoder (Trans  multiple                  233992    \n",
      " formerEncoder)                                                  \n",
      "                                                                 \n",
      " transformer_decoder (Trans  multiple                  467343    \n",
      " formerDecoder)                                                  \n",
      "                                                                 \n",
      " dropout (Dropout)           multiple                  0         \n",
      "                                                                 \n",
      " sequential_2 (Sequential)   (None, None, 42)          10836     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         multiple                  0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             multiple                  2666      \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Total params: 878165 (3.35 MB)\n",
      "Trainable params: 878165 (3.35 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/prod_v2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../models/prod_v2/assets\n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "\n",
    "model.save(f\"../models/{MODEL_NAME}\", save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFLiteModel(tf.Module):\n",
    "    def __init__(self, model):\n",
    "        super(TFLiteModel, self).__init__()\n",
    "        self.target_start_token_idx = start_token_idx\n",
    "        self.target_end_token_idx = end_token_idx\n",
    "        # Load the feature generation and main models\n",
    "        self.model = model\n",
    "\n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=[None, FEATURE_COLUMNS.shape[0]], dtype=tf.float32, name='inputs')])\n",
    "    def __call__(self, inputs, training=False):\n",
    "        # Preprocess Data\n",
    "        x = tf.cast(inputs, tf.float32)\n",
    "\n",
    "        x = x[None]\n",
    "\n",
    "        x = tf.cond(tf.shape(x)[1] == 0, lambda: tf.zeros((1, 1, FEATURE_COLUMNS.shape[0])), lambda: tf.identity(x))\n",
    "\n",
    "        x = x[0]\n",
    "\n",
    "        x = pre_process(x)\n",
    "        #shape after [MAX_LENGHT_SOURCE, FEATURE_SIZE]\n",
    "\n",
    "        x = x[None]\n",
    "\n",
    "        x = self.model.generate(x)\n",
    "\n",
    "        x = x[0]\n",
    "        idx = tf.argmax(tf.cast(tf.equal(x, self.target_end_token_idx), tf.int32))\n",
    "        idx = tf.where(tf.math.less(idx, 1), tf.constant(2, dtype=tf.int64), idx)\n",
    "        x = x[1:idx]\n",
    "\n",
    "        x = tf.one_hot(x, 59)\n",
    "        return {\"outputs\": x}\n",
    "\n",
    "tflitemodel_base = TFLiteModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpgbn0u_n9/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpgbn0u_n9/assets\n"
     ]
    }
   ],
   "source": [
    "keras_model_converter = tf.lite.TFLiteConverter.from_keras_model(tflitemodel_base)\n",
    "keras_model_converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
    "keras_model_converter.allow_custom_ops = True\n",
    "keras_model_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "tflite_model = keras_model_converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# with open('/kaggle/working/model.tflite', 'wb') as f:\n",
    "with open(\"../models/model.tflite\", \"wb\") as f:    \n",
    "    f.write(tflite_model)\n",
    "\n",
    "infargs = {\"selected_columns\" : list(FEATURE_COLUMNS)}\n",
    "\n",
    "# with open(\"inference_args.json\", \"w\") as json_file:\n",
    "with open(\"../models/inference_args.json\", \"w\") as json_file:\n",
    "    json.dump(infargs, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating: ../models/model.tflite (deflated 72%)\n",
      "updating: ../models/inference_args.json (deflated 84%)\n"
     ]
    }
   ],
   "source": [
    "!zip submission.zip  '../models/model.tflite' '../models/inference_args.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from batch 1\n",
    "\n",
    "source_batch, target_batch = next(iter(val_dataset))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated:  jespreltelto.com\n",
      "target:  5065 west 8th street\n",
      "generated:  jell co\n",
      "target:  +974-124-1526-3338\n",
      "generated:  jeliel conton\n",
      "target:  510-316-6437\n",
      "generated:  arclil ellon\n",
      "target:  2151 east hopi trail\n",
      "generated:  lililil stort\n",
      "target:  www.summitortho.com/\n",
      "generated:  jeil sonel\n",
      "target:  modul-pelatihan-akuntansi\n",
      "generated:  jesest conton\n",
      "target:  +92-32-64-950-534\n",
      "generated:  jelifa conton\n",
      "target:  dewayne levy\n",
      "generated:  jell forn\n",
      "target:  ginecologa.altervista.org\n",
      "generated:  coccelnellellonel.com\n",
      "target:  nuveitech/jupiler-league\n",
      "generated:  arrel janellon\n",
      "target:  frankie ponce\n",
      "generated:  kanla co\n",
      "target:  921 shoemaker canyon\n",
      "generated:  ililce branton\n",
      "target:  george munoz\n",
      "generated:  jelc finton\n",
      "target:  292-989-6767\n",
      "generated:  bill sannon\n",
      "target:  247656\n",
      "generated:  jicgie coun\n",
      "target:  843-494-0857\n",
      "generated:  jein co\n",
      "target:  663 owen oaks drive\n",
      "generated:  jiane count\n",
      "target:  downloadables\n",
      "generated:  seaccclillill\n",
      "target:  https://m1.mingkyaa.com\n",
      "generated:  j-0214\n",
      "target:  9622 lanetta drive\n",
      "generated:  4 blean con\n",
      "target:  6614 calvert school\n",
      "generated:  stoflilse ston\n",
      "target:  5978 clear creek access no 1\n",
      "generated:  jerle sterne\n",
      "target:  hva-former-barns-liv/newark\n",
      "generated:  jeill sord\n",
      "target:  +44-771-212\n",
      "generated:  87 bir coune\n",
      "target:  janet nielsen\n",
      "generated:  jellir conton\n",
      "target:  9726 nebraska lane southeast\n",
      "generated:  jisel coun\n",
      "target:  +65-9673-091-076\n",
      "generated:  illis conel\n",
      "target:  449 debi circle\n",
      "generated:  iselie co\n",
      "target:  4326 dogwood blossoms\n",
      "generated:  jelel janel\n",
      "target:  8816 bottomview\n",
      "generated:  sace cellon\n",
      "target:  cerebral-interfaces.com/\n",
      "generated:  sorre cerres\n",
      "target:  8675 leafy aspen\n",
      "generated:  jocd coun\n",
      "target:  /roquesaltes\n",
      "generated:  ilil bellinel\n",
      "target:  6800 falconbridge\n",
      "generated:  stellis clane\n",
      "target:  marketingedge.com.ng/qqlevel\n",
      "generated:  jeis bellel\n",
      "target:  ultramedica.sy/charlottetown\n",
      "generated:  7 elt road\n",
      "target:  carlos anderson\n",
      "generated:  jeii sto\n",
      "target:  8453 amoss mill\n",
      "generated:  jillil billen\n",
      "target:  +47-1780-53-82-1370\n",
      "generated:  jell sonon\n",
      "target:  hotelrestaurant.co.kr\n",
      "generated:  jiil sonton\n",
      "target:  goodbye-niche-site-school\n",
      "generated:  ililel conton\n",
      "target:  q-maruti-zen/\n",
      "generated:  jillel janell\n",
      "target:  /nordicfaunaicecream\n",
      "generated:  tora leise\n",
      "target:  +61-68-5690\n",
      "generated:  jeldel conon\n",
      "target:  /bucket-hat-anak\n",
      "generated:  jeldel co\n",
      "target:  92616/fairy-tail-chapter-437\n",
      "generated:  jiiie lane court\n",
      "target:  argent-fr/vigilantes\n",
      "generated:  jeiel sonel\n",
      "target:  6927 stearma drive\n",
      "generated:  4 lel count\n",
      "target:  225-429-4509\n",
      "generated:  jelt co\n",
      "target:  +27-0195-336-23-24\n",
      "generated:  illil caner\n",
      "target:  www.edogs.de\n",
      "generated:  87 lis stureet\n",
      "target:  397 chippingdon\n",
      "generated:  7 fira couth\n",
      "target:  www.niederlandeweltweit.nl\n",
      "generated:  jeldea cont\n",
      "target:  5033 inishbride\n",
      "generated:  jiciey brorto\n",
      "target:  +47-985-65-512-116\n",
      "generated:  andel coun\n",
      "target:  632-062-6193\n",
      "generated:  sacestornelline.com\n",
      "target:  geoffrey beil\n",
      "generated:  iilil coner\n",
      "target:  maximalatvija/legopouram\n",
      "generated:  87 jarg saner\n",
      "target:  leona huff\n",
      "generated:  j-j-j-j-jonela\n",
      "target:  +32-72-77-822-10-23739\n",
      "generated:  jell conton\n",
      "target:  ivan sampson\n",
      "generated:  jell sornon\n",
      "target:  57 south 143rd road\n",
      "generated:  jeil soner\n",
      "target:  manonlagreve.com\n",
      "generated:  jilir cinn\n",
      "target:  +45-476-068\n",
      "generated:  jeis coun\n",
      "target:  402 sandy knoll\n",
      "generated:  sanon cont\n",
      "target:  dora khan\n",
      "generated:  jell elline court\n",
      "target:  shop.vitcas.com/\n",
      "generated:  87 illil court\n",
      "target:  fenshuxian/\n",
      "generated:  jant conton\n",
      "target:  6643 lockart\n",
      "generated:  jist conton\n",
      "target:  290 baywood village\n",
      "generated:  7 janel court\n",
      "target:  pingkiti.hatenablog.com\n",
      "generated:  janel janer\n",
      "target:  www.compilerpress.ca/\n",
      "generated:  710 ylist cont\n",
      "target:  +43-2705-1586-281\n",
      "generated:  jellis lillon\n",
      "target:  8051 east 65th south\n",
      "generated:  77 lis jorto\n",
      "target:  www.whyf.org/mesajyo\n",
      "generated:  jiris coun\n",
      "target:  506-591-4284\n",
      "generated:  irele chant\n",
      "target:  156 namur\n",
      "generated:  jonel eane/\n",
      "target:  zachariah ventura\n",
      "generated:  snell clillel\n",
      "target:  2922 walnut dowler\n",
      "generated:  jell co\n",
      "target:  www.g-rent.it\n",
      "generated:  aclestereteel.com\n",
      "target:  aspoulx.footeo.com/46171\n",
      "generated:  corlilie con\n",
      "target:  +374-75-642-8321-08989\n",
      "generated:  112 acinte court\n",
      "target:  oyakouranai/critica-a-sobrio\n",
      "generated:  87 jerel co\n",
      "target:  chrunya/?spt=01299.10981/\n",
      "generated:  jisisnel cont\n",
      "target:  gesellschafts/saffron-harvest\n",
      "generated:  82 co road\n",
      "target:  mercedes dalton\n",
      "generated:  87 jane west\n",
      "target:  3307 cadwell court\n",
      "generated:  jeliel co\n",
      "target:  915-108-6761\n",
      "generated:  /jishon conon\n",
      "target:  +44-8405-56\n",
      "generated:  jein co\n",
      "target:  2752 foot trail to nakeen\n",
      "generated:  snil contin\n",
      "target:  2707 ures\n",
      "generated:  7 focan cont\n",
      "target:  1348 1511th\n",
      "generated:  /sisel con\n",
      "target:  5802 cll 503\n",
      "generated:  47 lie lane\n",
      "target:  www.zotac.com\n",
      "generated:  lise snege hant\n",
      "target:  www.agrodos.cz\n",
      "generated:  7 lell cont\n",
      "target:  204-616-6649\n",
      "generated:  7 fel ellane\n",
      "target:  dan moran\n",
      "generated:  jilis beant\n",
      "target:  stroydvorkashira.ru/\n",
      "generated:  ilil sonelin\n",
      "target:  umami-burger-hollywood/\n",
      "generated:  jilil sone court\n",
      "target:  2162 basket road\n",
      "generated:  surel jone\n",
      "target:  221792 nfs road 502\n",
      "generated:  jeil contan\n",
      "target:  orlando shelton\n",
      "generated:  janel conon\n",
      "target:  122507 moran haven\n",
      "generated:  blolsa janton\n",
      "target:  www.varotherham.org.uk\n",
      "generated:  jellie conton\n",
      "target:  ys1126.com/telma-hopkins\n",
      "generated:  sela cont\n",
      "target:  johnny rivers\n",
      "generated:  jeil conton\n",
      "target:  beatriz harrell\n",
      "generated:  j-77liel cont\n",
      "target:  761-309-3170\n",
      "generated:  jelcel co\n",
      "target:  116-844-6398\n",
      "generated:  7 ilrel court\n",
      "target:  kcconcerts.net\n",
      "generated:  stey jelline\n",
      "target:  gerry blake\n",
      "generated:  jele belde\n",
      "target:  670 flintlock lane southwest\n",
      "generated:  jillil illanel\n",
      "target:  +212-9146-625-877\n",
      "generated:  siyiy stont\n",
      "target:  200-633-8027\n",
      "generated:  jellisel cont\n",
      "target:  263 south alton court\n",
      "generated:  sobicellonon\n",
      "target:  /kursus-desain-grafis\n",
      "generated:  jene cont\n",
      "target:  ssam25.com\n",
      "generated:  jellel ellane\n",
      "target:  4950 hogadone place southwest\n",
      "generated:  jeldea cont\n",
      "target:  sylvester clay\n",
      "generated:  bill ellon\n",
      "target:  mimonel/meeting-agenda\n",
      "generated:  jelcel conton\n",
      "target:  www.sirius-travel.ba\n",
      "generated:  jies jane stouth\n",
      "target:  9681 w 49th pkwy\n",
      "generated:  jellel ellell\n",
      "target:  1991 canton roswell road\n",
      "generated:  anesrest clon\n",
      "target:  +86-8897-008-567-84-94\n",
      "generated:  /eslil jandint\n",
      "target:  7619 lathrop court\n",
      "generated:  le7 jodine pdroad\n",
      "target:  ctmfile.com/77051\n",
      "generated:  sorle cont\n",
      "target:  ypiaorid/poliveira\n",
      "generated:  jeice road\n",
      "target:  860-644-6115\n"
     ]
    }
   ],
   "source": [
    "REQUIRED_SIGNATURE = \"serving_default\"\n",
    "REQUIRED_OUTPUT = \"outputs\"\n",
    "\n",
    "# interpreter = tf.lite.Interpreter(\"model.tflite\")\n",
    "interpreter = tf.lite.Interpreter(\"../models/model.tflite\")\n",
    "\n",
    "# with open (\"/kaggle/input/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n",
    "with open (\"../data/asl-fingerspelling/character_to_prediction_index.json\", \"r\") as f:\n",
    "    character_map = json.load(f)\n",
    "\n",
    "rev_character_map = {j:i for i,j in character_map.items()}\n",
    "found_signatures = list(interpreter.get_signature_list().keys())\n",
    "\n",
    "if REQUIRED_SIGNATURE not in found_signatures:\n",
    "    raise KernelEvalException('Required input signature not found.')\n",
    "\n",
    "prediction_fn = interpreter.get_signature_runner(REQUIRED_SIGNATURE)\n",
    "\n",
    "prediction_str = \"\"\n",
    "for source_element, target_element in zip(source_batch, target_batch):\n",
    "    # print(tf.expand_dims(target_element, axis=0).numpy())\n",
    "\n",
    "    output = prediction_fn(inputs=source_element)\n",
    "\n",
    "    # print(output[REQUIRED_OUTPUT])\n",
    "\n",
    "    # break\n",
    "\n",
    "    print(\"generated: \", \"\".join([rev_character_map.get(s, \"\") for s in np.argmax(output[REQUIRED_OUTPUT], axis=1)]))\n",
    "    print(\"target: \", \"\".join([rev_character_map.get(s, \"\") for s in target_element.numpy()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'serving_default_inputs:0',\n",
       "  'index': 0,\n",
       "  'shape': array([128,  42], dtype=int32),\n",
       "  'shape_signature': array([-1, 84], dtype=int32),\n",
       "  'dtype': numpy.float32,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
       "   'zero_points': array([], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpreter.get_input_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fingerspelling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
